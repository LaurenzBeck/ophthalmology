{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udc41 - Contrastive Learning for Ophthalmology - \ud83d\udc41 Seminar in AI - JKU Linz \u26a0\ufe0f Active Development This Repository is in a very early concept/development stage and may drastically change. Use at your own risk. links to the official docs: \ud83d\udcbe Datasets \u2022 \ud83d\udd2c Experiments \u2022 \ud83d\udc0d API Reference Project This project was part of my master studies in Artificial Intelligence at the Johannes Kepler University in Linz. The goal of the Seminar \"practical work in AI\" was to conduct proper research and experiments in a chosen field. I decided to join the Machine Learning Institute for Life Science and was supervised by my two professors: Andreas F\u00fcrst Elisabeth Rumetshofer The domain of the project is computer vison applied to high-resolution retina scans to help developing models that can support physicians diagnosing certain eye deseases. Task Following current research trends from 2020/2021, contrastive representation learning methods and the influence of the view selection and generation process on the downstream performance was chosen as the main research focus. For a proper downstream evaluation of the learned representations, three tasks on retina scans are used: deasease grading of diabetic retinopathy image segmentation of certain anatomic and pathogen regions localization of important landmarks of the retina Dataset Three public datasets were used during the experiments of the project: Diabetic Retinopathy Detection Indian Diabetic Retinopathy Image Dataset Retina MNIST Installation To install the projects dependencies and create a virtual environment, make sure that your system has python (>=3.9,<3.10) and poetry installed. Then cd into the projects root directory and call: $poetry install","title":"Home"},{"location":"#active-development","text":"This Repository is in a very early concept/development stage and may drastically change. Use at your own risk. links to the official docs: \ud83d\udcbe Datasets \u2022 \ud83d\udd2c Experiments \u2022 \ud83d\udc0d API Reference","title":"\u26a0\ufe0f Active Development"},{"location":"#project","text":"This project was part of my master studies in Artificial Intelligence at the Johannes Kepler University in Linz. The goal of the Seminar \"practical work in AI\" was to conduct proper research and experiments in a chosen field. I decided to join the Machine Learning Institute for Life Science and was supervised by my two professors: Andreas F\u00fcrst Elisabeth Rumetshofer The domain of the project is computer vison applied to high-resolution retina scans to help developing models that can support physicians diagnosing certain eye deseases.","title":"Project"},{"location":"#task","text":"Following current research trends from 2020/2021, contrastive representation learning methods and the influence of the view selection and generation process on the downstream performance was chosen as the main research focus. For a proper downstream evaluation of the learned representations, three tasks on retina scans are used: deasease grading of diabetic retinopathy image segmentation of certain anatomic and pathogen regions localization of important landmarks of the retina","title":"Task"},{"location":"#dataset","text":"Three public datasets were used during the experiments of the project: Diabetic Retinopathy Detection Indian Diabetic Retinopathy Image Dataset Retina MNIST","title":"Dataset"},{"location":"#installation","text":"To install the projects dependencies and create a virtual environment, make sure that your system has python (>=3.9,<3.10) and poetry installed. Then cd into the projects root directory and call: $poetry install","title":"Installation"},{"location":"docs/commands/","text":"commands these are the commands to run the experiments on the jku server diabetic retinopathy desease grading - supervised baseline ImageNet pre-trained Resnet18 42 epochs, strong aug, batch_size 42 focal loss gamma 2 balanced loader python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 datamodule.batch_size = 42 lightning_module/loss = focal +datamodule.balanced_sampling = True logger.run_name = balanced_resnet18_focal_loss_strong_aug_42_epochs save_model = \"pretrained_resnet18_focal_strong_aug_balanced.pt\" trainer.gpus =[ 1 ] ImageNet pre-trained Resnet18 42 epochs, strong aug, batch_size 42 python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 datamodule.batch_size = 42 lightning_module/loss = weighted_cross_entropy logger.run_name = resnet18_strong_aug_42_epochs_weighted save_model = \"pretrained_resnet18_strong_aug_weighted.pt\" trainer.gpus =[ 2 ] ImageNet pre-trained Resnet18 42 epochs, strong aug, batch_size 42 regression python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 model.num_output_units = 1 datamodule.batch_size = 42 lightning_module = disease_grading_regression lightning_module/loss = mse logger.run_name = resnet18_regression_strong_aug_42_epochs save_model = \"pretrained_resnet18_regression_strong_aug.pt\" trainer.gpus =[ 1 ] RetinaMNIST ImageNet pre-trained Resnet18 1500 epochs, strong aug, batch_size 42 regression image_size=56 python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 model.num_output_units = 1 datamodule = retina_mnist datamodule.batch_size = 42 lightning_module = disease_grading_regression lightning_module/loss = mse lightning_module.num_train_samples = 1080 transforms@train_transforms = strong_normalize image_size = 56 logger.experiment_name = retina_mnist_disease_grading logger.run_name = resnet18_regression_strong_aug_1500_epochs_56imagesize save_model = \"retinamnist_pretrained_resnet18_regression_strong_aug_56imagesize.pt\" trainer.gpus =[ 0 ] IDRD ImageNet pre-trained Resnet18 4000 epochs, strong aug, batch_size 42 regression python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 model.num_output_units = 1 datamodule = indian_diabetic_retinopathy datamodule.batch_size = 42 lightning_module = disease_grading_regression lightning_module/loss = mse lightning_module.num_train_samples = 371 logger.experiment_name = indian_disease_grading logger.run_name = resnet18_regression_strong_aug_4000_epochs save_model = \"indian_pretrained_resnet18_regression_strong_aug.pt\" trainer.gpus =[ 1 ] diabetic retinopathycontrastive pre-training ImageNet pre-trained Resnet50 64 epochs, strong aug, batch_size 42 image size 256, balanced loader python ophthalmology/scripts/train_simclr.py environment = jku_ssd datamodule.batch_size = 42 trainer.max_epochs = 64 transforms@ssl_transforms = strong_normalize logger.run_name = balanced_pretrained_strong_aug_64_epochs save_model = \"pretrained_resnet50backbone_256image_balanced_strong_aug.pt\" trainer.gpus =[ 0 ] Supervised Baselines sup_drd_grading python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_drd_grading trainer.gpus =[ 1 ] sup_idrd_grading python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_idrd_grading trainer.gpus =[ 3 ] sup_idrd_segmentation sup_idrd_localization python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_idrd_localization trainer.gpus =[ 0 ] sup_mnist_grading python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_mnist_grading trainer.gpus =[ 2 ] Contrastive Pre-Training simclr_drd Supervised Fine-tuning ft_drd_grading python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_drd_grading trainer.gpus =[ 3 ] ft_idrd_grading python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_idrd_grading trainer.gpus =[ 2 ] ft_idrd_segmentation ft_idrd_localization python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_idrd_localization trainer.gpus =[ 0 ] ft_mnist_grading python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_mnist_grading trainer.gpus =[ 2 ]","title":"commands"},{"location":"docs/commands/#commands","text":"these are the commands to run the experiments on the jku server","title":"commands"},{"location":"docs/commands/#diabetic-retinopathy-desease-grading-supervised-baseline","text":"ImageNet pre-trained Resnet18 42 epochs, strong aug, batch_size 42 focal loss gamma 2 balanced loader python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 datamodule.batch_size = 42 lightning_module/loss = focal +datamodule.balanced_sampling = True logger.run_name = balanced_resnet18_focal_loss_strong_aug_42_epochs save_model = \"pretrained_resnet18_focal_strong_aug_balanced.pt\" trainer.gpus =[ 1 ] ImageNet pre-trained Resnet18 42 epochs, strong aug, batch_size 42 python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 datamodule.batch_size = 42 lightning_module/loss = weighted_cross_entropy logger.run_name = resnet18_strong_aug_42_epochs_weighted save_model = \"pretrained_resnet18_strong_aug_weighted.pt\" trainer.gpus =[ 2 ] ImageNet pre-trained Resnet18 42 epochs, strong aug, batch_size 42 regression python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 model.num_output_units = 1 datamodule.batch_size = 42 lightning_module = disease_grading_regression lightning_module/loss = mse logger.run_name = resnet18_regression_strong_aug_42_epochs save_model = \"pretrained_resnet18_regression_strong_aug.pt\" trainer.gpus =[ 1 ] RetinaMNIST ImageNet pre-trained Resnet18 1500 epochs, strong aug, batch_size 42 regression image_size=56 python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 model.num_output_units = 1 datamodule = retina_mnist datamodule.batch_size = 42 lightning_module = disease_grading_regression lightning_module/loss = mse lightning_module.num_train_samples = 1080 transforms@train_transforms = strong_normalize image_size = 56 logger.experiment_name = retina_mnist_disease_grading logger.run_name = resnet18_regression_strong_aug_1500_epochs_56imagesize save_model = \"retinamnist_pretrained_resnet18_regression_strong_aug_56imagesize.pt\" trainer.gpus =[ 0 ] IDRD ImageNet pre-trained Resnet18 4000 epochs, strong aug, batch_size 42 regression python ophthalmology/scripts/train_disease_grading.py environment = jku_ssd model = resnet18 model.num_output_units = 1 datamodule = indian_diabetic_retinopathy datamodule.batch_size = 42 lightning_module = disease_grading_regression lightning_module/loss = mse lightning_module.num_train_samples = 371 logger.experiment_name = indian_disease_grading logger.run_name = resnet18_regression_strong_aug_4000_epochs save_model = \"indian_pretrained_resnet18_regression_strong_aug.pt\" trainer.gpus =[ 1 ]","title":"diabetic retinopathy desease grading - supervised baseline"},{"location":"docs/commands/#diabetic-retinopathycontrastive-pre-training","text":"ImageNet pre-trained Resnet50 64 epochs, strong aug, batch_size 42 image size 256, balanced loader python ophthalmology/scripts/train_simclr.py environment = jku_ssd datamodule.batch_size = 42 trainer.max_epochs = 64 transforms@ssl_transforms = strong_normalize logger.run_name = balanced_pretrained_strong_aug_64_epochs save_model = \"pretrained_resnet50backbone_256image_balanced_strong_aug.pt\" trainer.gpus =[ 0 ]","title":"diabetic retinopathycontrastive pre-training"},{"location":"docs/commands/#supervised-baselines","text":"","title":"Supervised Baselines"},{"location":"docs/commands/#sup_drd_grading","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_drd_grading trainer.gpus =[ 1 ]","title":"sup_drd_grading"},{"location":"docs/commands/#sup_idrd_grading","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_idrd_grading trainer.gpus =[ 3 ]","title":"sup_idrd_grading"},{"location":"docs/commands/#sup_idrd_segmentation","text":"","title":"sup_idrd_segmentation"},{"location":"docs/commands/#sup_idrd_localization","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_idrd_localization trainer.gpus =[ 0 ]","title":"sup_idrd_localization"},{"location":"docs/commands/#sup_mnist_grading","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = sup_mnist_grading trainer.gpus =[ 2 ]","title":"sup_mnist_grading"},{"location":"docs/commands/#contrastive-pre-training","text":"","title":"Contrastive Pre-Training"},{"location":"docs/commands/#simclr_drd","text":"","title":"simclr_drd"},{"location":"docs/commands/#supervised-fine-tuning","text":"","title":"Supervised Fine-tuning"},{"location":"docs/commands/#ft_drd_grading","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_drd_grading trainer.gpus =[ 3 ]","title":"ft_drd_grading"},{"location":"docs/commands/#ft_idrd_grading","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_idrd_grading trainer.gpus =[ 2 ]","title":"ft_idrd_grading"},{"location":"docs/commands/#ft_idrd_segmentation","text":"","title":"ft_idrd_segmentation"},{"location":"docs/commands/#ft_idrd_localization","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_idrd_localization trainer.gpus =[ 0 ]","title":"ft_idrd_localization"},{"location":"docs/commands/#ft_mnist_grading","text":"python ophthalmology/scripts/train.py environment = jku_ssd experiment = ft_mnist_grading trainer.gpus =[ 2 ]","title":"ft_mnist_grading"},{"location":"docs/datasets/","text":"Datasets Three datasets from retina scans fundus images were considered: * Diabetic Retinopathy Detection - eyePACS * Indian Diabetic Retinopathy Image Dataset - IDRiD * MedMNIST v2 - RetinaMNIST Datasets Diabetic Retinopathy - Problem Diabetic Retinopathy Detection - eyePACS Indian Diabetic Retinopathy Image Dataset - IDRiD MedMNIST v2 - RetinaMNIST Diabetic Retinopathy - Problem Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people. The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment. Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment. Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient. The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible \u2013 ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection. introduction taken from: https://www.kaggle.com/c/diabetic-retinopathy-detection/overview Diabetic Retinopathy Detection - eyePACS link: https://www.kaggle.com/c/diabetic-retinopathy-detection/data dataset statistic value image size width: 4752, height: 3168 number of training samples 35126 number of testing samples 53575 image sample: @dataset { eyepacs , author = {Misra, Rishabh} , year = {2018} , month = {06} , pages = {} , title = {News Category Dataset} , doi = {10.13140/RG.2.2.20331.18729} } train samples with \"strong\" augmentation settings: Indian Diabetic Retinopathy Image Dataset - IDRiD link: https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid image sample: @data { idrid , doi = {10.21227/H25W98} , url = {https://dx.doi.org/10.21227/H25W98} , author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice} , publisher = {IEEE Dataport} , title = {Indian Diabetic Retinopathy Image Dataset (IDRiD)} , year = {2018} } train samples with \"strong\" augmentation settings: MedMNIST v2 - RetinaMNIST link: https://medmnist.com/ dataset statistic value image size width: 28, height: 28 number of training samples 1080 number of validation samples 120 number of testing samples 400 image sample: @article { medmnistv2 , title = {MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification} , author = {Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing} , journal = {arXiv preprint arXiv:2110.14795} , year = {2021} } train samples with \"strong\" augmentation settings:","title":"Datasets"},{"location":"docs/datasets/#datasets","text":"Three datasets from retina scans fundus images were considered: * Diabetic Retinopathy Detection - eyePACS * Indian Diabetic Retinopathy Image Dataset - IDRiD * MedMNIST v2 - RetinaMNIST Datasets Diabetic Retinopathy - Problem Diabetic Retinopathy Detection - eyePACS Indian Diabetic Retinopathy Image Dataset - IDRiD MedMNIST v2 - RetinaMNIST","title":"Datasets"},{"location":"docs/datasets/#diabetic-retinopathy-problem","text":"Diabetic retinopathy is the leading cause of blindness in the working-age population of the developed world. It is estimated to affect over 93 million people. The US Center for Disease Control and Prevention estimates that 29.1 million people in the US have diabetes and the World Health Organization estimates that 347 million people have the disease worldwide. Diabetic Retinopathy (DR) is an eye disease associated with long-standing diabetes. Around 40% to 45% of Americans with diabetes have some stage of the disease. Progression to vision impairment can be slowed or averted if DR is detected in time, however this can be difficult as the disease often shows few symptoms until it is too late to provide effective treatment. Currently, detecting DR is a time-consuming and manual process that requires a trained clinician to examine and evaluate digital color fundus photographs of the retina. By the time human readers submit their reviews, often a day or two later, the delayed results lead to lost follow up, miscommunication, and delayed treatment. Clinicians can identify DR by the presence of lesions associated with the vascular abnormalities caused by the disease. While this approach is effective, its resource demands are high. The expertise and equipment required are often lacking in areas where the rate of diabetes in local populations is high and DR detection is most needed. As the number of individuals with diabetes continues to grow, the infrastructure needed to prevent blindness due to DR will become even more insufficient. The need for a comprehensive and automated method of DR screening has long been recognized, and previous efforts have made good progress using image classification, pattern recognition, and machine learning. With color fundus photography as input, the goal of this competition is to push an automated detection system to the limit of what is possible \u2013 ideally resulting in models with realistic clinical potential. The winning models will be open sourced to maximize the impact such a model can have on improving DR detection. introduction taken from: https://www.kaggle.com/c/diabetic-retinopathy-detection/overview","title":"Diabetic Retinopathy - Problem"},{"location":"docs/datasets/#diabetic-retinopathy-detection-eyepacs","text":"link: https://www.kaggle.com/c/diabetic-retinopathy-detection/data dataset statistic value image size width: 4752, height: 3168 number of training samples 35126 number of testing samples 53575 image sample: @dataset { eyepacs , author = {Misra, Rishabh} , year = {2018} , month = {06} , pages = {} , title = {News Category Dataset} , doi = {10.13140/RG.2.2.20331.18729} } train samples with \"strong\" augmentation settings:","title":"Diabetic Retinopathy Detection - eyePACS"},{"location":"docs/datasets/#indian-diabetic-retinopathy-image-dataset-idrid","text":"link: https://ieee-dataport.org/open-access/indian-diabetic-retinopathy-image-dataset-idrid image sample: @data { idrid , doi = {10.21227/H25W98} , url = {https://dx.doi.org/10.21227/H25W98} , author = {Porwal, Prasanna and Pachade, Samiksha and Kamble, Ravi and Kokare, Manesh and Deshmukh, Girish and Sahasrabuddhe, Vivek and Meriaudeau, Fabrice} , publisher = {IEEE Dataport} , title = {Indian Diabetic Retinopathy Image Dataset (IDRiD)} , year = {2018} } train samples with \"strong\" augmentation settings:","title":"Indian Diabetic Retinopathy Image Dataset - IDRiD"},{"location":"docs/datasets/#medmnist-v2-retinamnist","text":"link: https://medmnist.com/ dataset statistic value image size width: 28, height: 28 number of training samples 1080 number of validation samples 120 number of testing samples 400 image sample: @article { medmnistv2 , title = {MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification} , author = {Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing} , journal = {arXiv preprint arXiv:2110.14795} , year = {2021} } train samples with \"strong\" augmentation settings:","title":"MedMNIST v2 - RetinaMNIST"},{"location":"docs/experiments/","text":"Supervised Baselines sup_drd_grading sup_idrd_grading sup_idrd_segmentation sup_idrd_localization sup_mnist_grading Contrastive Pre-Training simclr_drd Supervised Fine-tuning ft_drd_grading ft_idrd_grading ft_idrd_segmentation ft_idrd_localization ft_mnist_grading","title":"Experiments"},{"location":"docs/experiments/#supervised-baselines","text":"sup_drd_grading sup_idrd_grading sup_idrd_segmentation sup_idrd_localization sup_mnist_grading","title":"Supervised Baselines"},{"location":"docs/experiments/#contrastive-pre-training","text":"simclr_drd","title":"Contrastive Pre-Training"},{"location":"docs/experiments/#supervised-fine-tuning","text":"ft_drd_grading ft_idrd_grading ft_idrd_segmentation ft_idrd_localization ft_mnist_grading","title":"Supervised Fine-tuning"},{"location":"reference/ophthalmology/","text":"Module ophthalmology \ud83d\udc41 - Contrastive Learning for Ophthalmology - \ud83d\udc41 Documentation and implementations for the experiments conducted for the Seminar in AI at the JKU university in Linz 2021/2022. Sub-modules ophthalmology.callbacks ophthalmology.data ophthalmology.layers ophthalmology.models ophthalmology.modules ophthalmology.samplers ophthalmology.utils ophthalmology.visualization","title":"Index"},{"location":"reference/ophthalmology/#module-ophthalmology","text":"","title":"Module ophthalmology"},{"location":"reference/ophthalmology/#-contrastive-learning-for-ophthalmology-","text":"Documentation and implementations for the experiments conducted for the Seminar in AI at the JKU university in Linz 2021/2022.","title":"\ud83d\udc41 - Contrastive Learning for Ophthalmology - \ud83d\udc41"},{"location":"reference/ophthalmology/#sub-modules","text":"ophthalmology.callbacks ophthalmology.data ophthalmology.layers ophthalmology.models ophthalmology.modules ophthalmology.samplers ophthalmology.utils ophthalmology.visualization","title":"Sub-modules"},{"location":"reference/ophthalmology/callbacks/","text":"Module ophthalmology.callbacks Pytorch Lightning Callbacks None Classes LogDataSamplesCallback class LogDataSamplesCallback ( dataset : torch . utils . data . dataset . Dataset , rows : int = 4 ) Ancestors (in MRO) pytorch_lightning.callbacks.base.Callback abc.ABC Instance variables state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback. Methods on_after_backward def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped. on_batch_end def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends. on_batch_start def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins. on_before_accelerator_backend_setup def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup. on_before_backward def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() . on_before_optimizer_step def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() . on_before_zero_grad def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() . on_configure_sharded_model def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model. on_epoch_end def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins. on_exception def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception. on_fit_end def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends. on_fit_start def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins. on_init_end def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set. on_init_start def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set. on_keyboard_interrupt def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None on_load_checkpoint def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state . on_predict_batch_end def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends. on_predict_batch_start def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins. on_predict_end def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends. on_predict_epoch_end def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends. on_predict_epoch_start def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins. on_predict_start def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins. on_pretrain_routine_end def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends. on_pretrain_routine_start def on_pretrain_routine_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine begins. on_sanity_check_end def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends. on_sanity_check_start def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts. on_save_checkpoint def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state. on_test_batch_end def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends. on_test_batch_start def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins. on_test_end def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends. on_test_epoch_end def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends. on_test_epoch_start def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins. on_test_start def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins. on_train_batch_end def on_train_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch ends. on_train_batch_start def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins. on_train_end def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends. on_train_epoch_end def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook. on_train_epoch_start def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins. on_train_start def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins. on_validation_batch_end def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends. on_validation_batch_start def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins. on_validation_end def on_validation_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop ends. on_validation_epoch_end def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch ends. on_validation_epoch_start def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins. on_validation_start def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins. setup def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins. teardown def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends. LogSignalPropagationPlotCallback class LogSignalPropagationPlotCallback ( input_shape : List [ int ] = [ 64 , 3 , 256 , 256 ] ) Ancestors (in MRO) pytorch_lightning.callbacks.base.Callback abc.ABC Static methods average_channel_squared_mean def average_channel_squared_mean ( x ) average_channel_variance def average_channel_variance ( x ) extract_activations def extract_activations ( model , * args , ** kwargs ) get_average_channel_squared_mean_by_depth def get_average_channel_squared_mean_by_depth ( model , * args , ** kwargs ) get_average_channel_variance_by_depth def get_average_channel_variance_by_depth ( model , * args , ** kwargs ) Instance variables state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback. Methods hook def hook ( self , input , output , store = None , name = None ) on_after_backward def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped. on_batch_end def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends. on_batch_start def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins. on_before_accelerator_backend_setup def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup. on_before_backward def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() . on_before_optimizer_step def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() . on_before_zero_grad def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() . on_configure_sharded_model def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model. on_epoch_end def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins. on_exception def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception. on_fit_end def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends. on_fit_start def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins. on_init_end def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set. on_init_start def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set. on_keyboard_interrupt def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None on_load_checkpoint def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state . on_predict_batch_end def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends. on_predict_batch_start def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins. on_predict_end def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends. on_predict_epoch_end def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends. on_predict_epoch_start def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins. on_predict_start def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins. on_pretrain_routine_end def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends. on_pretrain_routine_start def on_pretrain_routine_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine begins. on_sanity_check_end def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends. on_sanity_check_start def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts. on_save_checkpoint def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state. on_test_batch_end def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends. on_test_batch_start def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins. on_test_end def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends. on_test_epoch_end def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends. on_test_epoch_start def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins. on_test_start def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins. on_train_batch_end def on_train_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch ends. on_train_batch_start def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins. on_train_end def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends. on_train_epoch_end def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook. on_train_epoch_start def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins. on_train_start def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins. on_validation_batch_end def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends. on_validation_batch_start def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins. on_validation_end def on_validation_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop ends. on_validation_epoch_end def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch ends. on_validation_epoch_start def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins. on_validation_start def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins. setup def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins. teardown def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends. OptunaPruningCallback class OptunaPruningCallback ( trial : optuna . trial . _trial . Trial , monitor : str ) Attributes Name Type Description Default trial None A :class: ~optuna.trial.Trial corresponding to the current evaluation of the objective function. None monitor None An evaluation metric for pruning, e.g., val_loss or val_acc . The metrics are obtained from the returned dictionaries from e.g. pytorch_lightning.LightningModule.training_step or pytorch_lightning.LightningModule.validation_epoch_end and the names thus depend on how this dictionary is formatted. None Ancestors (in MRO) pytorch_lightning.callbacks.base.Callback abc.ABC Instance variables state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback. Methods on_after_backward def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped. on_batch_end def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends. on_batch_start def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins. on_before_accelerator_backend_setup def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup. on_before_backward def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() . on_before_optimizer_step def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() . on_before_zero_grad def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() . on_configure_sharded_model def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model. on_epoch_end def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins. on_exception def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception. on_fit_end def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends. on_fit_start def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins. on_init_end def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set. on_init_start def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set. on_keyboard_interrupt def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None on_load_checkpoint def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state . on_predict_batch_end def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends. on_predict_batch_start def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins. on_predict_end def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends. on_predict_epoch_end def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends. on_predict_epoch_start def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins. on_predict_start def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins. on_pretrain_routine_end def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends. on_pretrain_routine_start def on_pretrain_routine_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine begins. on_sanity_check_end def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends. on_sanity_check_start def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts. on_save_checkpoint def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state. on_test_batch_end def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends. on_test_batch_start def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins. on_test_end def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends. on_test_epoch_end def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends. on_test_epoch_start def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins. on_test_start def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins. on_train_batch_end def on_train_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch ends. on_train_batch_start def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins. on_train_end def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends. on_train_epoch_end def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook. on_train_epoch_start def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins. on_train_start def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins. on_validation_batch_end def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends. on_validation_batch_start def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins. on_validation_end def on_validation_end ( self , trainer : pytorch_lightning . trainer . trainer . Trainer , pl_module : pytorch_lightning . core . lightning . LightningModule ) -> None Called when the validation loop ends. on_validation_epoch_end def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch ends. on_validation_epoch_start def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins. on_validation_start def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins. setup def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins. teardown def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends. SSLOnlineEvaluator class SSLOnlineEvaluator ( data_loader : torch . utils . data . dataloader . DataLoader , drop_p : float = 0.2 , hidden_dim : Optional [ int ] = None , z_dim : int = None , num_classes : int = None ) Ancestors (in MRO) pytorch_lightning.callbacks.base.Callback abc.ABC Instance variables state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback. Methods on_after_backward def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped. on_batch_end def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends. on_batch_start def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins. on_before_accelerator_backend_setup def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup. on_before_backward def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() . on_before_optimizer_step def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() . on_before_zero_grad def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() . on_configure_sharded_model def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model. on_epoch_end def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins. on_exception def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception. on_fit_end def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends. on_fit_start def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins. on_init_end def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set. on_init_start def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set. on_keyboard_interrupt def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None on_load_checkpoint def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state . on_predict_batch_end def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends. on_predict_batch_start def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins. on_predict_end def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends. on_predict_epoch_end def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends. on_predict_epoch_start def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins. on_predict_start def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins. on_pretrain_routine_end def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends. on_pretrain_routine_start def on_pretrain_routine_start ( self , trainer : pytorch_lightning . trainer . trainer . Trainer , pl_module : pytorch_lightning . core . lightning . LightningModule ) -> None Called when the pretrain routine begins. on_sanity_check_end def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends. on_sanity_check_start def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts. on_save_checkpoint def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state. on_test_batch_end def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends. on_test_batch_start def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins. on_test_end def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends. on_test_epoch_end def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends. on_test_epoch_start def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins. on_test_start def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins. on_train_batch_end def on_train_batch_end ( self , trainer_ : pytorch_lightning . trainer . trainer . Trainer , pl_module : pytorch_lightning . core . lightning . LightningModule , outputs_ : Sequence , batch_ : Sequence , batch_idx_ : int ) -> None Called when the train batch ends. on_train_batch_start def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins. on_train_end def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends. on_train_epoch_end def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook. on_train_epoch_start def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins. on_train_start def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins. on_validation_batch_end def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends. on_validation_batch_start def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins. on_validation_end def on_validation_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop ends. on_validation_epoch_end def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Do One Epoch of finetuning on the head and compute confusion matrix. on_validation_epoch_start def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins. on_validation_start def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins. setup def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins. teardown def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends. to_device def to_device ( self , batch : Sequence , device : Union [ str , torch . device ] ) -> Tuple [ torch . Tensor , torch . Tensor ]","title":"Callbacks"},{"location":"reference/ophthalmology/callbacks/#module-ophthalmologycallbacks","text":"Pytorch Lightning Callbacks None","title":"Module ophthalmology.callbacks"},{"location":"reference/ophthalmology/callbacks/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/callbacks/#logdatasamplescallback","text":"class LogDataSamplesCallback ( dataset : torch . utils . data . dataset . Dataset , rows : int = 4 )","title":"LogDataSamplesCallback"},{"location":"reference/ophthalmology/callbacks/#ancestors-in-mro","text":"pytorch_lightning.callbacks.base.Callback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/callbacks/#instance-variables","text":"state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.","title":"Instance variables"},{"location":"reference/ophthalmology/callbacks/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/callbacks/#on_after_backward","text":"def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped.","title":"on_after_backward"},{"location":"reference/ophthalmology/callbacks/#on_batch_end","text":"def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends.","title":"on_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_batch_start","text":"def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins.","title":"on_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_before_accelerator_backend_setup","text":"def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup.","title":"on_before_accelerator_backend_setup"},{"location":"reference/ophthalmology/callbacks/#on_before_backward","text":"def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() .","title":"on_before_backward"},{"location":"reference/ophthalmology/callbacks/#on_before_optimizer_step","text":"def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() .","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/callbacks/#on_before_zero_grad","text":"def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() .","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/callbacks/#on_configure_sharded_model","text":"def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model.","title":"on_configure_sharded_model"},{"location":"reference/ophthalmology/callbacks/#on_epoch_end","text":"def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_epoch_start","text":"def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_exception","text":"def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception.","title":"on_exception"},{"location":"reference/ophthalmology/callbacks/#on_fit_end","text":"def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends.","title":"on_fit_end"},{"location":"reference/ophthalmology/callbacks/#on_fit_start","text":"def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins.","title":"on_fit_start"},{"location":"reference/ophthalmology/callbacks/#on_init_end","text":"def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set.","title":"on_init_end"},{"location":"reference/ophthalmology/callbacks/#on_init_start","text":"def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set.","title":"on_init_start"},{"location":"reference/ophthalmology/callbacks/#on_keyboard_interrupt","text":"def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None","title":"on_keyboard_interrupt"},{"location":"reference/ophthalmology/callbacks/#on_load_checkpoint","text":"def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state .","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_end","text":"def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends.","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_start","text":"def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins.","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_end","text":"def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends.","title":"on_predict_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_end","text":"def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_start","text":"def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_start","text":"def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins.","title":"on_predict_start"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_end","text":"def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends.","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_start","text":"def on_pretrain_routine_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine begins.","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_end","text":"def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends.","title":"on_sanity_check_end"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_start","text":"def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts.","title":"on_sanity_check_start"},{"location":"reference/ophthalmology/callbacks/#on_save_checkpoint","text":"def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_end","text":"def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends.","title":"on_test_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_start","text":"def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins.","title":"on_test_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_end","text":"def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends.","title":"on_test_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_end","text":"def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_start","text":"def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_start","text":"def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins.","title":"on_test_start"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_end","text":"def on_train_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch ends.","title":"on_train_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_start","text":"def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins.","title":"on_train_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_end","text":"def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends.","title":"on_train_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_end","text":"def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook.","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_start","text":"def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_start","text":"def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins.","title":"on_train_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_end","text":"def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends.","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_start","text":"def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins.","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_end","text":"def on_validation_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop ends.","title":"on_validation_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_end","text":"def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch ends.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_start","text":"def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_start","text":"def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins.","title":"on_validation_start"},{"location":"reference/ophthalmology/callbacks/#setup","text":"def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins.","title":"setup"},{"location":"reference/ophthalmology/callbacks/#teardown","text":"def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends.","title":"teardown"},{"location":"reference/ophthalmology/callbacks/#logsignalpropagationplotcallback","text":"class LogSignalPropagationPlotCallback ( input_shape : List [ int ] = [ 64 , 3 , 256 , 256 ] )","title":"LogSignalPropagationPlotCallback"},{"location":"reference/ophthalmology/callbacks/#ancestors-in-mro_1","text":"pytorch_lightning.callbacks.base.Callback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/callbacks/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/callbacks/#average_channel_squared_mean","text":"def average_channel_squared_mean ( x )","title":"average_channel_squared_mean"},{"location":"reference/ophthalmology/callbacks/#average_channel_variance","text":"def average_channel_variance ( x )","title":"average_channel_variance"},{"location":"reference/ophthalmology/callbacks/#extract_activations","text":"def extract_activations ( model , * args , ** kwargs )","title":"extract_activations"},{"location":"reference/ophthalmology/callbacks/#get_average_channel_squared_mean_by_depth","text":"def get_average_channel_squared_mean_by_depth ( model , * args , ** kwargs )","title":"get_average_channel_squared_mean_by_depth"},{"location":"reference/ophthalmology/callbacks/#get_average_channel_variance_by_depth","text":"def get_average_channel_variance_by_depth ( model , * args , ** kwargs )","title":"get_average_channel_variance_by_depth"},{"location":"reference/ophthalmology/callbacks/#instance-variables_1","text":"state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.","title":"Instance variables"},{"location":"reference/ophthalmology/callbacks/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/callbacks/#hook","text":"def hook ( self , input , output , store = None , name = None )","title":"hook"},{"location":"reference/ophthalmology/callbacks/#on_after_backward_1","text":"def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped.","title":"on_after_backward"},{"location":"reference/ophthalmology/callbacks/#on_batch_end_1","text":"def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends.","title":"on_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_batch_start_1","text":"def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins.","title":"on_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_before_accelerator_backend_setup_1","text":"def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup.","title":"on_before_accelerator_backend_setup"},{"location":"reference/ophthalmology/callbacks/#on_before_backward_1","text":"def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() .","title":"on_before_backward"},{"location":"reference/ophthalmology/callbacks/#on_before_optimizer_step_1","text":"def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() .","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/callbacks/#on_before_zero_grad_1","text":"def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() .","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/callbacks/#on_configure_sharded_model_1","text":"def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model.","title":"on_configure_sharded_model"},{"location":"reference/ophthalmology/callbacks/#on_epoch_end_1","text":"def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_epoch_start_1","text":"def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_exception_1","text":"def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception.","title":"on_exception"},{"location":"reference/ophthalmology/callbacks/#on_fit_end_1","text":"def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends.","title":"on_fit_end"},{"location":"reference/ophthalmology/callbacks/#on_fit_start_1","text":"def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins.","title":"on_fit_start"},{"location":"reference/ophthalmology/callbacks/#on_init_end_1","text":"def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set.","title":"on_init_end"},{"location":"reference/ophthalmology/callbacks/#on_init_start_1","text":"def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set.","title":"on_init_start"},{"location":"reference/ophthalmology/callbacks/#on_keyboard_interrupt_1","text":"def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None","title":"on_keyboard_interrupt"},{"location":"reference/ophthalmology/callbacks/#on_load_checkpoint_1","text":"def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state .","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_end_1","text":"def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends.","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_start_1","text":"def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins.","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_end_1","text":"def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends.","title":"on_predict_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_end_1","text":"def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_start_1","text":"def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_start_1","text":"def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins.","title":"on_predict_start"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_end_1","text":"def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends.","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_start_1","text":"def on_pretrain_routine_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine begins.","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_end_1","text":"def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends.","title":"on_sanity_check_end"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_start_1","text":"def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts.","title":"on_sanity_check_start"},{"location":"reference/ophthalmology/callbacks/#on_save_checkpoint_1","text":"def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_end_1","text":"def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends.","title":"on_test_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_start_1","text":"def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins.","title":"on_test_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_end_1","text":"def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends.","title":"on_test_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_end_1","text":"def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_start_1","text":"def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_start_1","text":"def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins.","title":"on_test_start"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_end_1","text":"def on_train_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch ends.","title":"on_train_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_start_1","text":"def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins.","title":"on_train_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_end_1","text":"def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends.","title":"on_train_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_end_1","text":"def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook.","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_start_1","text":"def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_start_1","text":"def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins.","title":"on_train_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_end_1","text":"def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends.","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_start_1","text":"def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins.","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_end_1","text":"def on_validation_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop ends.","title":"on_validation_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_end_1","text":"def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch ends.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_start_1","text":"def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_start_1","text":"def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins.","title":"on_validation_start"},{"location":"reference/ophthalmology/callbacks/#setup_1","text":"def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins.","title":"setup"},{"location":"reference/ophthalmology/callbacks/#teardown_1","text":"def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends.","title":"teardown"},{"location":"reference/ophthalmology/callbacks/#optunapruningcallback","text":"class OptunaPruningCallback ( trial : optuna . trial . _trial . Trial , monitor : str )","title":"OptunaPruningCallback"},{"location":"reference/ophthalmology/callbacks/#attributes","text":"Name Type Description Default trial None A :class: ~optuna.trial.Trial corresponding to the current evaluation of the objective function. None monitor None An evaluation metric for pruning, e.g., val_loss or val_acc . The metrics are obtained from the returned dictionaries from e.g. pytorch_lightning.LightningModule.training_step or pytorch_lightning.LightningModule.validation_epoch_end and the names thus depend on how this dictionary is formatted. None","title":"Attributes"},{"location":"reference/ophthalmology/callbacks/#ancestors-in-mro_2","text":"pytorch_lightning.callbacks.base.Callback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/callbacks/#instance-variables_2","text":"state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.","title":"Instance variables"},{"location":"reference/ophthalmology/callbacks/#methods_2","text":"","title":"Methods"},{"location":"reference/ophthalmology/callbacks/#on_after_backward_2","text":"def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped.","title":"on_after_backward"},{"location":"reference/ophthalmology/callbacks/#on_batch_end_2","text":"def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends.","title":"on_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_batch_start_2","text":"def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins.","title":"on_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_before_accelerator_backend_setup_2","text":"def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup.","title":"on_before_accelerator_backend_setup"},{"location":"reference/ophthalmology/callbacks/#on_before_backward_2","text":"def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() .","title":"on_before_backward"},{"location":"reference/ophthalmology/callbacks/#on_before_optimizer_step_2","text":"def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() .","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/callbacks/#on_before_zero_grad_2","text":"def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() .","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/callbacks/#on_configure_sharded_model_2","text":"def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model.","title":"on_configure_sharded_model"},{"location":"reference/ophthalmology/callbacks/#on_epoch_end_2","text":"def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_epoch_start_2","text":"def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_exception_2","text":"def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception.","title":"on_exception"},{"location":"reference/ophthalmology/callbacks/#on_fit_end_2","text":"def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends.","title":"on_fit_end"},{"location":"reference/ophthalmology/callbacks/#on_fit_start_2","text":"def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins.","title":"on_fit_start"},{"location":"reference/ophthalmology/callbacks/#on_init_end_2","text":"def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set.","title":"on_init_end"},{"location":"reference/ophthalmology/callbacks/#on_init_start_2","text":"def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set.","title":"on_init_start"},{"location":"reference/ophthalmology/callbacks/#on_keyboard_interrupt_2","text":"def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None","title":"on_keyboard_interrupt"},{"location":"reference/ophthalmology/callbacks/#on_load_checkpoint_2","text":"def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state .","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_end_2","text":"def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends.","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_start_2","text":"def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins.","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_end_2","text":"def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends.","title":"on_predict_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_end_2","text":"def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_start_2","text":"def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_start_2","text":"def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins.","title":"on_predict_start"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_end_2","text":"def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends.","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_start_2","text":"def on_pretrain_routine_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine begins.","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_end_2","text":"def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends.","title":"on_sanity_check_end"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_start_2","text":"def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts.","title":"on_sanity_check_start"},{"location":"reference/ophthalmology/callbacks/#on_save_checkpoint_2","text":"def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_end_2","text":"def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends.","title":"on_test_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_start_2","text":"def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins.","title":"on_test_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_end_2","text":"def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends.","title":"on_test_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_end_2","text":"def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_start_2","text":"def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_start_2","text":"def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins.","title":"on_test_start"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_end_2","text":"def on_train_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch ends.","title":"on_train_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_start_2","text":"def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins.","title":"on_train_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_end_2","text":"def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends.","title":"on_train_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_end_2","text":"def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook.","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_start_2","text":"def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_start_2","text":"def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins.","title":"on_train_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_end_2","text":"def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends.","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_start_2","text":"def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins.","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_end_2","text":"def on_validation_end ( self , trainer : pytorch_lightning . trainer . trainer . Trainer , pl_module : pytorch_lightning . core . lightning . LightningModule ) -> None Called when the validation loop ends.","title":"on_validation_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_end_2","text":"def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch ends.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_start_2","text":"def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_start_2","text":"def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins.","title":"on_validation_start"},{"location":"reference/ophthalmology/callbacks/#setup_2","text":"def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins.","title":"setup"},{"location":"reference/ophthalmology/callbacks/#teardown_2","text":"def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends.","title":"teardown"},{"location":"reference/ophthalmology/callbacks/#sslonlineevaluator","text":"class SSLOnlineEvaluator ( data_loader : torch . utils . data . dataloader . DataLoader , drop_p : float = 0.2 , hidden_dim : Optional [ int ] = None , z_dim : int = None , num_classes : int = None )","title":"SSLOnlineEvaluator"},{"location":"reference/ophthalmology/callbacks/#ancestors-in-mro_3","text":"pytorch_lightning.callbacks.base.Callback abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/callbacks/#instance-variables_3","text":"state_key Identifier for the state of the callback. Used to store and retrieve a callback's state from the checkpoint dictionary by checkpoint[\"callbacks\"][state_key] . Implementations of a callback need to provide a unique state key if 1) the callback has state and 2) it is desired to maintain the state of multiple instances of that callback.","title":"Instance variables"},{"location":"reference/ophthalmology/callbacks/#methods_3","text":"","title":"Methods"},{"location":"reference/ophthalmology/callbacks/#on_after_backward_3","text":"def on_after_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called after loss.backward() and before optimizers are stepped.","title":"on_after_backward"},{"location":"reference/ophthalmology/callbacks/#on_batch_end_3","text":"def on_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch ends.","title":"on_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_batch_start_3","text":"def on_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the training batch begins.","title":"on_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_before_accelerator_backend_setup_3","text":"def on_before_accelerator_backend_setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before accelerator is being setup.","title":"on_before_accelerator_backend_setup"},{"location":"reference/ophthalmology/callbacks/#on_before_backward_3","text":"def on_before_backward ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , loss : torch . Tensor ) -> None Called before loss.backward() .","title":"on_before_backward"},{"location":"reference/ophthalmology/callbacks/#on_before_optimizer_step_3","text":"def on_before_optimizer_step ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer , opt_idx : int ) -> None Called before optimizer.step() .","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/callbacks/#on_before_zero_grad_3","text":"def on_before_zero_grad ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , optimizer : torch . optim . optimizer . Optimizer ) -> None Called before optimizer.zero_grad() .","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/callbacks/#on_configure_sharded_model_3","text":"def on_configure_sharded_model ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called before configure sharded model.","title":"on_configure_sharded_model"},{"location":"reference/ophthalmology/callbacks/#on_epoch_end_3","text":"def on_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_epoch_start_3","text":"def on_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_exception_3","text":"def on_exception ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , exception : BaseException ) -> None Called when any trainer execution is interrupted by an exception.","title":"on_exception"},{"location":"reference/ophthalmology/callbacks/#on_fit_end_3","text":"def on_fit_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit ends.","title":"on_fit_end"},{"location":"reference/ophthalmology/callbacks/#on_fit_start_3","text":"def on_fit_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when fit begins.","title":"on_fit_start"},{"location":"reference/ophthalmology/callbacks/#on_init_end_3","text":"def on_init_end ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization ends, model has not yet been set.","title":"on_init_end"},{"location":"reference/ophthalmology/callbacks/#on_init_start_3","text":"def on_init_start ( self , trainer : 'pl.Trainer' ) -> None Called when the trainer initialization begins, model has not yet been set.","title":"on_init_start"},{"location":"reference/ophthalmology/callbacks/#on_keyboard_interrupt_3","text":"def on_keyboard_interrupt ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None","title":"on_keyboard_interrupt"},{"location":"reference/ophthalmology/callbacks/#on_load_checkpoint_3","text":"def on_load_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , callback_state : Dict [ str , Any ] ) -> None Called when loading a model checkpoint, use to reload state. Args: trainer: the current :class: ~pytorch_lightning.trainer.Trainer instance. pl_module: the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. callback_state: the callback state returned by on_save_checkpoint . Note: The on_load_checkpoint won't be called with an undefined state. If your on_load_checkpoint hook behavior doesn't rely on a state, you will still need to override on_save_checkpoint to return a dummy state .","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_end_3","text":"def on_predict_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Any , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch ends.","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_batch_start_3","text":"def on_predict_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the predict batch begins.","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_end_3","text":"def on_predict_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when predict ends.","title":"on_predict_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_end_3","text":"def on_predict_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : List [ Any ] ) -> None Called when the predict epoch ends.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_predict_epoch_start_3","text":"def on_predict_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict epoch begins.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_predict_start_3","text":"def on_predict_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the predict begins.","title":"on_predict_start"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_end_3","text":"def on_pretrain_routine_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the pretrain routine ends.","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/callbacks/#on_pretrain_routine_start_3","text":"def on_pretrain_routine_start ( self , trainer : pytorch_lightning . trainer . trainer . Trainer , pl_module : pytorch_lightning . core . lightning . LightningModule ) -> None Called when the pretrain routine begins.","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_end_3","text":"def on_sanity_check_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check ends.","title":"on_sanity_check_end"},{"location":"reference/ophthalmology/callbacks/#on_sanity_check_start_3","text":"def on_sanity_check_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation sanity check starts.","title":"on_sanity_check_start"},{"location":"reference/ophthalmology/callbacks/#on_save_checkpoint_3","text":"def on_save_checkpoint ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , checkpoint : Dict [ str , Any ] ) -> dict Called when saving a model checkpoint, use to persist state. Parameters: Name Type Description Default trainer None the current :class: ~pytorch_lightning.trainer.Trainer instance. None pl_module None the current :class: ~pytorch_lightning.core.lightning.LightningModule instance. None checkpoint None the checkpoint dictionary that will be saved. None Returns: Type Description None The callback state.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_end_3","text":"def on_test_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch ends.","title":"on_test_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_batch_start_3","text":"def on_test_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the test batch begins.","title":"on_test_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_end_3","text":"def on_test_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test ends.","title":"on_test_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_end_3","text":"def on_test_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch ends.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_test_epoch_start_3","text":"def on_test_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test epoch begins.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_test_start_3","text":"def on_test_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the test begins.","title":"on_test_start"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_end_3","text":"def on_train_batch_end ( self , trainer_ : pytorch_lightning . trainer . trainer . Trainer , pl_module : pytorch_lightning . core . lightning . LightningModule , outputs_ : Sequence , batch_ : Sequence , batch_idx_ : int ) -> None Called when the train batch ends.","title":"on_train_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_batch_start_3","text":"def on_train_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called when the train batch begins.","title":"on_train_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_end_3","text":"def on_train_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train ends.","title":"on_train_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_end_3","text":"def on_train_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch ends. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule and access outputs via the module OR Cache data across train batch hooks inside the callback implementation to post-process in this hook.","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_train_epoch_start_3","text":"def on_train_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train epoch begins.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_train_start_3","text":"def on_train_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the train begins.","title":"on_train_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_end_3","text":"def on_validation_batch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch ends.","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_batch_start_3","text":"def on_validation_batch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called when the validation batch begins.","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_end_3","text":"def on_validation_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop ends.","title":"on_validation_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_end_3","text":"def on_validation_epoch_end ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Do One Epoch of finetuning on the head and compute confusion matrix.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/callbacks/#on_validation_epoch_start_3","text":"def on_validation_epoch_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the val epoch begins.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/callbacks/#on_validation_start_3","text":"def on_validation_start ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' ) -> None Called when the validation loop begins.","title":"on_validation_start"},{"location":"reference/ophthalmology/callbacks/#setup_3","text":"def setup ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune begins.","title":"setup"},{"location":"reference/ophthalmology/callbacks/#teardown_3","text":"def teardown ( self , trainer : 'pl.Trainer' , pl_module : 'pl.LightningModule' , stage : Optional [ str ] = None ) -> None Called when fit, validate, test, predict, or tune ends.","title":"teardown"},{"location":"reference/ophthalmology/callbacks/#to_device","text":"def to_device ( self , batch : Sequence , device : Union [ str , torch . device ] ) -> Tuple [ torch . Tensor , torch . Tensor ]","title":"to_device"},{"location":"reference/ophthalmology/samplers/","text":"Module ophthalmology.samplers None None Classes ImbalancedDatasetSampler class ImbalancedDatasetSampler ( dataset , indices : list = None , num_samples : int = None , callback_get_label : Callable = None , seed : int = 42 ) Attributes Name Type Description Default indices None a list of indices None num_samples None number of samples to draw None callback_get_label None a callback-like function which takes two arguments - dataset and index None Ancestors (in MRO) torch.utils.data.sampler.Sampler typing.Generic","title":"Samplers"},{"location":"reference/ophthalmology/samplers/#module-ophthalmologysamplers","text":"None None","title":"Module ophthalmology.samplers"},{"location":"reference/ophthalmology/samplers/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/samplers/#imbalanceddatasetsampler","text":"class ImbalancedDatasetSampler ( dataset , indices : list = None , num_samples : int = None , callback_get_label : Callable = None , seed : int = 42 )","title":"ImbalancedDatasetSampler"},{"location":"reference/ophthalmology/samplers/#attributes","text":"Name Type Description Default indices None a list of indices None num_samples None number of samples to draw None callback_get_label None a callback-like function which takes two arguments - dataset and index None","title":"Attributes"},{"location":"reference/ophthalmology/samplers/#ancestors-in-mro","text":"torch.utils.data.sampler.Sampler typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/utils/","text":"Module ophthalmology.utils Python utilities None Functions log_hyperparameters def log_hyperparameters ( config : omegaconf . dictconfig . DictConfig , extra_fields : List [ str ] = [] ) -> None This method logs some configured hyperparameters to mlflow.","title":"Utils"},{"location":"reference/ophthalmology/utils/#module-ophthalmologyutils","text":"Python utilities None","title":"Module ophthalmology.utils"},{"location":"reference/ophthalmology/utils/#functions","text":"","title":"Functions"},{"location":"reference/ophthalmology/utils/#log_hyperparameters","text":"def log_hyperparameters ( config : omegaconf . dictconfig . DictConfig , extra_fields : List [ str ] = [] ) -> None This method logs some configured hyperparameters to mlflow.","title":"log_hyperparameters"},{"location":"reference/ophthalmology/visualization/","text":"Module ophthalmology.visualization Visualization This module contains functions that produce plots and visualizations needed for logging, data exploration and the final dashboards. Functions plot_confusion_matrix def plot_confusion_matrix ( cm : Union [ < built - in function array > , < built - in method tensor of type object at 0x7fdd3fd031e0 > ], classes : Optional [ List [ str ]] = None , normalize : bool = False , title : str = 'Confusion Matrix' , cmap : Union [ str , matplotlib . colors . Colormap ] = < matplotlib . colors . LinearSegmentedColormap object at 0x7fdd43015760 > ) -> matplotlib . figure . Figure Create a matplotlib confusion matrix plot from a np.array or torch.tensor. Parameters: Name Type Description Default cm np.array torch.tensor Raw Confusion Matrix as np.array or torch.tensor. classes Optional[List[str]] If defined replace class indices on axes with class labels. Defaults to None. None normalize bool If True, Normalize the count of each class to 1 to see percentages instead of absolute counts. Defaults to False. False title str Figure Title. Defaults to \"Confusion Matrix\". \"Confusion Matrix\" cmap [str plt.Colormap Matplotlib colormap. Defaults to plt.cm.Blues. Returns: Type Description plt.Figure [description] visualize_samples_from_dataset def visualize_samples_from_dataset ( dataset : torch . utils . data . dataset . Dataset , rows : int = 5 , undo_normalization : Optional [ Tuple [ List [ float ], List [ float ]]] = ([ 0.3211 , 0.2243 , 0.1602 ], [ 0.2617 , 0.1825 , 0.1308 ]) ) -> matplotlib . figure . Figure Visualize a grid of samples without titles/labels in a single plot. Parameters: Name Type Description Default dataset torch.utils.data.Dataset Dataset to visualize samples from. None rows int How many samples will be in one row. Total number of samples will be rows^2. Defaults to 5. 5 Returns: Type Description plt.Figure matplotlib figure visualize_signal_propagation def visualize_signal_propagation ( name_values : list , title : str , * args , ** kwargs ) -> matplotlib . figure . Figure Visualize Signal Propagation Plot using matplolib and the utilities from the timm package. See: https://github.com/mehdidc/signal_propagation_plot/blob/main/signal_propagation_plot/pytorch.py Parameters: Name Type Description Default name_values torch.nn.Module pytorch model None input_shape List[int] Input Size of the model. None Returns: Type Description plt.Figure matplotlib figure","title":"Visualization"},{"location":"reference/ophthalmology/visualization/#module-ophthalmologyvisualization","text":"","title":"Module ophthalmology.visualization"},{"location":"reference/ophthalmology/visualization/#visualization","text":"This module contains functions that produce plots and visualizations needed for logging, data exploration and the final dashboards.","title":"Visualization"},{"location":"reference/ophthalmology/visualization/#functions","text":"","title":"Functions"},{"location":"reference/ophthalmology/visualization/#plot_confusion_matrix","text":"def plot_confusion_matrix ( cm : Union [ < built - in function array > , < built - in method tensor of type object at 0x7fdd3fd031e0 > ], classes : Optional [ List [ str ]] = None , normalize : bool = False , title : str = 'Confusion Matrix' , cmap : Union [ str , matplotlib . colors . Colormap ] = < matplotlib . colors . LinearSegmentedColormap object at 0x7fdd43015760 > ) -> matplotlib . figure . Figure Create a matplotlib confusion matrix plot from a np.array or torch.tensor. Parameters: Name Type Description Default cm np.array torch.tensor Raw Confusion Matrix as np.array or torch.tensor. classes Optional[List[str]] If defined replace class indices on axes with class labels. Defaults to None. None normalize bool If True, Normalize the count of each class to 1 to see percentages instead of absolute counts. Defaults to False. False title str Figure Title. Defaults to \"Confusion Matrix\". \"Confusion Matrix\" cmap [str plt.Colormap Matplotlib colormap. Defaults to plt.cm.Blues. Returns: Type Description plt.Figure [description]","title":"plot_confusion_matrix"},{"location":"reference/ophthalmology/visualization/#visualize_samples_from_dataset","text":"def visualize_samples_from_dataset ( dataset : torch . utils . data . dataset . Dataset , rows : int = 5 , undo_normalization : Optional [ Tuple [ List [ float ], List [ float ]]] = ([ 0.3211 , 0.2243 , 0.1602 ], [ 0.2617 , 0.1825 , 0.1308 ]) ) -> matplotlib . figure . Figure Visualize a grid of samples without titles/labels in a single plot. Parameters: Name Type Description Default dataset torch.utils.data.Dataset Dataset to visualize samples from. None rows int How many samples will be in one row. Total number of samples will be rows^2. Defaults to 5. 5 Returns: Type Description plt.Figure matplotlib figure","title":"visualize_samples_from_dataset"},{"location":"reference/ophthalmology/visualization/#visualize_signal_propagation","text":"def visualize_signal_propagation ( name_values : list , title : str , * args , ** kwargs ) -> matplotlib . figure . Figure Visualize Signal Propagation Plot using matplolib and the utilities from the timm package. See: https://github.com/mehdidc/signal_propagation_plot/blob/main/signal_propagation_plot/pytorch.py Parameters: Name Type Description Default name_values torch.nn.Module pytorch model None input_shape List[int] Input Size of the model. None Returns: Type Description plt.Figure matplotlib figure","title":"visualize_signal_propagation"},{"location":"reference/ophthalmology/data/","text":"Module ophthalmology.data data This module contains pytorch datasets as well as pytorch_lightning datamodules that also contain the pytorch dataloaders. Sub-modules ophthalmology.data.modules ophthalmology.data.sets","title":"Index"},{"location":"reference/ophthalmology/data/#module-ophthalmologydata","text":"","title":"Module ophthalmology.data"},{"location":"reference/ophthalmology/data/#data","text":"This module contains pytorch datasets as well as pytorch_lightning datamodules that also contain the pytorch dataloaders.","title":"data"},{"location":"reference/ophthalmology/data/#sub-modules","text":"ophthalmology.data.modules ophthalmology.data.sets","title":"Sub-modules"},{"location":"reference/ophthalmology/data/modules/","text":"Module ophthalmology.data.modules datamodules This module contains the pytorch_lightning datamodules for the different tasks and datasets Classes DiabeticRetinopythyDetection class DiabeticRetinopythyDetection ( train_transform : torch . nn . modules . module . Module , image_dir : str = '' , csv_file_train : str = '' , csv_file_test : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.8 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False ) Ancestors (in MRO) pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin Class variables name Static methods add_argparse_args def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes. from_argparse_args def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) from_datasets def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None get_init_arguments_and_types def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value). Instance variables dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset. Methods on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | size def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input train_dataloader def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | val_dataloader def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input IndianDiabeticRetinopythyDetection class IndianDiabeticRetinopythyDetection ( train_transform : torch . nn . modules . module . Module , image_dir_train : str = '' , image_dir_test : str = '' , csv_file_train : str = '' , csv_file_test : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.8 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False ) Ancestors (in MRO) pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin Class variables name Static methods add_argparse_args def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes. from_argparse_args def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) from_datasets def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None get_init_arguments_and_types def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value). Instance variables dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset. Methods on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | size def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input train_dataloader def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | val_dataloader def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input IndianDiabeticRetinopythyDetectionLocalization class IndianDiabeticRetinopythyDetectionLocalization ( train_transform : torch . nn . modules . module . Module , image_dir_train : str = '' , image_dir_test : str = '' , csv_file_train_disk : str = '' , csv_file_test_disk : str = '' , csv_file_train_fovea : str = '' , csv_file_test_fovea : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.8 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 ) Ancestors (in MRO) pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin Class variables name Static methods add_argparse_args def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes. from_argparse_args def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) from_datasets def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None get_init_arguments_and_types def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value). Instance variables dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset. Methods on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | size def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input train_dataloader def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | val_dataloader def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input RetinaMNIST class RetinaMNIST ( data_dir , train_transform : torch . nn . modules . module . Module = None , test_transform : Optional [ torch . nn . modules . module . Module ] = None , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False ) Ancestors (in MRO) pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin Class variables name Static methods add_argparse_args def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes. from_argparse_args def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) from_datasets def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None get_init_arguments_and_types def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value). Instance variables dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset. Methods on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | size def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input train_dataloader def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | val_dataloader def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input SSLDiabeticRetinopythyDetection class SSLDiabeticRetinopythyDetection ( ssl_transform : torch . nn . modules . module . Module , image_dir : str = '' , csv_file : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.98 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False , use_test_fraction : Optional [ float ] = None ) Ancestors (in MRO) pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin Class variables name Static methods add_argparse_args def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes. from_argparse_args def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args) from_datasets def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None get_init_arguments_and_types def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value). Instance variables dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset. Methods on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | size def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor. teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input train_dataloader def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | val_dataloader def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"Modules"},{"location":"reference/ophthalmology/data/modules/#module-ophthalmologydatamodules","text":"","title":"Module ophthalmology.data.modules"},{"location":"reference/ophthalmology/data/modules/#datamodules","text":"This module contains the pytorch_lightning datamodules for the different tasks and datasets","title":"datamodules"},{"location":"reference/ophthalmology/data/modules/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/data/modules/#diabeticretinopythydetection","text":"class DiabeticRetinopythyDetection ( train_transform : torch . nn . modules . module . Module , image_dir : str = '' , csv_file_train : str = '' , csv_file_test : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.8 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False )","title":"DiabeticRetinopythyDetection"},{"location":"reference/ophthalmology/data/modules/#ancestors-in-mro","text":"pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/modules/#class-variables","text":"name","title":"Class variables"},{"location":"reference/ophthalmology/data/modules/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/modules/#add_argparse_args","text":"def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes.","title":"add_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_argparse_args","text":"def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args)","title":"from_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_datasets","text":"def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None","title":"from_datasets"},{"location":"reference/ophthalmology/data/modules/#get_init_arguments_and_types","text":"def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value).","title":"get_init_arguments_and_types"},{"location":"reference/ophthalmology/data/modules/#instance-variables","text":"dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset.","title":"Instance variables"},{"location":"reference/ophthalmology/data/modules/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/modules/#on_after_batch_transfer","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_before_batch_transfer","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_load_checkpoint","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_predict_dataloader","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_test_dataloader","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_train_dataloader","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_val_dataloader","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/data/modules/#predict_dataloader","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#prepare_data","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/data/modules/#save_hyperparameters","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/data/modules/#setup","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/data/modules/#size","text":"def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor.","title":"size"},{"location":"reference/ophthalmology/data/modules/#teardown","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/data/modules/#test_dataloader","text":"def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input","title":"test_dataloader"},{"location":"reference/ophthalmology/data/modules/#train_dataloader","text":"def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input","title":"train_dataloader"},{"location":"reference/ophthalmology/data/modules/#transfer_batch_to_device","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/data/modules/#val_dataloader","text":"def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"val_dataloader"},{"location":"reference/ophthalmology/data/modules/#indiandiabeticretinopythydetection","text":"class IndianDiabeticRetinopythyDetection ( train_transform : torch . nn . modules . module . Module , image_dir_train : str = '' , image_dir_test : str = '' , csv_file_train : str = '' , csv_file_test : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.8 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False )","title":"IndianDiabeticRetinopythyDetection"},{"location":"reference/ophthalmology/data/modules/#ancestors-in-mro_1","text":"pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/modules/#class-variables_1","text":"name","title":"Class variables"},{"location":"reference/ophthalmology/data/modules/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/modules/#add_argparse_args_1","text":"def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes.","title":"add_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_argparse_args_1","text":"def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args)","title":"from_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_datasets_1","text":"def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None","title":"from_datasets"},{"location":"reference/ophthalmology/data/modules/#get_init_arguments_and_types_1","text":"def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value).","title":"get_init_arguments_and_types"},{"location":"reference/ophthalmology/data/modules/#instance-variables_1","text":"dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset.","title":"Instance variables"},{"location":"reference/ophthalmology/data/modules/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/modules/#on_after_batch_transfer_1","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_before_batch_transfer_1","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_load_checkpoint_1","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_predict_dataloader_1","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_save_checkpoint_1","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_test_dataloader_1","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_train_dataloader_1","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_val_dataloader_1","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/data/modules/#predict_dataloader_1","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#prepare_data_1","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/data/modules/#save_hyperparameters_1","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/data/modules/#setup_1","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/data/modules/#size_1","text":"def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor.","title":"size"},{"location":"reference/ophthalmology/data/modules/#teardown_1","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/data/modules/#test_dataloader_1","text":"def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input","title":"test_dataloader"},{"location":"reference/ophthalmology/data/modules/#train_dataloader_1","text":"def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input","title":"train_dataloader"},{"location":"reference/ophthalmology/data/modules/#transfer_batch_to_device_1","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/data/modules/#val_dataloader_1","text":"def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"val_dataloader"},{"location":"reference/ophthalmology/data/modules/#indiandiabeticretinopythydetectionlocalization","text":"class IndianDiabeticRetinopythyDetectionLocalization ( train_transform : torch . nn . modules . module . Module , image_dir_train : str = '' , image_dir_test : str = '' , csv_file_train_disk : str = '' , csv_file_test_disk : str = '' , csv_file_train_fovea : str = '' , csv_file_test_fovea : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.8 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 )","title":"IndianDiabeticRetinopythyDetectionLocalization"},{"location":"reference/ophthalmology/data/modules/#ancestors-in-mro_2","text":"pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/modules/#class-variables_2","text":"name","title":"Class variables"},{"location":"reference/ophthalmology/data/modules/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/modules/#add_argparse_args_2","text":"def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes.","title":"add_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_argparse_args_2","text":"def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args)","title":"from_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_datasets_2","text":"def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None","title":"from_datasets"},{"location":"reference/ophthalmology/data/modules/#get_init_arguments_and_types_2","text":"def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value).","title":"get_init_arguments_and_types"},{"location":"reference/ophthalmology/data/modules/#instance-variables_2","text":"dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset.","title":"Instance variables"},{"location":"reference/ophthalmology/data/modules/#methods_2","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/modules/#on_after_batch_transfer_2","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_before_batch_transfer_2","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_load_checkpoint_2","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_predict_dataloader_2","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_save_checkpoint_2","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_test_dataloader_2","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_train_dataloader_2","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_val_dataloader_2","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/data/modules/#predict_dataloader_2","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#prepare_data_2","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/data/modules/#save_hyperparameters_2","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/data/modules/#setup_2","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/data/modules/#size_2","text":"def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor.","title":"size"},{"location":"reference/ophthalmology/data/modules/#teardown_2","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/data/modules/#test_dataloader_2","text":"def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input","title":"test_dataloader"},{"location":"reference/ophthalmology/data/modules/#train_dataloader_2","text":"def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input","title":"train_dataloader"},{"location":"reference/ophthalmology/data/modules/#transfer_batch_to_device_2","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/data/modules/#val_dataloader_2","text":"def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"val_dataloader"},{"location":"reference/ophthalmology/data/modules/#retinamnist","text":"class RetinaMNIST ( data_dir , train_transform : torch . nn . modules . module . Module = None , test_transform : Optional [ torch . nn . modules . module . Module ] = None , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False )","title":"RetinaMNIST"},{"location":"reference/ophthalmology/data/modules/#ancestors-in-mro_3","text":"pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/modules/#class-variables_3","text":"name","title":"Class variables"},{"location":"reference/ophthalmology/data/modules/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/modules/#add_argparse_args_3","text":"def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes.","title":"add_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_argparse_args_3","text":"def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args)","title":"from_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_datasets_3","text":"def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None","title":"from_datasets"},{"location":"reference/ophthalmology/data/modules/#get_init_arguments_and_types_3","text":"def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value).","title":"get_init_arguments_and_types"},{"location":"reference/ophthalmology/data/modules/#instance-variables_3","text":"dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset.","title":"Instance variables"},{"location":"reference/ophthalmology/data/modules/#methods_3","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/modules/#on_after_batch_transfer_3","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_before_batch_transfer_3","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_load_checkpoint_3","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_predict_dataloader_3","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_save_checkpoint_3","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_test_dataloader_3","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_train_dataloader_3","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_val_dataloader_3","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/data/modules/#predict_dataloader_3","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#prepare_data_3","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/data/modules/#save_hyperparameters_3","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/data/modules/#setup_3","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/data/modules/#size_3","text":"def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor.","title":"size"},{"location":"reference/ophthalmology/data/modules/#teardown_3","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/data/modules/#test_dataloader_3","text":"def test_dataloader ( self ) Returns: Type Description None output - Testing data loader for the given input","title":"test_dataloader"},{"location":"reference/ophthalmology/data/modules/#train_dataloader_3","text":"def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input","title":"train_dataloader"},{"location":"reference/ophthalmology/data/modules/#transfer_batch_to_device_3","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/data/modules/#val_dataloader_3","text":"def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"val_dataloader"},{"location":"reference/ophthalmology/data/modules/#ssldiabeticretinopythydetection","text":"class SSLDiabeticRetinopythyDetection ( ssl_transform : torch . nn . modules . module . Module , image_dir : str = '' , csv_file : str = '' , test_transform : Optional [ torch . nn . modules . module . Module ] = None , train_test_split : float = 0.98 , batch_size : int = 16 , num_workers : int = 1 , pin_memory : bool = False , seed : int = 42 , balanced_sampling : bool = False , use_test_fraction : Optional [ float ] = None )","title":"SSLDiabeticRetinopythyDetection"},{"location":"reference/ophthalmology/data/modules/#ancestors-in-mro_4","text":"pytorch_lightning.core.datamodule.LightningDataModule pytorch_lightning.core.hooks.CheckpointHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/modules/#class-variables_4","text":"name","title":"Class variables"},{"location":"reference/ophthalmology/data/modules/#static-methods_4","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/modules/#add_argparse_args_4","text":"def add_argparse_args ( parent_parser : argparse . ArgumentParser , ** kwargs ) -> argparse . ArgumentParser Extends existing argparse by default LightningDataModule attributes.","title":"add_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_argparse_args_4","text":"def from_argparse_args ( args : Union [ argparse . Namespace , argparse . ArgumentParser ], ** kwargs ) Create an instance from CLI arguments. Args: args: The parser or namespace to take arguments from. Only known arguments will be parsed and passed to the :class: ~pytorch_lightning.core.datamodule.LightningDataModule . **kwargs: Additional keyword arguments that may override ones in the parser or namespace. These must be valid DataModule arguments. Example:: parser = ArgumentParser(add_help=False) parser = LightningDataModule.add_argparse_args(parser) module = LightningDataModule.from_argparse_args(args)","title":"from_argparse_args"},{"location":"reference/ophthalmology/data/modules/#from_datasets_4","text":"def from_datasets ( train_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], Mapping [ str , torch . utils . data . dataset . Dataset ], NoneType ] = None , val_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , test_dataset : Union [ torch . utils . data . dataset . Dataset , Sequence [ torch . utils . data . dataset . Dataset ], NoneType ] = None , batch_size : int = 1 , num_workers : int = 0 ) Create an instance from torch.utils.data.Dataset. Parameters: Name Type Description Default train_dataset None (optional) Dataset to be used for train_dataloader() None val_dataset None (optional) Dataset or list of Dataset to be used for val_dataloader() None test_dataset None (optional) Dataset or list of Dataset to be used for test_dataloader() None batch_size None Batch size to use for each dataloader. Default is 1. None num_workers None Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process. Number of CPUs available. None","title":"from_datasets"},{"location":"reference/ophthalmology/data/modules/#get_init_arguments_and_types_4","text":"def get_init_arguments_and_types ( ) -> List [ Tuple [ str , Tuple , Any ]] Scans the DataModule signature and returns argument names, types and default values. Returns: Type Description None List with tuples of 3 values: (argument name, set with argument types, argument default value).","title":"get_init_arguments_and_types"},{"location":"reference/ophthalmology/data/modules/#instance-variables_4","text":"dims A tuple describing the shape of your data. Extra functionality exposed in size . has_prepared_data Return bool letting you know if datamodule.prepare_data() has been called or not. has_setup_fit Return bool letting you know if datamodule.setup(stage='fit') has been called or not. has_setup_predict Return bool letting you know if datamodule.setup(stage='predict') has been called or not. has_setup_test Return bool letting you know if datamodule.setup(stage='test') has been called or not. has_setup_validate Return bool letting you know if datamodule.setup(stage='validate') has been called or not. has_teardown_fit Return bool letting you know if datamodule.teardown(stage='fit') has been called or not. has_teardown_predict Return bool letting you know if datamodule.teardown(stage='predict') has been called or not. has_teardown_test Return bool letting you know if datamodule.teardown(stage='test') has been called or not. has_teardown_validate Return bool letting you know if datamodule.teardown(stage='validate') has been called or not. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . test_transforms Optional transforms (or collection of transforms) you can apply to test dataset. train_transforms Optional transforms (or collection of transforms) you can apply to train dataset. val_transforms Optional transforms (or collection of transforms) you can apply to validation dataset.","title":"Instance variables"},{"location":"reference/ophthalmology/data/modules/#methods_4","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/modules/#on_after_batch_transfer_4","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_before_batch_transfer_4","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/data/modules/#on_load_checkpoint_4","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_predict_dataloader_4","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_save_checkpoint_4","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/data/modules/#on_test_dataloader_4","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_train_dataloader_4","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/data/modules/#on_val_dataloader_4","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/data/modules/#predict_dataloader_4","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/data/modules/#prepare_data_4","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/data/modules/#save_hyperparameters_4","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/data/modules/#setup_4","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/data/modules/#size_4","text":"def size ( self , dim = None ) -> Union [ Tuple , List [ Tuple ]] Return the dimension of each input either as a tuple or list of tuples. You can index this just as you would with a torch tensor.","title":"size"},{"location":"reference/ophthalmology/data/modules/#teardown_4","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/data/modules/#test_dataloader_4","text":"def test_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"test_dataloader"},{"location":"reference/ophthalmology/data/modules/#train_dataloader_4","text":"def train_dataloader ( self ) Returns: Type Description None output - Train data loader for the given input","title":"train_dataloader"},{"location":"reference/ophthalmology/data/modules/#transfer_batch_to_device_4","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/data/modules/#val_dataloader_4","text":"def val_dataloader ( self ) Returns: Type Description None output - Validation data loader for the given input","title":"val_dataloader"},{"location":"reference/ophthalmology/data/sets/","text":"Module ophthalmology.data.sets datasets This file contains pytorch.utils.Dataset classes. Classes DiabeticRetinopythyDetection class DiabeticRetinopythyDetection ( image_dir : str , csv_file : str , transform = None , use_fraction : Optional [ float ] = None ) Ancestors (in MRO) torch.utils.data.dataset.Dataset typing.Generic Class variables functions Static methods register_datapipe_as_function def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False ) register_function def register_function ( function_name , function ) Methods get_labels def get_labels ( self , indices : Optional [ List [ int ]] = None ) IndianDiabeticRetinopythyDetection class IndianDiabeticRetinopythyDetection ( image_dir : str , csv_file : str , transform = None , use_fraction : Optional [ float ] = None ) Ancestors (in MRO) torch.utils.data.dataset.Dataset typing.Generic Class variables functions Static methods register_datapipe_as_function def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False ) register_function def register_function ( function_name , function ) Methods get_labels def get_labels ( self , indices : Optional [ List [ int ]] = None ) IndianDiabeticRetinopythyDetectionLocalization class IndianDiabeticRetinopythyDetectionLocalization ( image_dir : str , csv_file_disk : str , csv_file_fovea : str , transform = None , use_fraction : Optional [ float ] = None ) Ancestors (in MRO) torch.utils.data.dataset.Dataset typing.Generic Class variables functions Static methods register_datapipe_as_function def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False ) register_function def register_function ( function_name , function ) SimCLRWrapper class SimCLRWrapper ( dataset : torch . utils . data . dataset . Dataset , transform : List [ torch . nn . modules . module . Module ] = [] ) Ancestors (in MRO) torch.utils.data.dataset.Dataset typing.Generic Class variables functions Static methods register_datapipe_as_function def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False ) register_function def register_function ( function_name , function )","title":"Sets"},{"location":"reference/ophthalmology/data/sets/#module-ophthalmologydatasets","text":"","title":"Module ophthalmology.data.sets"},{"location":"reference/ophthalmology/data/sets/#datasets","text":"This file contains pytorch.utils.Dataset classes.","title":"datasets"},{"location":"reference/ophthalmology/data/sets/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/data/sets/#diabeticretinopythydetection","text":"class DiabeticRetinopythyDetection ( image_dir : str , csv_file : str , transform = None , use_fraction : Optional [ float ] = None )","title":"DiabeticRetinopythyDetection"},{"location":"reference/ophthalmology/data/sets/#ancestors-in-mro","text":"torch.utils.data.dataset.Dataset typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/sets/#class-variables","text":"functions","title":"Class variables"},{"location":"reference/ophthalmology/data/sets/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/sets/#register_datapipe_as_function","text":"def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False )","title":"register_datapipe_as_function"},{"location":"reference/ophthalmology/data/sets/#register_function","text":"def register_function ( function_name , function )","title":"register_function"},{"location":"reference/ophthalmology/data/sets/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/sets/#get_labels","text":"def get_labels ( self , indices : Optional [ List [ int ]] = None )","title":"get_labels"},{"location":"reference/ophthalmology/data/sets/#indiandiabeticretinopythydetection","text":"class IndianDiabeticRetinopythyDetection ( image_dir : str , csv_file : str , transform = None , use_fraction : Optional [ float ] = None )","title":"IndianDiabeticRetinopythyDetection"},{"location":"reference/ophthalmology/data/sets/#ancestors-in-mro_1","text":"torch.utils.data.dataset.Dataset typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/sets/#class-variables_1","text":"functions","title":"Class variables"},{"location":"reference/ophthalmology/data/sets/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/sets/#register_datapipe_as_function_1","text":"def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False )","title":"register_datapipe_as_function"},{"location":"reference/ophthalmology/data/sets/#register_function_1","text":"def register_function ( function_name , function )","title":"register_function"},{"location":"reference/ophthalmology/data/sets/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/data/sets/#get_labels_1","text":"def get_labels ( self , indices : Optional [ List [ int ]] = None )","title":"get_labels"},{"location":"reference/ophthalmology/data/sets/#indiandiabeticretinopythydetectionlocalization","text":"class IndianDiabeticRetinopythyDetectionLocalization ( image_dir : str , csv_file_disk : str , csv_file_fovea : str , transform = None , use_fraction : Optional [ float ] = None )","title":"IndianDiabeticRetinopythyDetectionLocalization"},{"location":"reference/ophthalmology/data/sets/#ancestors-in-mro_2","text":"torch.utils.data.dataset.Dataset typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/sets/#class-variables_2","text":"functions","title":"Class variables"},{"location":"reference/ophthalmology/data/sets/#static-methods_2","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/sets/#register_datapipe_as_function_2","text":"def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False )","title":"register_datapipe_as_function"},{"location":"reference/ophthalmology/data/sets/#register_function_2","text":"def register_function ( function_name , function )","title":"register_function"},{"location":"reference/ophthalmology/data/sets/#simclrwrapper","text":"class SimCLRWrapper ( dataset : torch . utils . data . dataset . Dataset , transform : List [ torch . nn . modules . module . Module ] = [] )","title":"SimCLRWrapper"},{"location":"reference/ophthalmology/data/sets/#ancestors-in-mro_3","text":"torch.utils.data.dataset.Dataset typing.Generic","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/data/sets/#class-variables_3","text":"functions","title":"Class variables"},{"location":"reference/ophthalmology/data/sets/#static-methods_3","text":"","title":"Static methods"},{"location":"reference/ophthalmology/data/sets/#register_datapipe_as_function_3","text":"def register_datapipe_as_function ( function_name , cls_to_register , enable_df_api_tracing = False )","title":"register_datapipe_as_function"},{"location":"reference/ophthalmology/data/sets/#register_function_3","text":"def register_function ( function_name , function )","title":"register_function"},{"location":"reference/ophthalmology/layers/","text":"Module ophthalmology.layers layers This module contains computational primitives (or collections of those) that can be used as layers in a pytorch.nn.Module Sub-modules ophthalmology.layers.activations ophthalmology.layers.assp ophthalmology.layers.convolutional_stems ophthalmology.layers.involution ophthalmology.layers.losses ophthalmology.layers.noise ophthalmology.layers.residual_blocks ophthalmology.layers.squeeze_and_excitation ophthalmology.layers.transforms","title":"Index"},{"location":"reference/ophthalmology/layers/#module-ophthalmologylayers","text":"","title":"Module ophthalmology.layers"},{"location":"reference/ophthalmology/layers/#layers","text":"This module contains computational primitives (or collections of those) that can be used as layers in a pytorch.nn.Module","title":"layers"},{"location":"reference/ophthalmology/layers/#sub-modules","text":"ophthalmology.layers.activations ophthalmology.layers.assp ophthalmology.layers.convolutional_stems ophthalmology.layers.involution ophthalmology.layers.losses ophthalmology.layers.noise ophthalmology.layers.residual_blocks ophthalmology.layers.squeeze_and_excitation ophthalmology.layers.transforms","title":"Sub-modules"},{"location":"reference/ophthalmology/layers/activations/","text":"Module ophthalmology.layers.activations Activation Functions None Classes SELU class SELU ( ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Activations"},{"location":"reference/ophthalmology/layers/activations/#module-ophthalmologylayersactivations","text":"","title":"Module ophthalmology.layers.activations"},{"location":"reference/ophthalmology/layers/activations/#activation-functions","text":"None","title":"Activation Functions"},{"location":"reference/ophthalmology/layers/activations/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/activations/#selu","text":"class SELU ( )","title":"SELU"},{"location":"reference/ophthalmology/layers/activations/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/activations/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/activations/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/activations/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/activations/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/activations/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/activations/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/activations/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/activations/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/activations/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/activations/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/activations/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/activations/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/activations/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/activations/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/activations/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/activations/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/activations/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/activations/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/activations/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/activations/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/activations/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/activations/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/activations/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/activations/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/activations/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/activations/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/activations/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/activations/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/activations/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/activations/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/activations/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/activations/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/activations/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/activations/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/activations/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/activations/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/activations/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/activations/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/activations/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/activations/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/activations/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/activations/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/assp/","text":"Module ophthalmology.layers.assp Atrous Spatial Pyramid Pooling paper: Rethinking Atrous Convolution for Semantic Image Segmentation https://arxiv.org/abs/1706.05587 adapted from: https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/deeplabv3.py Classes ASSPLayer class ASSPLayer ( in_channels : int , pyramid_stage_channels : int = 8 , out_channels : Optional [ int ] = None ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Static methods conv2d def conv2d ( * args , ** kwargs ) Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Assp"},{"location":"reference/ophthalmology/layers/assp/#module-ophthalmologylayersassp","text":"","title":"Module ophthalmology.layers.assp"},{"location":"reference/ophthalmology/layers/assp/#atrous-spatial-pyramid-pooling","text":"paper: Rethinking Atrous Convolution for Semantic Image Segmentation https://arxiv.org/abs/1706.05587 adapted from: https://github.com/pytorch/vision/blob/master/torchvision/models/segmentation/deeplabv3.py","title":"Atrous Spatial Pyramid Pooling"},{"location":"reference/ophthalmology/layers/assp/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/assp/#assplayer","text":"class ASSPLayer ( in_channels : int , pyramid_stage_channels : int = 8 , out_channels : Optional [ int ] = None )","title":"ASSPLayer"},{"location":"reference/ophthalmology/layers/assp/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/assp/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/assp/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/layers/assp/#conv2d","text":"def conv2d ( * args , ** kwargs )","title":"conv2d"},{"location":"reference/ophthalmology/layers/assp/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/assp/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/assp/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/assp/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/assp/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/assp/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/assp/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/assp/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/assp/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/assp/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/assp/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/assp/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/assp/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/assp/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/assp/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/assp/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/assp/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/assp/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/assp/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/assp/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/assp/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/assp/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/assp/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/assp/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/assp/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/assp/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/assp/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/assp/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/assp/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/assp/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/assp/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/assp/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/assp/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/assp/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/assp/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/assp/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/assp/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/assp/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/assp/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/assp/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/assp/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/convolutional_stems/","text":"Module ophthalmology.layers.convolutional_stems Convolutional Stems These stems serve as the first convolutional layers in a CNN network. They are responsible for a low-level feature extraction and dimensionality reduction. Classes ASSPConvolutionalStem class ASSPConvolutionalStem ( in_channels : int = 1 , hidden_channels : int = 16 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = None ) Ancestors (in MRO) torch.nn.modules.container.Sequential torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None BasicConvolutionalStem class BasicConvolutionalStem ( in_channels : int = 1 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = None ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None Downscale class Downscale ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.str enum.Enum Class variables HALF QUARTER name value NFASSPConvolutionalStem class NFASSPConvolutionalStem ( in_channels : int = 1 , hidden_channels : int = 16 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = None ) Ancestors (in MRO) torch.nn.modules.container.Sequential torch.nn.modules.module.Module Class variables T_destination dump_patches Static methods conv2d def conv2d ( * args , ** kwargs ) Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Convolutional Stems"},{"location":"reference/ophthalmology/layers/convolutional_stems/#module-ophthalmologylayersconvolutional_stems","text":"","title":"Module ophthalmology.layers.convolutional_stems"},{"location":"reference/ophthalmology/layers/convolutional_stems/#convolutional-stems","text":"These stems serve as the first convolutional layers in a CNN network. They are responsible for a low-level feature extraction and dimensionality reduction.","title":"Convolutional Stems"},{"location":"reference/ophthalmology/layers/convolutional_stems/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/convolutional_stems/#asspconvolutionalstem","text":"class ASSPConvolutionalStem ( in_channels : int = 1 , hidden_channels : int = 16 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = None )","title":"ASSPConvolutionalStem"},{"location":"reference/ophthalmology/layers/convolutional_stems/#ancestors-in-mro","text":"torch.nn.modules.container.Sequential torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/convolutional_stems/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/convolutional_stems/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/convolutional_stems/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/convolutional_stems/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/convolutional_stems/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/convolutional_stems/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/convolutional_stems/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/convolutional_stems/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/convolutional_stems/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/convolutional_stems/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/convolutional_stems/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/convolutional_stems/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/convolutional_stems/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/convolutional_stems/#forward","text":"def forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/convolutional_stems/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/convolutional_stems/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/convolutional_stems/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/convolutional_stems/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/convolutional_stems/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/convolutional_stems/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/convolutional_stems/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/convolutional_stems/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/convolutional_stems/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/convolutional_stems/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/convolutional_stems/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/convolutional_stems/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/convolutional_stems/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/convolutional_stems/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/convolutional_stems/#basicconvolutionalstem","text":"class BasicConvolutionalStem ( in_channels : int = 1 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = None )","title":"BasicConvolutionalStem"},{"location":"reference/ophthalmology/layers/convolutional_stems/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/convolutional_stems/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/convolutional_stems/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/convolutional_stems/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/convolutional_stems/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/convolutional_stems/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/convolutional_stems/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/convolutional_stems/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/convolutional_stems/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/convolutional_stems/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/convolutional_stems/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/convolutional_stems/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/convolutional_stems/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/convolutional_stems/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/convolutional_stems/#forward_1","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/convolutional_stems/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/convolutional_stems/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/convolutional_stems/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/convolutional_stems/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/convolutional_stems/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/convolutional_stems/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/convolutional_stems/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/convolutional_stems/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/convolutional_stems/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/convolutional_stems/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/convolutional_stems/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/convolutional_stems/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/convolutional_stems/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/convolutional_stems/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/convolutional_stems/#downscale","text":"class Downscale ( / , * args , ** kwargs )","title":"Downscale"},{"location":"reference/ophthalmology/layers/convolutional_stems/#ancestors-in-mro_2","text":"builtins.str enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/convolutional_stems/#class-variables_2","text":"HALF QUARTER name value","title":"Class variables"},{"location":"reference/ophthalmology/layers/convolutional_stems/#nfasspconvolutionalstem","text":"class NFASSPConvolutionalStem ( in_channels : int = 1 , hidden_channels : int = 16 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = None )","title":"NFASSPConvolutionalStem"},{"location":"reference/ophthalmology/layers/convolutional_stems/#ancestors-in-mro_3","text":"torch.nn.modules.container.Sequential torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/convolutional_stems/#class-variables_3","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/convolutional_stems/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/layers/convolutional_stems/#conv2d","text":"def conv2d ( * args , ** kwargs )","title":"conv2d"},{"location":"reference/ophthalmology/layers/convolutional_stems/#methods_2","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/convolutional_stems/#add_module_2","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/convolutional_stems/#apply_2","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/convolutional_stems/#bfloat16_2","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/convolutional_stems/#buffers_2","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/convolutional_stems/#children_2","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/convolutional_stems/#cpu_2","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/convolutional_stems/#cuda_2","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/convolutional_stems/#double_2","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/convolutional_stems/#eval_2","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/convolutional_stems/#extra_repr_2","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/convolutional_stems/#float_2","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/convolutional_stems/#forward_2","text":"def forward ( self , input ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_buffer_2","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_extra_state_2","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_parameter_2","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/convolutional_stems/#get_submodule_2","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/convolutional_stems/#half_2","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/convolutional_stems/#load_state_dict_2","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/convolutional_stems/#modules_2","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_buffers_2","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_children_2","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_modules_2","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/convolutional_stems/#named_parameters_2","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/convolutional_stems/#parameters_2","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_backward_hook_2","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_buffer_2","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_forward_hook_2","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_forward_pre_hook_2","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_full_backward_hook_2","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/convolutional_stems/#register_parameter_2","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/convolutional_stems/#requires_grad__2","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/convolutional_stems/#set_extra_state_2","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/convolutional_stems/#share_memory_2","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/convolutional_stems/#state_dict_2","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/convolutional_stems/#to_2","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/convolutional_stems/#to_empty_2","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/convolutional_stems/#train_2","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/convolutional_stems/#type_2","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/convolutional_stems/#xpu_2","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/convolutional_stems/#zero_grad_2","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/involution/","text":"Module ophthalmology.layers.involution Involution layer Involution: Inverting the Inherence of Convolution for Visual Recognition https://arxiv.org/abs/2103.06255v2 Classes Involution class Involution ( in_channels , out_channels , groups = 1 , kernel_size = 3 , stride = 1 , reduction_ratio = 2 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Static methods get_name def get_name ( ) Return this layer name. Returns: Type Description str layer name. Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input_tensor ) Calculate Involution. override function from PyTorch. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None Involution2d class Involution2d ( in_channels : int , out_channels : int , sigma_mapping : Optional [ torch . nn . modules . module . Module ] = None , kernel_size : Union [ int , Tuple [ int , int ]] = ( 7 , 7 ), stride : Union [ int , Tuple [ int , int ]] = ( 1 , 1 ), groups : int = 1 , reduce_ratio : int = 1 , dilation : Union [ int , Tuple [ int , int ]] = ( 1 , 1 ), padding : Union [ int , Tuple [ int , int ]] = ( 3 , 3 ), bias : bool = False , ** kwargs ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input : torch . Tensor ) -> torch . Tensor Forward pass Parameters: Name Type Description Default input None (torch.Tensor) Input tensor of the shape [batch size, in channels, height, width] None Returns: Type Description None (torch.Tensor) Output tensor of the shape [batch size, out channels, height, width] (w/ same padding) get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Involution"},{"location":"reference/ophthalmology/layers/involution/#module-ophthalmologylayersinvolution","text":"","title":"Module ophthalmology.layers.involution"},{"location":"reference/ophthalmology/layers/involution/#involution-layer","text":"Involution: Inverting the Inherence of Convolution for Visual Recognition https://arxiv.org/abs/2103.06255v2","title":"Involution layer"},{"location":"reference/ophthalmology/layers/involution/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/involution/#involution","text":"class Involution ( in_channels , out_channels , groups = 1 , kernel_size = 3 , stride = 1 , reduction_ratio = 2 )","title":"Involution"},{"location":"reference/ophthalmology/layers/involution/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/involution/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/involution/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/layers/involution/#get_name","text":"def get_name ( ) Return this layer name. Returns: Type Description str layer name.","title":"get_name"},{"location":"reference/ophthalmology/layers/involution/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/involution/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/involution/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/involution/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/involution/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/involution/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/involution/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/involution/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/involution/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/involution/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/involution/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/involution/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/involution/#forward","text":"def forward ( self , input_tensor ) Calculate Involution. override function from PyTorch.","title":"forward"},{"location":"reference/ophthalmology/layers/involution/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/involution/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/involution/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/involution/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/involution/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/involution/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/involution/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/involution/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/involution/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/involution/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/involution/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/involution/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/involution/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/involution/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/involution/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/involution/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/involution/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/involution/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/involution/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/involution/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/involution/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/involution/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/involution/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/involution/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/involution/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/involution/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/involution/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/involution/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/involution/#involution2d","text":"class Involution2d ( in_channels : int , out_channels : int , sigma_mapping : Optional [ torch . nn . modules . module . Module ] = None , kernel_size : Union [ int , Tuple [ int , int ]] = ( 7 , 7 ), stride : Union [ int , Tuple [ int , int ]] = ( 1 , 1 ), groups : int = 1 , reduce_ratio : int = 1 , dilation : Union [ int , Tuple [ int , int ]] = ( 1 , 1 ), padding : Union [ int , Tuple [ int , int ]] = ( 3 , 3 ), bias : bool = False , ** kwargs )","title":"Involution2d"},{"location":"reference/ophthalmology/layers/involution/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/involution/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/involution/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/involution/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/involution/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/involution/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/involution/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/involution/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/involution/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/involution/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/involution/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/involution/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/involution/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/involution/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/involution/#forward_1","text":"def forward ( self , input : torch . Tensor ) -> torch . Tensor Forward pass Parameters: Name Type Description Default input None (torch.Tensor) Input tensor of the shape [batch size, in channels, height, width] None Returns: Type Description None (torch.Tensor) Output tensor of the shape [batch size, out channels, height, width] (w/ same padding)","title":"forward"},{"location":"reference/ophthalmology/layers/involution/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/involution/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/involution/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/involution/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/involution/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/involution/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/involution/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/involution/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/involution/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/involution/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/involution/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/involution/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/involution/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/involution/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/involution/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/involution/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/involution/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/involution/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/involution/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/involution/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/involution/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/involution/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/involution/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/involution/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/involution/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/involution/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/involution/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/involution/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/losses/","text":"Module ophthalmology.layers.losses Collection of Loss functions None Classes FocalLoss class FocalLoss ( weight = None , gamma = 0.0 , reduction = 'mean' ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self focal_loss def focal_loss ( self , input_values , gamma ) Computes the focal loss forward def forward ( self , input , target ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None MixupLoss class MixupLoss ( criterion : torch . nn . modules . module . Module ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , pred , targets ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None NLLLoss class NLLLoss ( ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input , targets ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None NTXentLoss class NTXentLoss ( temperature : float = 0.1 , eps : float = 1e-06 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , out_1 , out_2 ) assume out_1 and out_2 are normalized out_1: [batch_size, dim] out_2: [batch_size, dim] get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None SoftmaxMSELoss class SoftmaxMSELoss ( ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input , targets ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Losses"},{"location":"reference/ophthalmology/layers/losses/#module-ophthalmologylayerslosses","text":"Collection of Loss functions None","title":"Module ophthalmology.layers.losses"},{"location":"reference/ophthalmology/layers/losses/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/losses/#focalloss","text":"class FocalLoss ( weight = None , gamma = 0.0 , reduction = 'mean' )","title":"FocalLoss"},{"location":"reference/ophthalmology/layers/losses/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/losses/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/losses/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/losses/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/losses/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/losses/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/losses/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/losses/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/losses/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/losses/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/losses/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/losses/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/losses/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/losses/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/losses/#focal_loss","text":"def focal_loss ( self , input_values , gamma ) Computes the focal loss","title":"focal_loss"},{"location":"reference/ophthalmology/layers/losses/#forward","text":"def forward ( self , input , target ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/losses/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/losses/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/losses/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/losses/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/losses/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/losses/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/losses/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/losses/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/losses/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/losses/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/losses/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/losses/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/losses/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/losses/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/losses/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/losses/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/losses/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/losses/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/losses/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/losses/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/losses/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/losses/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/losses/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/losses/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/losses/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/losses/#mixuploss","text":"class MixupLoss ( criterion : torch . nn . modules . module . Module )","title":"MixupLoss"},{"location":"reference/ophthalmology/layers/losses/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/losses/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/losses/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/losses/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/losses/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/losses/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/losses/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/losses/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/losses/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/losses/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/losses/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/losses/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/losses/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/losses/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/losses/#forward_1","text":"def forward ( self , pred , targets ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/losses/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/losses/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/losses/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/losses/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/losses/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/losses/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/losses/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/losses/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/losses/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/losses/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/losses/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/losses/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/losses/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/losses/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/losses/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/losses/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/losses/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/losses/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/losses/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/losses/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/losses/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/losses/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/losses/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/losses/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/losses/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/losses/#nllloss","text":"class NLLLoss ( )","title":"NLLLoss"},{"location":"reference/ophthalmology/layers/losses/#ancestors-in-mro_2","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/losses/#class-variables_2","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/losses/#methods_2","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/losses/#add_module_2","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/losses/#apply_2","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/losses/#bfloat16_2","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/losses/#buffers_2","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/losses/#children_2","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/losses/#cpu_2","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/losses/#cuda_2","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/losses/#double_2","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/losses/#eval_2","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/losses/#extra_repr_2","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/losses/#float_2","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/losses/#forward_2","text":"def forward ( self , input , targets ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/losses/#get_buffer_2","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/losses/#get_extra_state_2","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/losses/#get_parameter_2","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/losses/#get_submodule_2","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/losses/#half_2","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/losses/#load_state_dict_2","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/losses/#modules_2","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/losses/#named_buffers_2","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/losses/#named_children_2","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/losses/#named_modules_2","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/losses/#named_parameters_2","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/losses/#parameters_2","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/losses/#register_backward_hook_2","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_buffer_2","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/losses/#register_forward_hook_2","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_forward_pre_hook_2","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/losses/#register_full_backward_hook_2","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_parameter_2","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/losses/#requires_grad__2","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/losses/#set_extra_state_2","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/losses/#share_memory_2","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/losses/#state_dict_2","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/losses/#to_2","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/losses/#to_empty_2","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/losses/#train_2","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/losses/#type_2","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/losses/#xpu_2","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/losses/#zero_grad_2","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/losses/#ntxentloss","text":"class NTXentLoss ( temperature : float = 0.1 , eps : float = 1e-06 )","title":"NTXentLoss"},{"location":"reference/ophthalmology/layers/losses/#ancestors-in-mro_3","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/losses/#class-variables_3","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/losses/#methods_3","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/losses/#add_module_3","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/losses/#apply_3","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/losses/#bfloat16_3","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/losses/#buffers_3","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/losses/#children_3","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/losses/#cpu_3","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/losses/#cuda_3","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/losses/#double_3","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/losses/#eval_3","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/losses/#extra_repr_3","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/losses/#float_3","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/losses/#forward_3","text":"def forward ( self , out_1 , out_2 ) assume out_1 and out_2 are normalized out_1: [batch_size, dim] out_2: [batch_size, dim]","title":"forward"},{"location":"reference/ophthalmology/layers/losses/#get_buffer_3","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/losses/#get_extra_state_3","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/losses/#get_parameter_3","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/losses/#get_submodule_3","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/losses/#half_3","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/losses/#load_state_dict_3","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/losses/#modules_3","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/losses/#named_buffers_3","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/losses/#named_children_3","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/losses/#named_modules_3","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/losses/#named_parameters_3","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/losses/#parameters_3","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/losses/#register_backward_hook_3","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_buffer_3","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/losses/#register_forward_hook_3","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_forward_pre_hook_3","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/losses/#register_full_backward_hook_3","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_parameter_3","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/losses/#requires_grad__3","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/losses/#set_extra_state_3","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/losses/#share_memory_3","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/losses/#state_dict_3","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/losses/#to_3","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/losses/#to_empty_3","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/losses/#train_3","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/losses/#type_3","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/losses/#xpu_3","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/losses/#zero_grad_3","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/losses/#softmaxmseloss","text":"class SoftmaxMSELoss ( )","title":"SoftmaxMSELoss"},{"location":"reference/ophthalmology/layers/losses/#ancestors-in-mro_4","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/losses/#class-variables_4","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/losses/#methods_4","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/losses/#add_module_4","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/losses/#apply_4","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/losses/#bfloat16_4","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/losses/#buffers_4","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/losses/#children_4","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/losses/#cpu_4","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/losses/#cuda_4","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/losses/#double_4","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/losses/#eval_4","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/losses/#extra_repr_4","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/losses/#float_4","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/losses/#forward_4","text":"def forward ( self , input , targets ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/losses/#get_buffer_4","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/losses/#get_extra_state_4","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/losses/#get_parameter_4","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/losses/#get_submodule_4","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/losses/#half_4","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/losses/#load_state_dict_4","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/losses/#modules_4","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/losses/#named_buffers_4","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/losses/#named_children_4","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/losses/#named_modules_4","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/losses/#named_parameters_4","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/losses/#parameters_4","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/losses/#register_backward_hook_4","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_buffer_4","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/losses/#register_forward_hook_4","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_forward_pre_hook_4","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/losses/#register_full_backward_hook_4","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/losses/#register_parameter_4","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/losses/#requires_grad__4","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/losses/#set_extra_state_4","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/losses/#share_memory_4","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/losses/#state_dict_4","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/losses/#to_4","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/losses/#to_empty_4","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/losses/#train_4","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/losses/#type_4","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/losses/#xpu_4","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/losses/#zero_grad_4","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/noise/","text":"Module ophthalmology.layers.noise Collection of Modules that add noise to an input. Mostly useful as transformations in data augmentation Classes RandomGaussianNoise class RandomGaussianNoise ( var : float , apply_prob : float = 0.5 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None RandomMeanOffset class RandomMeanOffset ( std : float = 0.1 , apply_prob : float = 0.5 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None RandomNullNoise class RandomNullNoise ( treshold : float = 0.005 , apply_prob : float = 0.5 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None RandomSaltAndPepperNoise class RandomSaltAndPepperNoise ( treshold : float = 0.005 , salt_value : float = 1.0 , pepper_value : float = - 1.0 , apply_prob : float = 0.5 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Noise"},{"location":"reference/ophthalmology/layers/noise/#module-ophthalmologylayersnoise","text":"Collection of Modules that add noise to an input. Mostly useful as transformations in data augmentation","title":"Module ophthalmology.layers.noise"},{"location":"reference/ophthalmology/layers/noise/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/noise/#randomgaussiannoise","text":"class RandomGaussianNoise ( var : float , apply_prob : float = 0.5 )","title":"RandomGaussianNoise"},{"location":"reference/ophthalmology/layers/noise/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/noise/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/noise/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/noise/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/noise/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/noise/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/noise/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/noise/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/noise/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/noise/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/noise/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/noise/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/noise/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/noise/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/noise/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/noise/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/noise/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/noise/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/noise/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/noise/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/noise/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/noise/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/noise/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/noise/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/noise/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/noise/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/noise/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/noise/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/noise/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/noise/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/noise/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/noise/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/noise/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/noise/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/noise/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/noise/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/noise/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/noise/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/noise/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/noise/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/noise/#randommeanoffset","text":"class RandomMeanOffset ( std : float = 0.1 , apply_prob : float = 0.5 )","title":"RandomMeanOffset"},{"location":"reference/ophthalmology/layers/noise/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/noise/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/noise/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/noise/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/noise/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/noise/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/noise/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/noise/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/noise/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/noise/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/noise/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/noise/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/noise/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/noise/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/noise/#forward_1","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/noise/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/noise/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/noise/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/noise/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/noise/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/noise/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/noise/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/noise/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/noise/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/noise/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/noise/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/noise/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/noise/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/noise/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/noise/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/noise/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/noise/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/noise/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/noise/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/noise/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/noise/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/noise/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/noise/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/noise/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/noise/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/noise/#randomnullnoise","text":"class RandomNullNoise ( treshold : float = 0.005 , apply_prob : float = 0.5 )","title":"RandomNullNoise"},{"location":"reference/ophthalmology/layers/noise/#ancestors-in-mro_2","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/noise/#class-variables_2","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/noise/#methods_2","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/noise/#add_module_2","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/noise/#apply_2","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/noise/#bfloat16_2","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/noise/#buffers_2","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/noise/#children_2","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/noise/#cpu_2","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/noise/#cuda_2","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/noise/#double_2","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/noise/#eval_2","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/noise/#extra_repr_2","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/noise/#float_2","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/noise/#forward_2","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/noise/#get_buffer_2","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/noise/#get_extra_state_2","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/noise/#get_parameter_2","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/noise/#get_submodule_2","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/noise/#half_2","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/noise/#load_state_dict_2","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/noise/#modules_2","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/noise/#named_buffers_2","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/noise/#named_children_2","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/noise/#named_modules_2","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/noise/#named_parameters_2","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/noise/#parameters_2","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/noise/#register_backward_hook_2","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_buffer_2","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/noise/#register_forward_hook_2","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_forward_pre_hook_2","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/noise/#register_full_backward_hook_2","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_parameter_2","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/noise/#requires_grad__2","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/noise/#set_extra_state_2","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/noise/#share_memory_2","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/noise/#state_dict_2","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/noise/#to_2","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/noise/#to_empty_2","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/noise/#train_2","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/noise/#type_2","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/noise/#xpu_2","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/noise/#zero_grad_2","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/noise/#randomsaltandpeppernoise","text":"class RandomSaltAndPepperNoise ( treshold : float = 0.005 , salt_value : float = 1.0 , pepper_value : float = - 1.0 , apply_prob : float = 0.5 )","title":"RandomSaltAndPepperNoise"},{"location":"reference/ophthalmology/layers/noise/#ancestors-in-mro_3","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/noise/#class-variables_3","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/noise/#methods_3","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/noise/#add_module_3","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/noise/#apply_3","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/noise/#bfloat16_3","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/noise/#buffers_3","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/noise/#children_3","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/noise/#cpu_3","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/noise/#cuda_3","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/noise/#double_3","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/noise/#eval_3","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/noise/#extra_repr_3","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/noise/#float_3","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/noise/#forward_3","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/noise/#get_buffer_3","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/noise/#get_extra_state_3","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/noise/#get_parameter_3","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/noise/#get_submodule_3","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/noise/#half_3","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/noise/#load_state_dict_3","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/noise/#modules_3","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/noise/#named_buffers_3","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/noise/#named_children_3","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/noise/#named_modules_3","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/noise/#named_parameters_3","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/noise/#parameters_3","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/noise/#register_backward_hook_3","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_buffer_3","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/noise/#register_forward_hook_3","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_forward_pre_hook_3","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/noise/#register_full_backward_hook_3","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/noise/#register_parameter_3","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/noise/#requires_grad__3","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/noise/#set_extra_state_3","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/noise/#share_memory_3","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/noise/#state_dict_3","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/noise/#to_3","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/noise/#to_empty_3","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/noise/#train_3","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/noise/#type_3","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/noise/#xpu_3","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/noise/#zero_grad_3","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/residual_blocks/","text":"Module ophthalmology.layers.residual_blocks Residual Blocks None Classes BasicResidualBlock class BasicResidualBlock ( in_channels : int = 8 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None NFSEResidualBlock class NFSEResidualBlock ( in_channels : int = 8 , alpha : float = 0.2 , beta : float = 0.9165 , involution_layer : bool = False ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Static methods conv2d def conv2d ( * args , ** kwargs ) Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None SEResidualBlock class SEResidualBlock ( in_channels : int = 8 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Residual Blocks"},{"location":"reference/ophthalmology/layers/residual_blocks/#module-ophthalmologylayersresidual_blocks","text":"","title":"Module ophthalmology.layers.residual_blocks"},{"location":"reference/ophthalmology/layers/residual_blocks/#residual-blocks","text":"None","title":"Residual Blocks"},{"location":"reference/ophthalmology/layers/residual_blocks/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/residual_blocks/#basicresidualblock","text":"class BasicResidualBlock ( in_channels : int = 8 )","title":"BasicResidualBlock"},{"location":"reference/ophthalmology/layers/residual_blocks/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/residual_blocks/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/residual_blocks/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/residual_blocks/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/residual_blocks/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/residual_blocks/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/residual_blocks/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/residual_blocks/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/residual_blocks/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/residual_blocks/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/residual_blocks/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/residual_blocks/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/residual_blocks/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/residual_blocks/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/residual_blocks/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/residual_blocks/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/residual_blocks/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/residual_blocks/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/residual_blocks/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/residual_blocks/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/residual_blocks/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/residual_blocks/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/residual_blocks/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/residual_blocks/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/residual_blocks/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/residual_blocks/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/residual_blocks/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/residual_blocks/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/residual_blocks/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/residual_blocks/#nfseresidualblock","text":"class NFSEResidualBlock ( in_channels : int = 8 , alpha : float = 0.2 , beta : float = 0.9165 , involution_layer : bool = False )","title":"NFSEResidualBlock"},{"location":"reference/ophthalmology/layers/residual_blocks/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/residual_blocks/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/residual_blocks/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/layers/residual_blocks/#conv2d","text":"def conv2d ( * args , ** kwargs )","title":"conv2d"},{"location":"reference/ophthalmology/layers/residual_blocks/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/residual_blocks/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/residual_blocks/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/residual_blocks/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/residual_blocks/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/residual_blocks/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/residual_blocks/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/residual_blocks/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/residual_blocks/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/residual_blocks/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/residual_blocks/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/residual_blocks/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/residual_blocks/#forward_1","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/residual_blocks/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/residual_blocks/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/residual_blocks/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/residual_blocks/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/residual_blocks/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/residual_blocks/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/residual_blocks/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/residual_blocks/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/residual_blocks/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/residual_blocks/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/residual_blocks/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/residual_blocks/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/residual_blocks/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/residual_blocks/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/residual_blocks/#seresidualblock","text":"class SEResidualBlock ( in_channels : int = 8 )","title":"SEResidualBlock"},{"location":"reference/ophthalmology/layers/residual_blocks/#ancestors-in-mro_2","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/residual_blocks/#class-variables_2","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/residual_blocks/#methods_2","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/residual_blocks/#add_module_2","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/residual_blocks/#apply_2","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/residual_blocks/#bfloat16_2","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/residual_blocks/#buffers_2","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/residual_blocks/#children_2","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/residual_blocks/#cpu_2","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/residual_blocks/#cuda_2","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/residual_blocks/#double_2","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/residual_blocks/#eval_2","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/residual_blocks/#extra_repr_2","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/residual_blocks/#float_2","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/residual_blocks/#forward_2","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_buffer_2","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_extra_state_2","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_parameter_2","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/residual_blocks/#get_submodule_2","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/residual_blocks/#half_2","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/residual_blocks/#load_state_dict_2","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/residual_blocks/#modules_2","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_buffers_2","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_children_2","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_modules_2","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/residual_blocks/#named_parameters_2","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/residual_blocks/#parameters_2","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_backward_hook_2","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_buffer_2","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_forward_hook_2","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_forward_pre_hook_2","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_full_backward_hook_2","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/residual_blocks/#register_parameter_2","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/residual_blocks/#requires_grad__2","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/residual_blocks/#set_extra_state_2","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/residual_blocks/#share_memory_2","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/residual_blocks/#state_dict_2","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/residual_blocks/#to_2","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/residual_blocks/#to_empty_2","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/residual_blocks/#train_2","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/residual_blocks/#type_2","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/residual_blocks/#xpu_2","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/residual_blocks/#zero_grad_2","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/","text":"Module ophthalmology.layers.squeeze_and_excitation Squeeze and Excitation layer Attention based primitive proposed by the paper \"Squeeze-and-Excitation Networks\": https://arxiv.org/abs/1709.01507 Classes SELayer class SELayer ( num_channels , reduction_ratio = 2 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , input_tensor ) Parameters: Name Type Description Default input_tensor None X, shape = (batch_size, num_channels, H, W) None Returns: Type Description None output tensor get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Squeeze And Excitation"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#module-ophthalmologylayerssqueeze_and_excitation","text":"","title":"Module ophthalmology.layers.squeeze_and_excitation"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#squeeze-and-excitation-layer","text":"Attention based primitive proposed by the paper \"Squeeze-and-Excitation Networks\": https://arxiv.org/abs/1709.01507","title":"Squeeze and Excitation layer"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#selayer","text":"class SELayer ( num_channels , reduction_ratio = 2 )","title":"SELayer"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#forward","text":"def forward ( self , input_tensor ) Parameters: Name Type Description Default input_tensor None X, shape = (batch_size, num_channels, H, W) None Returns: Type Description None output tensor","title":"forward"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/squeeze_and_excitation/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/layers/transforms/","text":"Module ophthalmology.layers.transforms Collection of data transformations and augmentations None Classes SimCLRAug class SimCLRAug ( size : int , rotate : bool = True , jitter : bool = True , bw : bool = True , blur : bool = True , resize_scale = ( 0.2 , 1.0 ), resize_ratio = ( 0.75 , 1.3333333333333333 ), rotate_deg : int = 30 , jitter_s : float = 0.6 , blur_s = ( 4 , 32 ), same_on_batch : bool = False , flip_p : float = 0.5 , rotate_p : float = 0.3 , jitter_p : float = 0.3 , bw_p : float = 0.3 , blur_p : float = 0.3 , stats = ([ 0.3211 , 0.2243 , 0.1602 ], [ 0.2617 , 0.1825 , 0.1308 ]), cuda : bool = True ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Transforms"},{"location":"reference/ophthalmology/layers/transforms/#module-ophthalmologylayerstransforms","text":"Collection of data transformations and augmentations None","title":"Module ophthalmology.layers.transforms"},{"location":"reference/ophthalmology/layers/transforms/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/layers/transforms/#simclraug","text":"class SimCLRAug ( size : int , rotate : bool = True , jitter : bool = True , bw : bool = True , blur : bool = True , resize_scale = ( 0.2 , 1.0 ), resize_ratio = ( 0.75 , 1.3333333333333333 ), rotate_deg : int = 30 , jitter_s : float = 0.6 , blur_s = ( 4 , 32 ), same_on_batch : bool = False , flip_p : float = 0.5 , rotate_p : float = 0.3 , jitter_p : float = 0.3 , bw_p : float = 0.3 , blur_p : float = 0.3 , stats = ([ 0.3211 , 0.2243 , 0.1602 ], [ 0.2617 , 0.1825 , 0.1308 ]), cuda : bool = True )","title":"SimCLRAug"},{"location":"reference/ophthalmology/layers/transforms/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/layers/transforms/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/layers/transforms/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/layers/transforms/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/layers/transforms/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/layers/transforms/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/layers/transforms/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/layers/transforms/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/layers/transforms/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/layers/transforms/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/layers/transforms/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/layers/transforms/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/layers/transforms/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/layers/transforms/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/layers/transforms/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/layers/transforms/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/layers/transforms/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/layers/transforms/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/layers/transforms/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/layers/transforms/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/layers/transforms/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/layers/transforms/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/layers/transforms/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/layers/transforms/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/layers/transforms/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/layers/transforms/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/layers/transforms/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/layers/transforms/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/layers/transforms/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/layers/transforms/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/layers/transforms/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/layers/transforms/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/layers/transforms/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/layers/transforms/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/layers/transforms/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/layers/transforms/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/layers/transforms/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/layers/transforms/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/layers/transforms/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/layers/transforms/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/layers/transforms/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/layers/transforms/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/layers/transforms/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/models/","text":"Module ophthalmology.models Models Pytorch.nn.Modules implementing custom and popular neural architectures. Sub-modules ophthalmology.models.attentive_nf_resnet ophthalmology.models.heads ophthalmology.models.resnet","title":"Index"},{"location":"reference/ophthalmology/models/#module-ophthalmologymodels","text":"","title":"Module ophthalmology.models"},{"location":"reference/ophthalmology/models/#models","text":"Pytorch.nn.Modules implementing custom and popular neural architectures.","title":"Models"},{"location":"reference/ophthalmology/models/#sub-modules","text":"ophthalmology.models.attentive_nf_resnet ophthalmology.models.heads ophthalmology.models.resnet","title":"Sub-modules"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/","text":"Module ophthalmology.models.attentive_nf_resnet Custom Architectures similar to NFNets from Andrew Brock None Classes AttentiveNFResNet class AttentiveNFResNet ( in_channels : int = 1 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = < Downscale . QUARTER : '4' > , num_res_blocks : int = 4 , num_classes : int = 5 , dropout : Optional [ float ] = None , nf : bool = False , involution : bool = False , alpha : float = 0.55 , beta : float = 1.14 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None AttentiveNFResNetBackbone class AttentiveNFResNetBackbone ( in_channels : int = 1 , out_channels : int = 80 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = < Downscale . QUARTER : '4' > , num_res_blocks : int = 8 , num_features : int = 64 , involution : bool = True , alpha : float = 0.55 , beta : float = 1.14 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Attentive Nf Resnet"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#module-ophthalmologymodelsattentive_nf_resnet","text":"","title":"Module ophthalmology.models.attentive_nf_resnet"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#custom-architectures-similar-to-nfnets-from-andrew-brock","text":"None","title":"Custom Architectures similar to NFNets from Andrew Brock"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#attentivenfresnet","text":"class AttentiveNFResNet ( in_channels : int = 1 , out_channels : int = 8 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = < Downscale . QUARTER : '4' > , num_res_blocks : int = 4 , num_classes : int = 5 , dropout : Optional [ float ] = None , nf : bool = False , involution : bool = False , alpha : float = 0.55 , beta : float = 1.14 )","title":"AttentiveNFResNet"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#attentivenfresnetbackbone","text":"class AttentiveNFResNetBackbone ( in_channels : int = 1 , out_channels : int = 80 , downscale : Optional [ ophthalmology . layers . convolutional_stems . Downscale ] = < Downscale . QUARTER : '4' > , num_res_blocks : int = 8 , num_features : int = 64 , involution : bool = True , alpha : float = 0.55 , beta : float = 1.14 )","title":"AttentiveNFResNetBackbone"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#forward_1","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/models/attentive_nf_resnet/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/models/heads/","text":"Module ophthalmology.models.heads Task-specific heads Different heads that expect their inputs to be flattened. Classes MultilayerHead class MultilayerHead ( in_features : int = 8 , hidden_layers : List [ int ] = [ 16 , 16 ], num_output_units : int = 5 , dropout : Optional [ float ] = None ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None ProjectionHead class ProjectionHead ( input_dim : int = 84 , hidden_dim = 64 , output_dim = 32 , normalize_output : bool = True ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Heads"},{"location":"reference/ophthalmology/models/heads/#module-ophthalmologymodelsheads","text":"","title":"Module ophthalmology.models.heads"},{"location":"reference/ophthalmology/models/heads/#task-specific-heads","text":"Different heads that expect their inputs to be flattened.","title":"Task-specific heads"},{"location":"reference/ophthalmology/models/heads/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/models/heads/#multilayerhead","text":"class MultilayerHead ( in_features : int = 8 , hidden_layers : List [ int ] = [ 16 , 16 ], num_output_units : int = 5 , dropout : Optional [ float ] = None )","title":"MultilayerHead"},{"location":"reference/ophthalmology/models/heads/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/models/heads/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/models/heads/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/models/heads/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/models/heads/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/models/heads/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/models/heads/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/models/heads/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/models/heads/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/models/heads/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/models/heads/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/models/heads/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/models/heads/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/models/heads/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/models/heads/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/models/heads/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/models/heads/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/models/heads/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/models/heads/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/models/heads/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/models/heads/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/models/heads/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/models/heads/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/models/heads/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/models/heads/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/models/heads/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/models/heads/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/models/heads/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/models/heads/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/models/heads/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/models/heads/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/models/heads/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/models/heads/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/models/heads/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/models/heads/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/models/heads/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/models/heads/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/models/heads/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/models/heads/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/models/heads/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/models/heads/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/models/heads/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/models/heads/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/models/heads/#projectionhead","text":"class ProjectionHead ( input_dim : int = 84 , hidden_dim = 64 , output_dim = 32 , normalize_output : bool = True )","title":"ProjectionHead"},{"location":"reference/ophthalmology/models/heads/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/models/heads/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/models/heads/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/models/heads/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/models/heads/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/models/heads/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/models/heads/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/models/heads/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/models/heads/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/models/heads/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/models/heads/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/models/heads/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/models/heads/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/models/heads/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/models/heads/#forward_1","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/models/heads/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/models/heads/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/models/heads/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/models/heads/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/models/heads/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/models/heads/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/models/heads/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/models/heads/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/models/heads/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/models/heads/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/models/heads/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/models/heads/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/models/heads/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/models/heads/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/models/heads/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/models/heads/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/models/heads/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/models/heads/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/models/heads/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/models/heads/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/models/heads/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/models/heads/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/models/heads/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/models/heads/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/models/heads/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/models/heads/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/models/heads/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/models/heads/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/models/resnet/","text":"Module ophthalmology.models.resnet ResNet models Implementations of the ResNet familiy from the paper \"Deep Residual Learning for Image Recognition\": https://arxiv.org/abs/1512.03385v1 Classes ResNet class ResNet ( name : str , num_output_units : int , num_resnet_features : int = 2048 , pretrained : bool = False ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None ResNetBackbone class ResNetBackbone ( name : str , pretrained : bool = False ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Resnet"},{"location":"reference/ophthalmology/models/resnet/#module-ophthalmologymodelsresnet","text":"","title":"Module ophthalmology.models.resnet"},{"location":"reference/ophthalmology/models/resnet/#resnet-models","text":"Implementations of the ResNet familiy from the paper \"Deep Residual Learning for Image Recognition\": https://arxiv.org/abs/1512.03385v1","title":"ResNet models"},{"location":"reference/ophthalmology/models/resnet/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/models/resnet/#resnet","text":"class ResNet ( name : str , num_output_units : int , num_resnet_features : int = 2048 , pretrained : bool = False )","title":"ResNet"},{"location":"reference/ophthalmology/models/resnet/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/models/resnet/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/models/resnet/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/models/resnet/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/models/resnet/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/models/resnet/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/models/resnet/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/models/resnet/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/models/resnet/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/models/resnet/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/models/resnet/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/models/resnet/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/models/resnet/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/models/resnet/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/models/resnet/#forward","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/models/resnet/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/models/resnet/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/models/resnet/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/models/resnet/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/models/resnet/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/models/resnet/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/models/resnet/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/models/resnet/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/models/resnet/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/models/resnet/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/models/resnet/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/models/resnet/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/models/resnet/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/models/resnet/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/models/resnet/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/models/resnet/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/models/resnet/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/models/resnet/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/models/resnet/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/models/resnet/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/models/resnet/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/models/resnet/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/models/resnet/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/models/resnet/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/models/resnet/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/models/resnet/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/models/resnet/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/models/resnet/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/models/resnet/#resnetbackbone","text":"class ResNetBackbone ( name : str , pretrained : bool = False )","title":"ResNetBackbone"},{"location":"reference/ophthalmology/models/resnet/#ancestors-in-mro_1","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/models/resnet/#class-variables_1","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/models/resnet/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/models/resnet/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/models/resnet/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/models/resnet/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/models/resnet/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/models/resnet/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/models/resnet/#cpu_1","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/models/resnet/#cuda_1","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/models/resnet/#double_1","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/models/resnet/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/models/resnet/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/models/resnet/#float_1","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/models/resnet/#forward_1","text":"def forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.","title":"forward"},{"location":"reference/ophthalmology/models/resnet/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/models/resnet/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/models/resnet/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/models/resnet/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/models/resnet/#half_1","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/models/resnet/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/models/resnet/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/models/resnet/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/models/resnet/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/models/resnet/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/models/resnet/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/models/resnet/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/models/resnet/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/models/resnet/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/models/resnet/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/models/resnet/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/models/resnet/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/models/resnet/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/models/resnet/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/models/resnet/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/models/resnet/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/models/resnet/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/models/resnet/#to_1","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/models/resnet/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/models/resnet/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/models/resnet/#type_1","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/models/resnet/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/models/resnet/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/modules/","text":"Module ophthalmology.modules Pytorch-lightning modules pytorch lightning modules contain the training, evaluation and testing logic of a training algorithms. Each task that needs different optimizers, metrics or other adjustments in their training loop get a sepparate lightning module. Sub-modules ophthalmology.modules.disease_grading ophthalmology.modules.localization ophthalmology.modules.simclr","title":"Index"},{"location":"reference/ophthalmology/modules/#module-ophthalmologymodules","text":"","title":"Module ophthalmology.modules"},{"location":"reference/ophthalmology/modules/#pytorch-lightning-modules","text":"pytorch lightning modules contain the training, evaluation and testing logic of a training algorithms. Each task that needs different optimizers, metrics or other adjustments in their training loop get a sepparate lightning module.","title":"Pytorch-lightning modules"},{"location":"reference/ophthalmology/modules/#sub-modules","text":"ophthalmology.modules.disease_grading ophthalmology.modules.localization ophthalmology.modules.simclr","title":"Sub-modules"},{"location":"reference/ophthalmology/modules/disease_grading/","text":"Module ophthalmology.modules.disease_grading Supervised Disease Grading Task. pytorch-lightning module for the supervised disease grading task. Classes DiseaseGradingClassification class DiseaseGradingClassification ( model : torch . nn . modules . module . Module , loss : torch . nn . modules . module . Module , num_train_samples : int , learning_rate : float = 0.003 , batch_size : int = 32 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 120 , weight_decay : float = 1e-05 ) Ancestors (in MRO) pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module Class variables CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches Static methods load_from_checkpoint def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) Instance variables automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None add_to_queue def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7. all_gather def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | backward def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module clip_gradients def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None configure_callbacks def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. configure_gradient_clipping def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0) configure_optimizers def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the configure_sharded_model def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent. cpu def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self cuda def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self forward def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output freeze def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_from_queue def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7. get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_progress_bar_dict def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | log def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_dict def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_grad_norm def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) lr_schedulers def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers . manual_backward def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | on_after_backward def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_backward def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_before_optimizer_step def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None on_before_zero_grad def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None on_epoch_end def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins. on_fit_end def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process on_fit_start def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process on_hpc_load def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None on_hpc_save def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_post_move_to_device def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after on_predict_batch_end def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_batch_start def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_predict_end def on_predict_end ( self ) -> None Called at the end of predicting. on_predict_epoch_end def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting. on_predict_epoch_start def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting. on_predict_model_eval def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop. on_predict_start def on_predict_start ( self ) -> None Called at the beginning of predicting. on_pretrain_routine_end def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_pretrain_routine_start def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_batch_end def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_batch_start def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_test_end def on_test_end ( self ) Called at the end of testing. on_test_epoch_end def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch. on_test_epoch_start def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch. on_test_model_eval def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop. on_test_model_train def on_test_model_train ( self ) -> None Sets the model to train during the test loop. on_test_start def on_test_start ( self ) -> None Called at the beginning of testing. on_train_batch_end def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_batch_start def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_train_end def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. on_train_epoch_end def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook on_train_epoch_start def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. on_train_start def on_train_start ( self ) -> None Called at the beginning of training after sanity check. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. on_validation_batch_end def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_batch_start def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_end def on_validation_end ( self ) -> None Called at the end of validation. on_validation_epoch_end def on_validation_epoch_end ( self ) Called in the validation loop at the very end of the epoch. on_validation_epoch_start def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch. on_validation_model_eval def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop. on_validation_model_train def on_validation_model_train ( self ) -> None Sets the model to train during the val loop. on_validation_start def on_validation_start ( self ) -> None Called at the beginning of validation. optimizer_step def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) optimizer_zero_grad def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. optimizers def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present. parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. predict_step def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() print def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | summarize def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object tbptt_split_batch def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set test_epoch_end def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None | test_step def test_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. test_step_end def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. to def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self to_onnx def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None to_torchscript def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not. toggle_optimizer def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self train_dataloader def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set training_epoch_end def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None | training_step def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. training_step_end def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None | transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | type def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self unfreeze def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() untoggle_optimizer def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None val_dataloader def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set validation_epoch_end def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None | validation_step def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. validation_step_end def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None DiseaseGradingRegression class DiseaseGradingRegression ( model : torch . nn . modules . module . Module , loss : torch . nn . modules . module . Module , num_train_samples : int , learning_rate : float = 0.003 , batch_size : int = 32 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 120 , weight_decay : float = 1e-05 ) Ancestors (in MRO) pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module Class variables CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches Static methods load_from_checkpoint def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) Instance variables automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None add_to_queue def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7. all_gather def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | backward def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module clip_gradients def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None configure_callbacks def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. configure_gradient_clipping def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0) configure_optimizers def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the configure_sharded_model def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent. cpu def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self cuda def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self forward def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output freeze def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_from_queue def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7. get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_progress_bar_dict def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | log def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_dict def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_grad_norm def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) lr_schedulers def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers . manual_backward def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | on_after_backward def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_backward def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_before_optimizer_step def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None on_before_zero_grad def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None on_epoch_end def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins. on_fit_end def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process on_fit_start def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process on_hpc_load def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None on_hpc_save def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_post_move_to_device def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after on_predict_batch_end def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_batch_start def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_predict_end def on_predict_end ( self ) -> None Called at the end of predicting. on_predict_epoch_end def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting. on_predict_epoch_start def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting. on_predict_model_eval def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop. on_predict_start def on_predict_start ( self ) -> None Called at the beginning of predicting. on_pretrain_routine_end def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_pretrain_routine_start def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_batch_end def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_batch_start def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_test_end def on_test_end ( self ) Called at the end of testing. on_test_epoch_end def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch. on_test_epoch_start def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch. on_test_model_eval def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop. on_test_model_train def on_test_model_train ( self ) -> None Sets the model to train during the test loop. on_test_start def on_test_start ( self ) -> None Called at the beginning of testing. on_train_batch_end def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_batch_start def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_train_end def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. on_train_epoch_end def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook on_train_epoch_start def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. on_train_start def on_train_start ( self ) -> None Called at the beginning of training after sanity check. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. on_validation_batch_end def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_batch_start def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_end def on_validation_end ( self ) -> None Called at the end of validation. on_validation_epoch_end def on_validation_epoch_end ( self ) Called in the validation loop at the very end of the epoch. on_validation_epoch_start def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch. on_validation_model_eval def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop. on_validation_model_train def on_validation_model_train ( self ) -> None Sets the model to train during the val loop. on_validation_start def on_validation_start ( self ) -> None Called at the beginning of validation. optimizer_step def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) optimizer_zero_grad def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. optimizers def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present. parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. predict_step def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() print def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None quantize def quantize ( self , preds ) register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | summarize def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object tbptt_split_batch def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set test_epoch_end def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None | test_step def test_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. test_step_end def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. to def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self to_onnx def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None to_torchscript def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not. toggle_optimizer def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self train_dataloader def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set training_epoch_end def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None | training_step def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. training_step_end def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None | transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | type def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self unfreeze def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() untoggle_optimizer def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None val_dataloader def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set validation_epoch_end def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None | validation_step def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. validation_step_end def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Disease Grading"},{"location":"reference/ophthalmology/modules/disease_grading/#module-ophthalmologymodulesdisease_grading","text":"","title":"Module ophthalmology.modules.disease_grading"},{"location":"reference/ophthalmology/modules/disease_grading/#supervised-disease-grading-task","text":"pytorch-lightning module for the supervised disease grading task.","title":"Supervised Disease Grading Task."},{"location":"reference/ophthalmology/modules/disease_grading/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/modules/disease_grading/#diseasegradingclassification","text":"class DiseaseGradingClassification ( model : torch . nn . modules . module . Module , loss : torch . nn . modules . module . Module , num_train_samples : int , learning_rate : float = 0.003 , batch_size : int = 32 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 120 , weight_decay : float = 1e-05 )","title":"DiseaseGradingClassification"},{"location":"reference/ophthalmology/modules/disease_grading/#ancestors-in-mro","text":"pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/modules/disease_grading/#class-variables","text":"CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/modules/disease_grading/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/modules/disease_grading/#load_from_checkpoint","text":"def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x)","title":"load_from_checkpoint"},{"location":"reference/ophthalmology/modules/disease_grading/#instance-variables","text":"automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the","title":"Instance variables"},{"location":"reference/ophthalmology/modules/disease_grading/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/modules/disease_grading/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/modules/disease_grading/#add_to_queue","text":"def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7.","title":"add_to_queue"},{"location":"reference/ophthalmology/modules/disease_grading/#all_gather","text":"def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape.","title":"all_gather"},{"location":"reference/ophthalmology/modules/disease_grading/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/modules/disease_grading/#backward","text":"def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None","title":"backward"},{"location":"reference/ophthalmology/modules/disease_grading/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/modules/disease_grading/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/modules/disease_grading/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/modules/disease_grading/#clip_gradients","text":"def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None","title":"clip_gradients"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_callbacks","text":"def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here.","title":"configure_callbacks"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_gradient_clipping","text":"def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0)","title":"configure_gradient_clipping"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_optimizers","text":"def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the","title":"configure_optimizers"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_sharded_model","text":"def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent.","title":"configure_sharded_model"},{"location":"reference/ophthalmology/modules/disease_grading/#cpu","text":"def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/modules/disease_grading/#cuda","text":"def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/modules/disease_grading/#double","text":"def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/modules/disease_grading/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/modules/disease_grading/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/modules/disease_grading/#float","text":"def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/modules/disease_grading/#forward","text":"def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output","title":"forward"},{"location":"reference/ophthalmology/modules/disease_grading/#freeze","text":"def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze()","title":"freeze"},{"location":"reference/ophthalmology/modules/disease_grading/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/modules/disease_grading/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/modules/disease_grading/#get_from_queue","text":"def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7.","title":"get_from_queue"},{"location":"reference/ophthalmology/modules/disease_grading/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/modules/disease_grading/#get_progress_bar_dict","text":"def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]","title":"get_progress_bar_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/modules/disease_grading/#half","text":"def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/modules/disease_grading/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#log","text":"def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log"},{"location":"reference/ophthalmology/modules/disease_grading/#log_dict","text":"def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#log_grad_norm","text":"def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True)","title":"log_grad_norm"},{"location":"reference/ophthalmology/modules/disease_grading/#lr_schedulers","text":"def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers .","title":"lr_schedulers"},{"location":"reference/ophthalmology/modules/disease_grading/#manual_backward","text":"def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None","title":"manual_backward"},{"location":"reference/ophthalmology/modules/disease_grading/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/modules/disease_grading/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/modules/disease_grading/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/modules/disease_grading/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/modules/disease_grading/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/modules/disease_grading/#on_after_backward","text":"def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients.","title":"on_after_backward"},{"location":"reference/ophthalmology/modules/disease_grading/#on_after_batch_transfer","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_backward","text":"def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None","title":"on_before_backward"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_batch_transfer","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_optimizer_step","text":"def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_zero_grad","text":"def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/modules/disease_grading/#on_epoch_end","text":"def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_epoch_start","text":"def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_fit_end","text":"def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process","title":"on_fit_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_fit_start","text":"def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process","title":"on_fit_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_hpc_load","text":"def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None","title":"on_hpc_load"},{"location":"reference/ophthalmology/modules/disease_grading/#on_hpc_save","text":"def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None","title":"on_hpc_save"},{"location":"reference/ophthalmology/modules/disease_grading/#on_load_checkpoint","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/modules/disease_grading/#on_post_move_to_device","text":"def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after","title":"on_post_move_to_device"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_batch_end","text":"def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_batch_start","text":"def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_dataloader","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_end","text":"def on_predict_end ( self ) -> None Called at the end of predicting.","title":"on_predict_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_epoch_end","text":"def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_epoch_start","text":"def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_model_eval","text":"def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop.","title":"on_predict_model_eval"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_start","text":"def on_predict_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_pretrain_routine_end","text":"def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_pretrain_routine_start","text":"def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_batch_end","text":"def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_batch_start","text":"def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_dataloader","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_end","text":"def on_test_end ( self ) Called at the end of testing.","title":"on_test_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_epoch_end","text":"def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_epoch_start","text":"def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_model_eval","text":"def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop.","title":"on_test_model_eval"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_model_train","text":"def on_test_model_train ( self ) -> None Sets the model to train during the test loop.","title":"on_test_model_train"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_start","text":"def on_test_start ( self ) -> None Called at the beginning of testing.","title":"on_test_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_batch_end","text":"def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_batch_start","text":"def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_dataloader","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_end","text":"def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed.","title":"on_train_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_epoch_end","text":"def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_epoch_start","text":"def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_start","text":"def on_train_start ( self ) -> None Called at the beginning of training after sanity check.","title":"on_train_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_val_dataloader","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_batch_end","text":"def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_batch_start","text":"def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_end","text":"def on_validation_end ( self ) -> None Called at the end of validation.","title":"on_validation_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_epoch_end","text":"def on_validation_epoch_end ( self ) Called in the validation loop at the very end of the epoch.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_epoch_start","text":"def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_model_eval","text":"def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop.","title":"on_validation_model_eval"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_model_train","text":"def on_validation_model_train ( self ) -> None Sets the model to train during the val loop.","title":"on_validation_model_train"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_start","text":"def on_validation_start ( self ) -> None Called at the beginning of validation.","title":"on_validation_start"},{"location":"reference/ophthalmology/modules/disease_grading/#optimizer_step","text":"def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure)","title":"optimizer_step"},{"location":"reference/ophthalmology/modules/disease_grading/#optimizer_zero_grad","text":"def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example.","title":"optimizer_zero_grad"},{"location":"reference/ophthalmology/modules/disease_grading/#optimizers","text":"def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present.","title":"optimizers"},{"location":"reference/ophthalmology/modules/disease_grading/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/modules/disease_grading/#predict_dataloader","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#predict_step","text":"def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output","title":"predict_step"},{"location":"reference/ophthalmology/modules/disease_grading/#prepare_data","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/modules/disease_grading/#print","text":"def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None","title":"print"},{"location":"reference/ophthalmology/modules/disease_grading/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/modules/disease_grading/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/modules/disease_grading/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/modules/disease_grading/#save_hyperparameters","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/modules/disease_grading/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/modules/disease_grading/#setup","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/modules/disease_grading/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/modules/disease_grading/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#summarize","text":"def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object","title":"summarize"},{"location":"reference/ophthalmology/modules/disease_grading/#tbptt_split_batch","text":"def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step .","title":"tbptt_split_batch"},{"location":"reference/ophthalmology/modules/disease_grading/#teardown","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/modules/disease_grading/#test_dataloader","text":"def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set","title":"test_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#test_epoch_end","text":"def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None |","title":"test_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#test_step","text":"def test_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled.","title":"test_step"},{"location":"reference/ophthalmology/modules/disease_grading/#test_step_end","text":"def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"test_step_end"},{"location":"reference/ophthalmology/modules/disease_grading/#to","text":"def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16","title":"to"},{"location":"reference/ophthalmology/modules/disease_grading/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/modules/disease_grading/#to_onnx","text":"def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None","title":"to_onnx"},{"location":"reference/ophthalmology/modules/disease_grading/#to_torchscript","text":"def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not.","title":"to_torchscript"},{"location":"reference/ophthalmology/modules/disease_grading/#toggle_optimizer","text":"def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None","title":"toggle_optimizer"},{"location":"reference/ophthalmology/modules/disease_grading/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/modules/disease_grading/#train_dataloader","text":"def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set","title":"train_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#training_epoch_end","text":"def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None |","title":"training_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#training_step","text":"def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.","title":"training_step"},{"location":"reference/ophthalmology/modules/disease_grading/#training_step_end","text":"def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None |","title":"training_step_end"},{"location":"reference/ophthalmology/modules/disease_grading/#transfer_batch_to_device","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/modules/disease_grading/#type","text":"def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/modules/disease_grading/#unfreeze","text":"def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze()","title":"unfreeze"},{"location":"reference/ophthalmology/modules/disease_grading/#untoggle_optimizer","text":"def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None","title":"untoggle_optimizer"},{"location":"reference/ophthalmology/modules/disease_grading/#val_dataloader","text":"def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set","title":"val_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#validation_epoch_end","text":"def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None |","title":"validation_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#validation_step","text":"def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.","title":"validation_step"},{"location":"reference/ophthalmology/modules/disease_grading/#validation_step_end","text":"def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"validation_step_end"},{"location":"reference/ophthalmology/modules/disease_grading/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/modules/disease_grading/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/modules/disease_grading/#diseasegradingregression","text":"class DiseaseGradingRegression ( model : torch . nn . modules . module . Module , loss : torch . nn . modules . module . Module , num_train_samples : int , learning_rate : float = 0.003 , batch_size : int = 32 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 120 , weight_decay : float = 1e-05 )","title":"DiseaseGradingRegression"},{"location":"reference/ophthalmology/modules/disease_grading/#ancestors-in-mro_1","text":"pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/modules/disease_grading/#class-variables_1","text":"CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/modules/disease_grading/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/ophthalmology/modules/disease_grading/#load_from_checkpoint_1","text":"def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x)","title":"load_from_checkpoint"},{"location":"reference/ophthalmology/modules/disease_grading/#instance-variables_1","text":"automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the","title":"Instance variables"},{"location":"reference/ophthalmology/modules/disease_grading/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/modules/disease_grading/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/modules/disease_grading/#add_to_queue_1","text":"def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7.","title":"add_to_queue"},{"location":"reference/ophthalmology/modules/disease_grading/#all_gather_1","text":"def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape.","title":"all_gather"},{"location":"reference/ophthalmology/modules/disease_grading/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/modules/disease_grading/#backward_1","text":"def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None","title":"backward"},{"location":"reference/ophthalmology/modules/disease_grading/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/modules/disease_grading/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/modules/disease_grading/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/modules/disease_grading/#clip_gradients_1","text":"def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None","title":"clip_gradients"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_callbacks_1","text":"def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here.","title":"configure_callbacks"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_gradient_clipping_1","text":"def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0)","title":"configure_gradient_clipping"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_optimizers_1","text":"def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the","title":"configure_optimizers"},{"location":"reference/ophthalmology/modules/disease_grading/#configure_sharded_model_1","text":"def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent.","title":"configure_sharded_model"},{"location":"reference/ophthalmology/modules/disease_grading/#cpu_1","text":"def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/modules/disease_grading/#cuda_1","text":"def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/modules/disease_grading/#double_1","text":"def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/modules/disease_grading/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/modules/disease_grading/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/modules/disease_grading/#float_1","text":"def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/modules/disease_grading/#forward_1","text":"def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output","title":"forward"},{"location":"reference/ophthalmology/modules/disease_grading/#freeze_1","text":"def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze()","title":"freeze"},{"location":"reference/ophthalmology/modules/disease_grading/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/modules/disease_grading/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/modules/disease_grading/#get_from_queue_1","text":"def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7.","title":"get_from_queue"},{"location":"reference/ophthalmology/modules/disease_grading/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/modules/disease_grading/#get_progress_bar_dict_1","text":"def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]","title":"get_progress_bar_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/modules/disease_grading/#half_1","text":"def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/modules/disease_grading/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#log_1","text":"def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log"},{"location":"reference/ophthalmology/modules/disease_grading/#log_dict_1","text":"def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#log_grad_norm_1","text":"def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True)","title":"log_grad_norm"},{"location":"reference/ophthalmology/modules/disease_grading/#lr_schedulers_1","text":"def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers .","title":"lr_schedulers"},{"location":"reference/ophthalmology/modules/disease_grading/#manual_backward_1","text":"def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None","title":"manual_backward"},{"location":"reference/ophthalmology/modules/disease_grading/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/modules/disease_grading/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/modules/disease_grading/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/modules/disease_grading/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/modules/disease_grading/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/modules/disease_grading/#on_after_backward_1","text":"def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients.","title":"on_after_backward"},{"location":"reference/ophthalmology/modules/disease_grading/#on_after_batch_transfer_1","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_backward_1","text":"def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None","title":"on_before_backward"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_batch_transfer_1","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_optimizer_step_1","text":"def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/modules/disease_grading/#on_before_zero_grad_1","text":"def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/modules/disease_grading/#on_epoch_end_1","text":"def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_epoch_start_1","text":"def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_fit_end_1","text":"def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process","title":"on_fit_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_fit_start_1","text":"def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process","title":"on_fit_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_hpc_load_1","text":"def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None","title":"on_hpc_load"},{"location":"reference/ophthalmology/modules/disease_grading/#on_hpc_save_1","text":"def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None","title":"on_hpc_save"},{"location":"reference/ophthalmology/modules/disease_grading/#on_load_checkpoint_1","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/modules/disease_grading/#on_post_move_to_device_1","text":"def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after","title":"on_post_move_to_device"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_batch_end_1","text":"def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_batch_start_1","text":"def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_dataloader_1","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_end_1","text":"def on_predict_end ( self ) -> None Called at the end of predicting.","title":"on_predict_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_epoch_end_1","text":"def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_epoch_start_1","text":"def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_model_eval_1","text":"def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop.","title":"on_predict_model_eval"},{"location":"reference/ophthalmology/modules/disease_grading/#on_predict_start_1","text":"def on_predict_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_pretrain_routine_end_1","text":"def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_pretrain_routine_start_1","text":"def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_save_checkpoint_1","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_batch_end_1","text":"def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_batch_start_1","text":"def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_dataloader_1","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_end_1","text":"def on_test_end ( self ) Called at the end of testing.","title":"on_test_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_epoch_end_1","text":"def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_epoch_start_1","text":"def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_model_eval_1","text":"def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop.","title":"on_test_model_eval"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_model_train_1","text":"def on_test_model_train ( self ) -> None Sets the model to train during the test loop.","title":"on_test_model_train"},{"location":"reference/ophthalmology/modules/disease_grading/#on_test_start_1","text":"def on_test_start ( self ) -> None Called at the beginning of testing.","title":"on_test_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_batch_end_1","text":"def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_batch_start_1","text":"def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_dataloader_1","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_end_1","text":"def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed.","title":"on_train_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_epoch_end_1","text":"def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_epoch_start_1","text":"def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_train_start_1","text":"def on_train_start ( self ) -> None Called at the beginning of training after sanity check.","title":"on_train_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_val_dataloader_1","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_batch_end_1","text":"def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_batch_start_1","text":"def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_end_1","text":"def on_validation_end ( self ) -> None Called at the end of validation.","title":"on_validation_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_epoch_end_1","text":"def on_validation_epoch_end ( self ) Called in the validation loop at the very end of the epoch.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_epoch_start_1","text":"def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_model_eval_1","text":"def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop.","title":"on_validation_model_eval"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_model_train_1","text":"def on_validation_model_train ( self ) -> None Sets the model to train during the val loop.","title":"on_validation_model_train"},{"location":"reference/ophthalmology/modules/disease_grading/#on_validation_start_1","text":"def on_validation_start ( self ) -> None Called at the beginning of validation.","title":"on_validation_start"},{"location":"reference/ophthalmology/modules/disease_grading/#optimizer_step_1","text":"def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure)","title":"optimizer_step"},{"location":"reference/ophthalmology/modules/disease_grading/#optimizer_zero_grad_1","text":"def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example.","title":"optimizer_zero_grad"},{"location":"reference/ophthalmology/modules/disease_grading/#optimizers_1","text":"def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present.","title":"optimizers"},{"location":"reference/ophthalmology/modules/disease_grading/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/modules/disease_grading/#predict_dataloader_1","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#predict_step_1","text":"def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output","title":"predict_step"},{"location":"reference/ophthalmology/modules/disease_grading/#prepare_data_1","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/modules/disease_grading/#print_1","text":"def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None","title":"print"},{"location":"reference/ophthalmology/modules/disease_grading/#quantize","text":"def quantize ( self , preds )","title":"quantize"},{"location":"reference/ophthalmology/modules/disease_grading/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/modules/disease_grading/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/modules/disease_grading/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/modules/disease_grading/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/modules/disease_grading/#save_hyperparameters_1","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/modules/disease_grading/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/modules/disease_grading/#setup_1","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/modules/disease_grading/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/modules/disease_grading/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/modules/disease_grading/#summarize_1","text":"def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object","title":"summarize"},{"location":"reference/ophthalmology/modules/disease_grading/#tbptt_split_batch_1","text":"def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step .","title":"tbptt_split_batch"},{"location":"reference/ophthalmology/modules/disease_grading/#teardown_1","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/modules/disease_grading/#test_dataloader_1","text":"def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set","title":"test_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#test_epoch_end_1","text":"def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None |","title":"test_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#test_step_1","text":"def test_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled.","title":"test_step"},{"location":"reference/ophthalmology/modules/disease_grading/#test_step_end_1","text":"def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"test_step_end"},{"location":"reference/ophthalmology/modules/disease_grading/#to_1","text":"def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16","title":"to"},{"location":"reference/ophthalmology/modules/disease_grading/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/modules/disease_grading/#to_onnx_1","text":"def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None","title":"to_onnx"},{"location":"reference/ophthalmology/modules/disease_grading/#to_torchscript_1","text":"def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not.","title":"to_torchscript"},{"location":"reference/ophthalmology/modules/disease_grading/#toggle_optimizer_1","text":"def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None","title":"toggle_optimizer"},{"location":"reference/ophthalmology/modules/disease_grading/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/modules/disease_grading/#train_dataloader_1","text":"def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set","title":"train_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#training_epoch_end_1","text":"def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None |","title":"training_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#training_step_1","text":"def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.","title":"training_step"},{"location":"reference/ophthalmology/modules/disease_grading/#training_step_end_1","text":"def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None |","title":"training_step_end"},{"location":"reference/ophthalmology/modules/disease_grading/#transfer_batch_to_device_1","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/modules/disease_grading/#type_1","text":"def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/modules/disease_grading/#unfreeze_1","text":"def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze()","title":"unfreeze"},{"location":"reference/ophthalmology/modules/disease_grading/#untoggle_optimizer_1","text":"def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None","title":"untoggle_optimizer"},{"location":"reference/ophthalmology/modules/disease_grading/#val_dataloader_1","text":"def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set","title":"val_dataloader"},{"location":"reference/ophthalmology/modules/disease_grading/#validation_epoch_end_1","text":"def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None |","title":"validation_epoch_end"},{"location":"reference/ophthalmology/modules/disease_grading/#validation_step_1","text":"def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.","title":"validation_step"},{"location":"reference/ophthalmology/modules/disease_grading/#validation_step_end_1","text":"def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"validation_step_end"},{"location":"reference/ophthalmology/modules/disease_grading/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/modules/disease_grading/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/modules/localization/","text":"Module ophthalmology.modules.localization Supervised Optical Disk and Fovea Localization Task. pytorch-lightning module for the supervised localization task. Classes Localization class Localization ( model : torch . nn . modules . module . Module , loss : torch . nn . modules . module . Module , num_train_samples : int , learning_rate : float = 0.003 , batch_size : int = 32 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 120 , weight_decay : float = 1e-05 ) Ancestors (in MRO) pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module Class variables CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches Static methods load_from_checkpoint def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) Instance variables automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None add_to_queue def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7. all_gather def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | backward def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module clip_gradients def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None configure_callbacks def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. configure_gradient_clipping def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0) configure_optimizers def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the configure_sharded_model def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent. cpu def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self cuda def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self forward def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output freeze def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_from_queue def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7. get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_progress_bar_dict def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | log def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_dict def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_grad_norm def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) lr_schedulers def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers . manual_backward def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | on_after_backward def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_backward def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_before_optimizer_step def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None on_before_zero_grad def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None on_epoch_end def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins. on_fit_end def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process on_fit_start def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process on_hpc_load def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None on_hpc_save def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_post_move_to_device def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after on_predict_batch_end def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_batch_start def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_predict_end def on_predict_end ( self ) -> None Called at the end of predicting. on_predict_epoch_end def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting. on_predict_epoch_start def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting. on_predict_model_eval def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop. on_predict_start def on_predict_start ( self ) -> None Called at the beginning of predicting. on_pretrain_routine_end def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_pretrain_routine_start def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_batch_end def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_batch_start def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_test_end def on_test_end ( self ) -> None Called at the end of testing. on_test_epoch_end def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch. on_test_epoch_start def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch. on_test_model_eval def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop. on_test_model_train def on_test_model_train ( self ) -> None Sets the model to train during the test loop. on_test_start def on_test_start ( self ) -> None Called at the beginning of testing. on_train_batch_end def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_batch_start def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_train_end def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. on_train_epoch_end def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook on_train_epoch_start def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. on_train_start def on_train_start ( self ) -> None Called at the beginning of training after sanity check. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. on_validation_batch_end def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_batch_start def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_end def on_validation_end ( self ) -> None Called at the end of validation. on_validation_epoch_end def on_validation_epoch_end ( self ) -> None Called in the validation loop at the very end of the epoch. on_validation_epoch_start def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch. on_validation_model_eval def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop. on_validation_model_train def on_validation_model_train ( self ) -> None Sets the model to train during the val loop. on_validation_start def on_validation_start ( self ) -> None Called at the beginning of validation. optimizer_step def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) optimizer_zero_grad def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. optimizers def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present. parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. predict_step def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() print def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | summarize def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object tbptt_split_batch def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set test_epoch_end def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None | test_step def test_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. test_step_end def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. to def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self to_onnx def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None to_torchscript def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not. toggle_optimizer def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self train_dataloader def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set training_epoch_end def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None | training_step def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. training_step_end def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None | transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | type def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self unfreeze def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() untoggle_optimizer def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None val_dataloader def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set validation_epoch_end def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None | validation_step def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. validation_step_end def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Localization"},{"location":"reference/ophthalmology/modules/localization/#module-ophthalmologymoduleslocalization","text":"","title":"Module ophthalmology.modules.localization"},{"location":"reference/ophthalmology/modules/localization/#supervised-optical-disk-and-fovea-localization-task","text":"pytorch-lightning module for the supervised localization task.","title":"Supervised Optical Disk and Fovea Localization Task."},{"location":"reference/ophthalmology/modules/localization/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/modules/localization/#localization","text":"class Localization ( model : torch . nn . modules . module . Module , loss : torch . nn . modules . module . Module , num_train_samples : int , learning_rate : float = 0.003 , batch_size : int = 32 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 120 , weight_decay : float = 1e-05 )","title":"Localization"},{"location":"reference/ophthalmology/modules/localization/#ancestors-in-mro","text":"pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/modules/localization/#class-variables","text":"CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/modules/localization/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/modules/localization/#load_from_checkpoint","text":"def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x)","title":"load_from_checkpoint"},{"location":"reference/ophthalmology/modules/localization/#instance-variables","text":"automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the","title":"Instance variables"},{"location":"reference/ophthalmology/modules/localization/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/modules/localization/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/modules/localization/#add_to_queue","text":"def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7.","title":"add_to_queue"},{"location":"reference/ophthalmology/modules/localization/#all_gather","text":"def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape.","title":"all_gather"},{"location":"reference/ophthalmology/modules/localization/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/modules/localization/#backward","text":"def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None","title":"backward"},{"location":"reference/ophthalmology/modules/localization/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/modules/localization/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/modules/localization/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/modules/localization/#clip_gradients","text":"def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None","title":"clip_gradients"},{"location":"reference/ophthalmology/modules/localization/#configure_callbacks","text":"def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here.","title":"configure_callbacks"},{"location":"reference/ophthalmology/modules/localization/#configure_gradient_clipping","text":"def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0)","title":"configure_gradient_clipping"},{"location":"reference/ophthalmology/modules/localization/#configure_optimizers","text":"def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the","title":"configure_optimizers"},{"location":"reference/ophthalmology/modules/localization/#configure_sharded_model","text":"def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent.","title":"configure_sharded_model"},{"location":"reference/ophthalmology/modules/localization/#cpu","text":"def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/modules/localization/#cuda","text":"def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/modules/localization/#double","text":"def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/modules/localization/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/modules/localization/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/modules/localization/#float","text":"def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/modules/localization/#forward","text":"def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output","title":"forward"},{"location":"reference/ophthalmology/modules/localization/#freeze","text":"def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze()","title":"freeze"},{"location":"reference/ophthalmology/modules/localization/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/modules/localization/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/modules/localization/#get_from_queue","text":"def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7.","title":"get_from_queue"},{"location":"reference/ophthalmology/modules/localization/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/modules/localization/#get_progress_bar_dict","text":"def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]","title":"get_progress_bar_dict"},{"location":"reference/ophthalmology/modules/localization/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/modules/localization/#half","text":"def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/modules/localization/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/modules/localization/#log","text":"def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log"},{"location":"reference/ophthalmology/modules/localization/#log_dict","text":"def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log_dict"},{"location":"reference/ophthalmology/modules/localization/#log_grad_norm","text":"def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True)","title":"log_grad_norm"},{"location":"reference/ophthalmology/modules/localization/#lr_schedulers","text":"def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers .","title":"lr_schedulers"},{"location":"reference/ophthalmology/modules/localization/#manual_backward","text":"def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None","title":"manual_backward"},{"location":"reference/ophthalmology/modules/localization/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/modules/localization/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/modules/localization/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/modules/localization/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/modules/localization/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/modules/localization/#on_after_backward","text":"def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients.","title":"on_after_backward"},{"location":"reference/ophthalmology/modules/localization/#on_after_batch_transfer","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/modules/localization/#on_before_backward","text":"def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None","title":"on_before_backward"},{"location":"reference/ophthalmology/modules/localization/#on_before_batch_transfer","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/modules/localization/#on_before_optimizer_step","text":"def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/modules/localization/#on_before_zero_grad","text":"def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/modules/localization/#on_epoch_end","text":"def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#on_epoch_start","text":"def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/modules/localization/#on_fit_end","text":"def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process","title":"on_fit_end"},{"location":"reference/ophthalmology/modules/localization/#on_fit_start","text":"def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process","title":"on_fit_start"},{"location":"reference/ophthalmology/modules/localization/#on_hpc_load","text":"def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None","title":"on_hpc_load"},{"location":"reference/ophthalmology/modules/localization/#on_hpc_save","text":"def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None","title":"on_hpc_save"},{"location":"reference/ophthalmology/modules/localization/#on_load_checkpoint","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/modules/localization/#on_post_move_to_device","text":"def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after","title":"on_post_move_to_device"},{"location":"reference/ophthalmology/modules/localization/#on_predict_batch_end","text":"def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/modules/localization/#on_predict_batch_start","text":"def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/modules/localization/#on_predict_dataloader","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/modules/localization/#on_predict_end","text":"def on_predict_end ( self ) -> None Called at the end of predicting.","title":"on_predict_end"},{"location":"reference/ophthalmology/modules/localization/#on_predict_epoch_end","text":"def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#on_predict_epoch_start","text":"def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/modules/localization/#on_predict_model_eval","text":"def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop.","title":"on_predict_model_eval"},{"location":"reference/ophthalmology/modules/localization/#on_predict_start","text":"def on_predict_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_start"},{"location":"reference/ophthalmology/modules/localization/#on_pretrain_routine_end","text":"def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/modules/localization/#on_pretrain_routine_start","text":"def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/modules/localization/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/modules/localization/#on_test_batch_end","text":"def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_end"},{"location":"reference/ophthalmology/modules/localization/#on_test_batch_start","text":"def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_start"},{"location":"reference/ophthalmology/modules/localization/#on_test_dataloader","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/modules/localization/#on_test_end","text":"def on_test_end ( self ) -> None Called at the end of testing.","title":"on_test_end"},{"location":"reference/ophthalmology/modules/localization/#on_test_epoch_end","text":"def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#on_test_epoch_start","text":"def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/modules/localization/#on_test_model_eval","text":"def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop.","title":"on_test_model_eval"},{"location":"reference/ophthalmology/modules/localization/#on_test_model_train","text":"def on_test_model_train ( self ) -> None Sets the model to train during the test loop.","title":"on_test_model_train"},{"location":"reference/ophthalmology/modules/localization/#on_test_start","text":"def on_test_start ( self ) -> None Called at the beginning of testing.","title":"on_test_start"},{"location":"reference/ophthalmology/modules/localization/#on_train_batch_end","text":"def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_end"},{"location":"reference/ophthalmology/modules/localization/#on_train_batch_start","text":"def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_start"},{"location":"reference/ophthalmology/modules/localization/#on_train_dataloader","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/modules/localization/#on_train_end","text":"def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed.","title":"on_train_end"},{"location":"reference/ophthalmology/modules/localization/#on_train_epoch_end","text":"def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#on_train_epoch_start","text":"def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/modules/localization/#on_train_start","text":"def on_train_start ( self ) -> None Called at the beginning of training after sanity check.","title":"on_train_start"},{"location":"reference/ophthalmology/modules/localization/#on_val_dataloader","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/modules/localization/#on_validation_batch_end","text":"def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/modules/localization/#on_validation_batch_start","text":"def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/modules/localization/#on_validation_end","text":"def on_validation_end ( self ) -> None Called at the end of validation.","title":"on_validation_end"},{"location":"reference/ophthalmology/modules/localization/#on_validation_epoch_end","text":"def on_validation_epoch_end ( self ) -> None Called in the validation loop at the very end of the epoch.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#on_validation_epoch_start","text":"def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/modules/localization/#on_validation_model_eval","text":"def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop.","title":"on_validation_model_eval"},{"location":"reference/ophthalmology/modules/localization/#on_validation_model_train","text":"def on_validation_model_train ( self ) -> None Sets the model to train during the val loop.","title":"on_validation_model_train"},{"location":"reference/ophthalmology/modules/localization/#on_validation_start","text":"def on_validation_start ( self ) -> None Called at the beginning of validation.","title":"on_validation_start"},{"location":"reference/ophthalmology/modules/localization/#optimizer_step","text":"def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure)","title":"optimizer_step"},{"location":"reference/ophthalmology/modules/localization/#optimizer_zero_grad","text":"def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example.","title":"optimizer_zero_grad"},{"location":"reference/ophthalmology/modules/localization/#optimizers","text":"def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present.","title":"optimizers"},{"location":"reference/ophthalmology/modules/localization/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/modules/localization/#predict_dataloader","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/modules/localization/#predict_step","text":"def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output","title":"predict_step"},{"location":"reference/ophthalmology/modules/localization/#prepare_data","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/modules/localization/#print","text":"def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None","title":"print"},{"location":"reference/ophthalmology/modules/localization/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/modules/localization/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/modules/localization/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/modules/localization/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/modules/localization/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/modules/localization/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/modules/localization/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/modules/localization/#save_hyperparameters","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/modules/localization/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/modules/localization/#setup","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/modules/localization/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/modules/localization/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/modules/localization/#summarize","text":"def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object","title":"summarize"},{"location":"reference/ophthalmology/modules/localization/#tbptt_split_batch","text":"def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step .","title":"tbptt_split_batch"},{"location":"reference/ophthalmology/modules/localization/#teardown","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/modules/localization/#test_dataloader","text":"def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set","title":"test_dataloader"},{"location":"reference/ophthalmology/modules/localization/#test_epoch_end","text":"def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None |","title":"test_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#test_step","text":"def test_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled.","title":"test_step"},{"location":"reference/ophthalmology/modules/localization/#test_step_end","text":"def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"test_step_end"},{"location":"reference/ophthalmology/modules/localization/#to","text":"def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16","title":"to"},{"location":"reference/ophthalmology/modules/localization/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/modules/localization/#to_onnx","text":"def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None","title":"to_onnx"},{"location":"reference/ophthalmology/modules/localization/#to_torchscript","text":"def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not.","title":"to_torchscript"},{"location":"reference/ophthalmology/modules/localization/#toggle_optimizer","text":"def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None","title":"toggle_optimizer"},{"location":"reference/ophthalmology/modules/localization/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/modules/localization/#train_dataloader","text":"def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set","title":"train_dataloader"},{"location":"reference/ophthalmology/modules/localization/#training_epoch_end","text":"def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None |","title":"training_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#training_step","text":"def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.","title":"training_step"},{"location":"reference/ophthalmology/modules/localization/#training_step_end","text":"def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None |","title":"training_step_end"},{"location":"reference/ophthalmology/modules/localization/#transfer_batch_to_device","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/modules/localization/#type","text":"def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/modules/localization/#unfreeze","text":"def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze()","title":"unfreeze"},{"location":"reference/ophthalmology/modules/localization/#untoggle_optimizer","text":"def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None","title":"untoggle_optimizer"},{"location":"reference/ophthalmology/modules/localization/#val_dataloader","text":"def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set","title":"val_dataloader"},{"location":"reference/ophthalmology/modules/localization/#validation_epoch_end","text":"def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None |","title":"validation_epoch_end"},{"location":"reference/ophthalmology/modules/localization/#validation_step","text":"def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.","title":"validation_step"},{"location":"reference/ophthalmology/modules/localization/#validation_step_end","text":"def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"validation_step_end"},{"location":"reference/ophthalmology/modules/localization/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/modules/localization/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/modules/simclr/","text":"Module ophthalmology.modules.simclr Contrastive representation learning task SimCLR Pytorch Lightning implementation adopted from: https://github.com/PyTorchLightning/Lightning-Bolts/blob/master/pl_bolts/models/self_supervised/simclr/simclr_module.py#L61-L300 Classes NTXentLoss class NTXentLoss ( temperature : float = 0.1 , eps : float = 1e-06 ) Ancestors (in MRO) torch.nn.modules.module.Module Class variables T_destination dump_patches Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module cpu def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self cuda def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self forward def forward ( self , out_1 , out_2 ) assume out_1 and out_2 are normalized out_1: [batch_size, dim] out_2: [batch_size, dim] get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | to def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) | to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self type def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None SimCLR class SimCLR ( model : torch . nn . modules . module . Module , learning_rate : float = 0.001 , batch_size : int = 16 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 60 , num_train_samples : int = 15378 , num_features : int = 64 , num_hidden_projection_features : int = 265 , num_projection_features : int = 128 , weight_decay : float = 1e-05 , temperature : float = 0.1 ) Ancestors (in MRO) pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module Class variables CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches Static methods load_from_checkpoint def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x) Instance variables automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the Methods add_module def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None add_to_queue def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7. all_gather def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape. apply def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) | backward def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None bfloat16 def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self buffers def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | children def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module clip_gradients def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None configure_callbacks def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here. configure_gradient_clipping def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0) configure_optimizers def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the configure_sharded_model def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent. cpu def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self cuda def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self double def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self eval def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self exclude_from_wt_decay def exclude_from_wt_decay ( self , named_params , weight_decay , skip_list = ( 'bias' , 'bn' ) ) Prepare the parameters to only weight-decay non bias and bn layers. This has proven to be a best practice. Code from: https://github.com/PyTorchLightning/lightning-bolts/blob/master/pl_bolts/models/self_supervised/simclr/simclr_module.py Parameters: Name Type Description Default named_params [type] [description] None weight_decay [type] [description] None skip_list tuple [description]. Defaults to (\"bias\", \"bn\"). (\"bias\", \"bn\") Returns: Type Description [type] [description] extra_repr def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self forward def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output freeze def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze() get_buffer def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer get_extra_state def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict get_from_queue def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7. get_parameter def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter get_progress_bar_dict def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]] get_submodule def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module half def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self load_state_dict def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . | log def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_dict def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None log_grad_norm def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True) lr_schedulers def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers . manual_backward def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None modules def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) | named_buffers def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) | named_children def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) | named_modules def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) named_parameters def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) | on_after_backward def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients. on_after_batch_transfer def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device | on_before_backward def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None on_before_batch_transfer def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device | on_before_optimizer_step def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None on_before_zero_grad def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None on_epoch_end def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends. on_epoch_start def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins. on_fit_end def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process on_fit_start def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process on_hpc_load def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None on_hpc_save def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None on_load_checkpoint def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training. on_post_move_to_device def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after on_predict_batch_end def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_batch_start def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_predict_dataloader def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader. on_predict_end def on_predict_end ( self ) -> None Called at the end of predicting. on_predict_epoch_end def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting. on_predict_epoch_start def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting. on_predict_model_eval def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop. on_predict_start def on_predict_start ( self ) -> None Called at the beginning of predicting. on_pretrain_routine_end def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_pretrain_routine_start def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start on_save_checkpoint def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training. on_test_batch_end def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_batch_start def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_test_dataloader def on_test_dataloader ( self ) -> None Called before requesting the test dataloader. on_test_end def on_test_end ( self ) -> None Called at the end of testing. on_test_epoch_end def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch. on_test_epoch_start def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch. on_test_model_eval def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop. on_test_model_train def on_test_model_train ( self ) -> None Sets the model to train during the test loop. on_test_start def on_test_start ( self ) -> None Called at the beginning of testing. on_train_batch_end def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_batch_start def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None on_train_dataloader def on_train_dataloader ( self ) -> None Called before requesting the train dataloader. on_train_end def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed. on_train_epoch_end def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook on_train_epoch_start def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch. on_train_start def on_train_start ( self ) -> None Called at the beginning of training after sanity check. on_val_dataloader def on_val_dataloader ( self ) -> None Called before requesting the val dataloader. on_validation_batch_end def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_batch_start def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None on_validation_end def on_validation_end ( self ) -> None Called at the end of validation. on_validation_epoch_end def on_validation_epoch_end ( self ) -> None Called in the validation loop at the very end of the epoch. on_validation_epoch_start def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch. on_validation_model_eval def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop. on_validation_model_train def on_validation_model_train ( self ) -> None Sets the model to train during the val loop. on_validation_start def on_validation_start ( self ) -> None Called at the beginning of validation. optimizer_step def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure) optimizer_zero_grad def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example. optimizers def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present. parameters def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) | predict_dataloader def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here. predict_step def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output prepare_data def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader() print def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None register_backward_hook def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_buffer def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) register_forward_hook def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after register_forward_pre_hook def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_full_backward_hook def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() register_parameter def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None requires_grad_ def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self save_hyperparameters def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 set_extra_state def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding setup def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None | share_memory def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_ state_dict def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] | summarize def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object tbptt_split_batch def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step . teardown def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None test_dataloader def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set test_epoch_end def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None | test_step def test_step ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled. test_step_end def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. to def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16 to_empty def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self to_onnx def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None to_torchscript def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not. toggle_optimizer def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None train def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self train_dataloader def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set training_epoch_end def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None | training_step def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. training_step_end def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None | transfer_batch_to_device def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection | type def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self unfreeze def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze() untoggle_optimizer def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None val_dataloader def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set validation_epoch_end def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None | validation_step def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. validation_step_end def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details. xpu def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self zero_grad def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"Simclr"},{"location":"reference/ophthalmology/modules/simclr/#module-ophthalmologymodulessimclr","text":"","title":"Module ophthalmology.modules.simclr"},{"location":"reference/ophthalmology/modules/simclr/#contrastive-representation-learning-task","text":"SimCLR Pytorch Lightning implementation adopted from: https://github.com/PyTorchLightning/Lightning-Bolts/blob/master/pl_bolts/models/self_supervised/simclr/simclr_module.py#L61-L300","title":"Contrastive representation learning task"},{"location":"reference/ophthalmology/modules/simclr/#classes","text":"","title":"Classes"},{"location":"reference/ophthalmology/modules/simclr/#ntxentloss","text":"class NTXentLoss ( temperature : float = 0.1 , eps : float = 1e-06 )","title":"NTXentLoss"},{"location":"reference/ophthalmology/modules/simclr/#ancestors-in-mro","text":"torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/modules/simclr/#class-variables","text":"T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/modules/simclr/#methods","text":"","title":"Methods"},{"location":"reference/ophthalmology/modules/simclr/#add_module","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/modules/simclr/#apply","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/modules/simclr/#bfloat16","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/modules/simclr/#buffers","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/modules/simclr/#children","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/modules/simclr/#cpu","text":"def cpu ( self : ~ T ) -> ~ T Moves all model parameters and buffers to the CPU. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/modules/simclr/#cuda","text":"def cuda ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/modules/simclr/#double","text":"def double ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to double datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/modules/simclr/#eval","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/modules/simclr/#extra_repr","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/modules/simclr/#float","text":"def float ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to float datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/modules/simclr/#forward","text":"def forward ( self , out_1 , out_2 ) assume out_1 and out_2 are normalized out_1: [batch_size, dim] out_2: [batch_size, dim]","title":"forward"},{"location":"reference/ophthalmology/modules/simclr/#get_buffer","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/modules/simclr/#get_extra_state","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/modules/simclr/#get_parameter","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/modules/simclr/#get_submodule","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/modules/simclr/#half","text":"def half ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to half datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/modules/simclr/#load_state_dict","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/modules/simclr/#modules","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/modules/simclr/#named_buffers","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/modules/simclr/#named_children","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/modules/simclr/#named_modules","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/modules/simclr/#named_parameters","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/modules/simclr/#parameters","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/modules/simclr/#register_backward_hook","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_buffer","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/modules/simclr/#register_forward_hook","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_forward_pre_hook","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_full_backward_hook","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_parameter","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/modules/simclr/#requires_grad_","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/modules/simclr/#set_extra_state","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/modules/simclr/#share_memory","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/modules/simclr/#state_dict","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/modules/simclr/#to","text":"def to ( self , * args , ** kwargs ) Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) :noindex: .. function:: to(dtype, non_blocking=False) :noindex: .. function:: to(tensor, non_blocking=False) :noindex: .. function:: to(memory_format=torch.channels_last) :noindex: Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype \\ s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device ( None class: torch.device ): the desired device of the parameters and buffers in this module None dtype ( None class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module None tensor torch.Tensor Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None memory_format ( None class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) None Returns: Type Description Module self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) |","title":"to"},{"location":"reference/ophthalmology/modules/simclr/#to_empty","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/modules/simclr/#train","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/modules/simclr/#type","text":"def type ( self : ~ T , dst_type : Union [ torch . dtype , str ] ) -> ~ T Casts all parameters and buffers to :attr: dst_type . .. note:: This method modifies the module in-place. Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/modules/simclr/#xpu","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/modules/simclr/#zero_grad","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"},{"location":"reference/ophthalmology/modules/simclr/#simclr","text":"class SimCLR ( model : torch . nn . modules . module . Module , learning_rate : float = 0.001 , batch_size : int = 16 , print_model_info_for_input : Union [ Tuple [ int , ... ], List [ int ], NoneType ] = None , epochs : int = 60 , num_train_samples : int = 15378 , num_features : int = 64 , num_hidden_projection_features : int = 265 , num_projection_features : int = 128 , weight_decay : float = 1e-05 , temperature : float = 0.1 )","title":"SimCLR"},{"location":"reference/ophthalmology/modules/simclr/#ancestors-in-mro_1","text":"pytorch_lightning.core.lightning.LightningModule pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin pytorch_lightning.core.saving.ModelIO pytorch_lightning.core.hooks.ModelHooks pytorch_lightning.core.hooks.DataHooks pytorch_lightning.core.hooks.CheckpointHooks torch.nn.modules.module.Module","title":"Ancestors (in MRO)"},{"location":"reference/ophthalmology/modules/simclr/#class-variables_1","text":"CHECKPOINT_HYPER_PARAMS_KEY CHECKPOINT_HYPER_PARAMS_NAME CHECKPOINT_HYPER_PARAMS_TYPE T_destination dump_patches","title":"Class variables"},{"location":"reference/ophthalmology/modules/simclr/#static-methods","text":"","title":"Static methods"},{"location":"reference/ophthalmology/modules/simclr/#load_from_checkpoint","text":"def load_from_checkpoint ( checkpoint_path : Union [ str , IO ], map_location : Union [ Dict [ str , str ], str , torch . device , int , Callable , NoneType ] = None , hparams_file : Optional [ str ] = None , strict : bool = True , ** kwargs ) Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments passed to __init__ in the checkpoint under hyper_parameters Any arguments specified through *args and **kwargs will override args stored in hyper_parameters . Args: checkpoint_path: Path to checkpoint. This can also be a URL, or file-like object map_location: If your checkpoint saved a GPU model and you now load on CPUs or a different number of GPUs, use this to map to the new setup. The behaviour is the same as in :func: torch.load . hparams_file: Optional path to a .yaml file with hierarchical structure as in this example:: drop_prob: 0.2 dataloader: batch_size: 32 You most likely won't need this since Lightning will always save the hyperparameters to the checkpoint. However, if your checkpoint weights don't have the hyperparameters saved, use this method to pass in a .yaml file with the hparams you'd like to use. These will be converted into a :class:`~dict` and passed into your :class:`LightningModule` for use. If your model's `hparams` argument is :class:`~argparse.Namespace` and .yaml file has hierarchical structure, you need to refactor your model to treat `hparams` as :class:`~dict`. strict: Whether to strictly enforce that the keys in :attr:`checkpoint_path` match the keys returned by this module's state dict. Default: `True`. kwargs: Any extra keyword args needed to init the model. Can also be used to override saved hyperparameter values. Return: :class: LightningModule with loaded weights and hyperparameters (if available). Example:: # load weights without mapping ... MyLightningModule.load_from_checkpoint('path/to/checkpoint.ckpt') # or load weights mapping all weights from GPU 1 to GPU 0 ... map_location = {'cuda:1':'cuda:0'} MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', map_location=map_location ) # or load weights and hyperparameters from separate files. MyLightningModule.load_from_checkpoint( 'path/to/checkpoint.ckpt', hparams_file='/path/to/hparams_file.yaml' ) # override some of the params with new values MyLightningModule.load_from_checkpoint( PATH, num_layers=128, pretrained_ckpt_path=NEW_PATH, ) # predict pretrained_model.eval() pretrained_model.freeze() y_hat = pretrained_model(x)","title":"load_from_checkpoint"},{"location":"reference/ophthalmology/modules/simclr/#instance-variables","text":"automatic_optimization If set to False you are responsible for calling .backward() , .step() , .zero_grad() . current_epoch The current epoch in the Trainer. If no Trainer is attached, this propery is 0. device dtype example_input_array The example input array is a specification of what the module can consume in the :meth: forward method. The return type is interpreted as follows: Single tensor: It is assumed the model takes a single argument, i.e., model.forward(model.example_input_array) Tuple: The input array should be interpreted as a sequence of positional arguments, i.e., model.forward(*model.example_input_array) Dict: The input array represents named keyword arguments, i.e., model.forward(**model.example_input_array) global_rank The index of the current process across all nodes and devices. global_step Total training batches seen across all epochs. If no Trainer is attached, this propery is 0. hparams The collection of hyperparameters saved with :meth: save_hyperparameters . It is mutable by the user. For the frozen set of initial hyperparameters, use :attr: hparams_initial . hparams_initial The collection of hyperparameters saved with :meth: save_hyperparameters . These contents are read-only. Manual updates to the saved hyperparameters can instead be performed through :attr: hparams . loaded_optimizer_states_dict local_rank The index of the current process within a single node. logger Reference to the logger object in the Trainer. model_size Returns the model size in MegaBytes (MB) Note: This property will not return correct value for Deepspeed (stage 3) and fully-sharded training. on_gpu Returns True if this model is currently located on a GPU. Useful to set flags around the LightningModule for different CPU vs GPU behavior. truncated_bptt_steps Enables Truncated Backpropagation Through Time in the Trainer when set to a positive integer. It represents the number of times :meth: training_step gets called before backpropagation. If this is > 0, the","title":"Instance variables"},{"location":"reference/ophthalmology/modules/simclr/#methods_1","text":"","title":"Methods"},{"location":"reference/ophthalmology/modules/simclr/#add_module_1","text":"def add_module ( self , name : str , module : Optional [ ForwardRef ( 'Module' )] ) -> None Adds a child module to the current module. The module can be accessed as an attribute using the given name. Parameters: Name Type Description Default name string name of the child module. The child module can be accessed from this module using the given name None module Module child module to be added to the module. None","title":"add_module"},{"location":"reference/ophthalmology/modules/simclr/#add_to_queue","text":"def add_to_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Appends the :attr: trainer.callback_metrics dictionary to the given queue. To avoid issues with memory sharing, we cast the data to numpy. Args: queue: the instance of the queue to append the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.add_to_queue and will be removed in v1.7.","title":"add_to_queue"},{"location":"reference/ophthalmology/modules/simclr/#all_gather","text":"def all_gather ( self , data : Union [ torch . Tensor , Dict , List , Tuple ], group : Optional [ Any ] = None , sync_grads : bool = False ) Allows users to call self.all_gather() from the LightningModule, thus making the all_gather operation accelerator agnostic. all_gather is a function provided by accelerators to gather a tensor from several distributed processes. Args: data: int, float, tensor of shape (batch, ...), or a (possibly nested) collection thereof. group: the process group to gather results from. Defaults to all processes (world) sync_grads: flag that allows users to synchronize gradients for the all_gather operation Return: A tensor of shape (world_size, batch, ...), or if the input was a collection the output will also be a collection with tensors of this shape.","title":"all_gather"},{"location":"reference/ophthalmology/modules/simclr/#apply_1","text":"def apply ( self : ~ T , fn : Callable [[ ForwardRef ( 'Module' )], NoneType ] ) -> ~ T Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Parameters: Name Type Description Default fn ( None class: Module -> None): function to be applied to each submodule None Returns: Type Description Module self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) |","title":"apply"},{"location":"reference/ophthalmology/modules/simclr/#backward","text":"def backward ( self , loss : torch . Tensor , optimizer : Optional [ torch . optim . optimizer . Optimizer ], optimizer_idx : Optional [ int ], * args , ** kwargs ) -> None Called to perform backward on the loss returned in :meth: training_step . Override this hook with your own implementation if you need to. Parameters: Name Type Description Default loss None The loss tensor returned by :meth: training_step . If gradient accumulation is used, the loss here holds the normalized value (scaled by 1 / accumulation steps). None optimizer None Current optimizer being used. None if using manual optimization. None optimizer_idx None Index of the current optimizer being used. None if using manual optimization. Example:: None def backward self, loss, optimizer, optimizer_idx loss.backward() None","title":"backward"},{"location":"reference/ophthalmology/modules/simclr/#bfloat16_1","text":"def bfloat16 ( self : ~ T ) -> ~ T Casts all floating point parameters and buffers to bfloat16 datatype. .. note:: This method modifies the module in-place. Returns: Type Description Module self","title":"bfloat16"},{"location":"reference/ophthalmology/modules/simclr/#buffers_1","text":"def buffers ( self , recurse : bool = True ) -> Iterator [ torch . Tensor ] Returns an iterator over module buffers. Parameters: Name Type Description Default recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description torch.Tensor module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"buffers"},{"location":"reference/ophthalmology/modules/simclr/#children_1","text":"def children ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over immediate children modules. Yields: Type Description Module a child module","title":"children"},{"location":"reference/ophthalmology/modules/simclr/#clip_gradients","text":"def clip_gradients ( self , optimizer : torch . optim . optimizer . Optimizer , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Handles gradient clipping internally. Note: Do not override this method. If you want to customize gradient clipping, consider using :meth: configure_gradient_clipping method. Parameters: Name Type Description Default optimizer None Current optimizer being used. None gradient_clip_val None The value at which to clip gradients. None gradient_clip_algorithm None The gradient clipping algorithm to use. Pass gradient_clip_algorithm=\"value\" to clip by value, and gradient_clip_algorithm=\"norm\" to clip by norm. None","title":"clip_gradients"},{"location":"reference/ophthalmology/modules/simclr/#configure_callbacks","text":"def configure_callbacks ( self ) Configure model-specific callbacks. When the model gets attached, e.g., when .fit() or .test() gets called, the list returned here will be merged with the list of callbacks passed to the Trainer's callbacks argument. If a callback returned here has the same type as one or several callbacks already present in the Trainer's callbacks list, it will take priority and replace them. In addition, Lightning will make sure :class: ~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint callbacks run last. Return: A list of callbacks which will extend the list of callbacks in the Trainer. Example:: def configure_callbacks(self): early_stop = EarlyStopping(monitor=\"val_acc\", mode=\"max\") checkpoint = ModelCheckpoint(monitor=\"val_loss\") return [early_stop, checkpoint] Note: Certain callback methods like :meth: ~pytorch_lightning.callbacks.base.Callback.on_init_start will never be invoked on the new callbacks returned here.","title":"configure_callbacks"},{"location":"reference/ophthalmology/modules/simclr/#configure_gradient_clipping","text":"def configure_gradient_clipping ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int , gradient_clip_val : Union [ int , float , NoneType ] = None , gradient_clip_algorithm : Optional [ str ] = None ) Perform gradient clipping for the optimizer parameters. Called before :meth: optimizer_step . Args: optimizer: Current optimizer being used. optimizer_idx: Index of the current optimizer being used. gradient_clip_val: The value at which to clip gradients. By default value passed in Trainer will be available here. gradient_clip_algorithm: The gradient clipping algorithm to use. By default value passed in Trainer will be available here. Example:: # Perform gradient clipping on gradients associated with discriminator (optimizer_idx=1) in GAN def configure_gradient_clipping(self, optimizer, optimizer_idx, gradient_clip_val, gradient_clip_algorithm): if optimizer_idx == 1: # Lightning will handle the gradient clipping self.clip_gradients( optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm ) else: # implement your own custom logic to clip gradients for generator (optimizer_idx=0)","title":"configure_gradient_clipping"},{"location":"reference/ophthalmology/modules/simclr/#configure_optimizers","text":"def configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Return: Any of these 6 options. - **Single optimizer**. - **List or Tuple** of optimizers. - **Two lists** - The first list has multiple optimizers, and the second has multiple LR schedulers (or multiple ``lr_scheduler_config``). - **Dictionary**, with an ``\"optimizer\"`` key, and (optionally) a ``\"lr_scheduler\"`` key whose value is a single LR scheduler or ``lr_scheduler_config``. - **Tuple of dictionaries** as described above, with an optional ``\"frequency\"`` key. - **None** - Fit will run without any optimizer. The lr_scheduler_config is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python lr_scheduler_config = { # REQUIRED: The scheduler instance \"scheduler\": lr_scheduler, # The unit of the scheduler's step size, could also be 'step'. # 'epoch' updates the scheduler on epoch end whereas 'step' # updates it after a optimizer update. \"interval\": \"epoch\", # How many epochs/steps should pass between calls to # `scheduler.step()`. 1 corresponds to updating the learning # rate after every epoch/step. \"frequency\": 1, # Metric to to monitor for schedulers like `ReduceLROnPlateau` \"monitor\": \"val_loss\", # If set to `True`, will enforce that the value specified 'monitor' # is available when the scheduler is updated, thus stopping # training if not found. If set to `False`, it will only produce a warning \"strict\": True, # If using the `LearningRateMonitor` callback to monitor the # learning rate progress, this keyword can be used to specify # a custom logged name \"name\": None, } When there are schedulers in which the .step() method is conditioned on a value, such as the","title":"configure_optimizers"},{"location":"reference/ophthalmology/modules/simclr/#configure_sharded_model","text":"def configure_sharded_model ( self ) -> None Hook to create modules in a distributed aware context. This is useful for when using sharded plugins, where we'd like to shard the model instantly, which is useful for extremely large models which can save memory and initialization time. This hook is called during each of fit/val/test/predict stages in the same process, so ensure that implementation of this hook is idempotent.","title":"configure_sharded_model"},{"location":"reference/ophthalmology/modules/simclr/#cpu_1","text":"def cpu ( self ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the CPU. Returns: Type Description Module self","title":"cpu"},{"location":"reference/ophthalmology/modules/simclr/#cuda_1","text":"def cuda ( self , device : Union [ torch . device , int , NoneType ] = None ) -> 'DeviceDtypeModuleMixin' Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Parameters: Name Type Description Default device None if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"cuda"},{"location":"reference/ophthalmology/modules/simclr/#double_1","text":"def double ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to double datatype. Returns: Type Description Module self","title":"double"},{"location":"reference/ophthalmology/modules/simclr/#eval_1","text":"def eval ( self : ~ T ) -> ~ T Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . See :ref: locally-disable-grad-doc for a comparison between .eval() and several similar mechanisms that may be confused with it. Returns: Type Description Module self","title":"eval"},{"location":"reference/ophthalmology/modules/simclr/#exclude_from_wt_decay","text":"def exclude_from_wt_decay ( self , named_params , weight_decay , skip_list = ( 'bias' , 'bn' ) ) Prepare the parameters to only weight-decay non bias and bn layers. This has proven to be a best practice. Code from: https://github.com/PyTorchLightning/lightning-bolts/blob/master/pl_bolts/models/self_supervised/simclr/simclr_module.py Parameters: Name Type Description Default named_params [type] [description] None weight_decay [type] [description] None skip_list tuple [description]. Defaults to (\"bias\", \"bn\"). (\"bias\", \"bn\") Returns: Type Description [type] [description]","title":"exclude_from_wt_decay"},{"location":"reference/ophthalmology/modules/simclr/#extra_repr_1","text":"def extra_repr ( self ) -> str Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.","title":"extra_repr"},{"location":"reference/ophthalmology/modules/simclr/#float_1","text":"def float ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to float datatype. Returns: Type Description Module self","title":"float"},{"location":"reference/ophthalmology/modules/simclr/#forward_1","text":"def forward ( self , x ) Same as :meth: torch.nn.Module.forward() . Args: args: Whatever you decide to pass into the forward method. *kwargs: Keyword arguments are also possible. Return: Your model's output","title":"forward"},{"location":"reference/ophthalmology/modules/simclr/#freeze","text":"def freeze ( self ) -> None Freeze all params for inference. Example:: model = MyLightningModule(...) model.freeze()","title":"freeze"},{"location":"reference/ophthalmology/modules/simclr/#get_buffer_1","text":"def get_buffer ( self , target : str ) -> 'Tensor' Returns the buffer given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the buffer to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.Tensor The buffer referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not a buffer","title":"get_buffer"},{"location":"reference/ophthalmology/modules/simclr/#get_extra_state_1","text":"def get_extra_state ( self ) -> Any Returns any extra state to include in the module's state_dict. Implement this and a corresponding :func: set_extra_state for your module if you need to store extra state. This function is called when building the module's state_dict() . Note that extra state should be pickleable to ensure working serialization of the state_dict. We only provide provide backwards compatibility guarantees for serializing Tensors; other objects may break backwards compatibility if their serialized pickled form changes. Returns: Type Description object Any extra state to store in the module's state_dict","title":"get_extra_state"},{"location":"reference/ophthalmology/modules/simclr/#get_from_queue","text":"def get_from_queue ( self , queue : < bound method BaseContext . SimpleQueue of < multiprocessing . context . DefaultContext object at 0x7fdd68b55ac0 >> ) -> None Retrieve the :attr: trainer.callback_metrics dictionary from the given queue. To preserve consistency, we cast back the data to torch.Tensor . Args: queue: the instance of the queue from where to get the data. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of DDPSpawnPlugin.get_from_queue and will be removed in v1.7.","title":"get_from_queue"},{"location":"reference/ophthalmology/modules/simclr/#get_parameter_1","text":"def get_parameter ( self , target : str ) -> 'Parameter' Returns the parameter given by target if it exists, otherwise throws an error. See the docstring for get_submodule for a more detailed explanation of this method's functionality as well as how to correctly specify target . Parameters: Name Type Description Default target None The fully-qualified string name of the Parameter to look for. (See get_submodule for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Parameter The Parameter referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Parameter","title":"get_parameter"},{"location":"reference/ophthalmology/modules/simclr/#get_progress_bar_dict","text":"def get_progress_bar_dict ( self ) -> Dict [ str , Union [ int , str ]]","title":"get_progress_bar_dict"},{"location":"reference/ophthalmology/modules/simclr/#get_submodule_1","text":"def get_submodule ( self , target : str ) -> 'Module' Returns the submodule given by target if it exists, otherwise throws an error. For example, let's say you have an nn.Module A that looks like this: .. code-block::text A( (net_b): Module( (net_c): Module( (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2)) ) (linear): Linear(in_features=100, out_features=200, bias=True) ) ) (The diagram shows an nn.Module A . A has a nested submodule net_b , which itself has two submodules net_c and linear . net_c then has a submodule conv .) To check whether or not we have the linear submodule, we would call get_submodule(\"net_b.linear\") . To check whether we have the conv submodule, we would call get_submodule(\"net_b.net_c.conv\") . The runtime of get_submodule is bounded by the degree of module nesting in target . A query against named_modules achieves the same result, but it is O(N) in the number of transitive modules. So, for a simple check to see if some submodule exists, get_submodule should always be used. Parameters: Name Type Description Default target None The fully-qualified string name of the submodule to look for. (See above example for how to specify a fully-qualified string.) None Returns: Type Description torch.nn.Module The submodule referenced by target Raises: Type Description AttributeError If the target string references an invalid path or resolves to something that is not an nn.Module","title":"get_submodule"},{"location":"reference/ophthalmology/modules/simclr/#half_1","text":"def half ( self ) -> 'DeviceDtypeModuleMixin' Casts all floating point parameters and buffers to half datatype. Returns: Type Description Module self","title":"half"},{"location":"reference/ophthalmology/modules/simclr/#load_state_dict_1","text":"def load_state_dict ( self , state_dict : 'OrderedDict[str, Tensor]' , strict : bool = True ) Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Parameters: Name Type Description Default state_dict dict a dict containing parameters and persistent buffers. None strict bool whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True None Returns: Type Description None NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Note: If a parameter or buffer is registered as None and its corresponding key exists in :attr: state_dict , :meth: load_state_dict will raise a RuntimeError . |","title":"load_state_dict"},{"location":"reference/ophthalmology/modules/simclr/#log","text":"def log ( self , name : str , value : Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional = None , tbptt_pad_token : Optional = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , metric_attribute : Optional [ str ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a key, value pair. Example:: self.log('train_loss', loss) The default behavior per hook is as follows: .. csv-table:: * also applies to the test loop :header: \"LightningModule Hook\", \"on_step\", \"on_epoch\", \"prog_bar\", \"logger\" :widths: 20, 10, 10, 10, 10 \"training_step\", \"T\", \"F\", \"F\", \"T\" \"training_step_end\", \"T\", \"F\", \"F\", \"T\" \"training_epoch_end\", \"F\", \"T\", \"F\", \"T\" \"validation_step \", \"F\", \"T\", \"F\", \"T\" \"validation_step_end \", \"F\", \"T\", \"F\", \"T\" \"validation_epoch_end*\", \"F\", \"T\", \"F\", \"T\" Parameters: Name Type Description Default name None key to log None value None value to log. Can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress bar None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs at the training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs at the val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group to sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None metric_attribute None To restore the metric state, Lightning requires the reference of the :class: torchmetrics.Metric in your model. This is found automatically if it is a model attribute. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log"},{"location":"reference/ophthalmology/modules/simclr/#log_dict","text":"def log_dict ( self , dictionary : Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float , Mapping [ str , Union [ torchmetrics . metric . Metric , torch . Tensor , int , float ]]]], prog_bar : bool = False , logger : bool = True , on_step : Optional [ bool ] = None , on_epoch : Optional [ bool ] = None , reduce_fx : Union [ str , Callable ] = 'default' , tbptt_reduce_fx : Optional [ Any ] = None , tbptt_pad_token : Optional [ Any ] = None , enable_graph : bool = False , sync_dist : bool = False , sync_dist_op : Optional [ Any ] = None , sync_dist_group : Optional [ Any ] = None , add_dataloader_idx : bool = True , batch_size : Optional [ int ] = None , rank_zero_only : Optional [ bool ] = None ) -> None Log a dictionary of values at once. Example:: values = {'loss': loss, 'acc': acc, ..., 'metric_n': metric_n} self.log_dict(values) Parameters: Name Type Description Default dictionary None key value pairs. The values can be a float , Tensor , Metric , or a dictionary of the former. None prog_bar None if True logs to the progress base None logger None if True logs to the logger None on_step None if True logs at this step. None auto-logs for training_step but not validation/test_step None on_epoch None if True logs epoch accumulated metrics. None auto-logs for val/test step but not training_step None reduce_fx None reduction function over step values for end of epoch. :meth: torch.mean by default. None enable_graph None if True, will not auto detach the graph None sync_dist None if True, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant communication overhead. None sync_dist_group None the ddp group sync across None add_dataloader_idx None if True, appends the index of the current dataloader to the name (when using multiple). If False, user needs to give unique names for each dataloader to not mix values None batch_size None Current batch_size. This will be directly inferred from the loaded batch, but some data structures might need to explicitly provide it. None rank_zero_only None Whether the value will be logged only on rank 0. This will prevent synchronization which would produce a deadlock as not all processes would perform this log call. None","title":"log_dict"},{"location":"reference/ophthalmology/modules/simclr/#log_grad_norm","text":"def log_grad_norm ( self , grad_norm_dict : Dict [ str , float ] ) -> None Override this method to change the default behaviour of log_grad_norm . If clipping gradients, the gradients will not have been clipped yet. Args: grad_norm_dict: Dictionary containing current grad norm metrics Example:: # DEFAULT def log_grad_norm(self, grad_norm_dict): self.log_dict(grad_norm_dict, on_step=False, on_epoch=True, prog_bar=False, logger=True)","title":"log_grad_norm"},{"location":"reference/ophthalmology/modules/simclr/#lr_schedulers","text":"def lr_schedulers ( self ) -> Union [ Any , List [ Any ], NoneType ] Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization. Returns: Type Description None A single scheduler, or a list of schedulers in case multiple ones are present, or None if no schedulers were returned in :meth: configure_optimizers .","title":"lr_schedulers"},{"location":"reference/ophthalmology/modules/simclr/#manual_backward","text":"def manual_backward ( self , loss : torch . Tensor , * args , ** kwargs ) -> None Call this directly from your :meth: training_step when doing optimizations manually. By using this, Lightning can ensure that all the proper scaling gets applied when using mixed precision. See :ref: manual optimization<common/optimizers:Manual optimization> for more examples. Example:: def training_step(...): opt = self.optimizers() loss = ... opt.zero_grad() # automatically applies scaling, etc... self.manual_backward(loss) opt.step() Parameters: Name Type Description Default loss None The tensor on which to compute gradients. Must have a graph attached. None *args None Additional positional arguments to be forwarded to :meth: ~torch.Tensor.backward None **kwargs None Additional keyword arguments to be forwarded to :meth: ~torch.Tensor.backward None","title":"manual_backward"},{"location":"reference/ophthalmology/modules/simclr/#modules_1","text":"def modules ( self ) -> Iterator [ ForwardRef ( 'Module' )] Returns an iterator over all modules in the network. Yields: Type Description Module a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) |","title":"modules"},{"location":"reference/ophthalmology/modules/simclr/#named_buffers_1","text":"def named_buffers ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . Tensor ]] Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Parameters: Name Type Description Default prefix str prefix to prepend to all buffer names. None recurse bool if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. None Yields: Type Description None (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) |","title":"named_buffers"},{"location":"reference/ophthalmology/modules/simclr/#named_children_1","text":"def named_children ( self ) -> Iterator [ Tuple [ str , ForwardRef ( 'Module' )]] Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: Type Description None (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) |","title":"named_children"},{"location":"reference/ophthalmology/modules/simclr/#named_modules_1","text":"def named_modules ( self , memo : Optional [ Set [ ForwardRef ( 'Module' )]] = None , prefix : str = '' , remove_duplicate : bool = True ) Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Args: memo: a memo to store the set of modules already added to the result prefix: a prefix that will be added to the name of the module remove_duplicate: whether to remove the duplicated module instances in the result or not Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True))","title":"named_modules"},{"location":"reference/ophthalmology/modules/simclr/#named_parameters_1","text":"def named_parameters ( self , prefix : str = '' , recurse : bool = True ) -> Iterator [ Tuple [ str , torch . nn . parameter . Parameter ]] Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Parameters: Name Type Description Default prefix str prefix to prepend to all parameter names. None recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description None (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) |","title":"named_parameters"},{"location":"reference/ophthalmology/modules/simclr/#on_after_backward","text":"def on_after_backward ( self ) -> None Called after loss.backward() and before optimizers are stepped. Note: If using native AMP, the gradients will not be unscaled at this point. Use the on_before_optimizer_step if you need the unscaled gradients.","title":"on_after_backward"},{"location":"reference/ophthalmology/modules/simclr/#on_after_batch_transfer","text":"def on_after_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch after it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_after_batch_transfer(self, batch, dataloader_idx): batch['x'] = gpu_transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_before_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_after_batch_transfer"},{"location":"reference/ophthalmology/modules/simclr/#on_before_backward","text":"def on_before_backward ( self , loss : torch . Tensor ) -> None Called before loss.backward() . Parameters: Name Type Description Default loss None Loss divided by number of batches for gradient accumulation and scaled if using native AMP. None","title":"on_before_backward"},{"location":"reference/ophthalmology/modules/simclr/#on_before_batch_transfer","text":"def on_before_batch_transfer ( self , batch : Any , dataloader_idx : int ) -> Any Override to alter or apply batch augmentations to your batch before it is transferred to the device. Note: To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be altered or augmented. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A batch of data Example:: def on_before_batch_transfer(self, batch, dataloader_idx): batch['x'] = transforms(batch['x']) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: on_after_batch_transfer | | - | meth: transfer_batch_to_device |","title":"on_before_batch_transfer"},{"location":"reference/ophthalmology/modules/simclr/#on_before_optimizer_step","text":"def on_before_optimizer_step ( self , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) -> None Called before optimizer.step() . The hook is only called if gradients do not need to be accumulated. See: :paramref: ~pytorch_lightning.trainer.Trainer.accumulate_grad_batches . If using native AMP, the loss will be unscaled before calling this hook. See these docs <https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients> __ for more information on the scaling of gradients. If clipping gradients, the gradients will not have been clipped yet. Parameters: Name Type Description Default optimizer None Current optimizer being used. None optimizer_idx None Index of the current optimizer being used. Example:: None def on_before_optimizer_step self, optimizer, optimizer_idx # example to inspect gradient information in tensorboard if self.trainer.global_step % 25 == 0: # don't make the tf file huge for k, v in self.named_parameters(): self.logger.experiment.add_histogram( tag=k, values=v.grad, global_step=self.trainer.global_step ) None","title":"on_before_optimizer_step"},{"location":"reference/ophthalmology/modules/simclr/#on_before_zero_grad","text":"def on_before_zero_grad ( self , optimizer : torch . optim . optimizer . Optimizer ) -> None Called after training_step() and before optimizer.zero_grad() . Called in the training loop after taking an optimizer step and before zeroing grads. Good place to inspect weight information with weights updated. This is where it is called:: for optimizer in optimizers: out = training_step(...) model.on_before_zero_grad(optimizer) # < ---- called here optimizer.zero_grad() backward() Parameters: Name Type Description Default optimizer None The optimizer for which grads should be zeroed. None","title":"on_before_zero_grad"},{"location":"reference/ophthalmology/modules/simclr/#on_epoch_end","text":"def on_epoch_end ( self ) -> None Called when either of train/val/test epoch ends.","title":"on_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_epoch_start","text":"def on_epoch_start ( self ) -> None Called when either of train/val/test epoch begins.","title":"on_epoch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_fit_end","text":"def on_fit_end ( self ) -> None Called at the very end of fit. If on DDP it is called on every process","title":"on_fit_end"},{"location":"reference/ophthalmology/modules/simclr/#on_fit_start","text":"def on_fit_start ( self ) -> None Called at the very beginning of fit. If on DDP it is called on every process","title":"on_fit_start"},{"location":"reference/ophthalmology/modules/simclr/#on_hpc_load","text":"def on_hpc_load ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager loads the model. Parameters: Name Type Description Default checkpoint None A dictionary with variables from the checkpoint. None","title":"on_hpc_load"},{"location":"reference/ophthalmology/modules/simclr/#on_hpc_save","text":"def on_hpc_save ( self , checkpoint : Dict [ str , Any ] ) -> None Hook to do whatever you need right before Slurm manager saves the model. Parameters: Name Type Description Default checkpoint None A dictionary in which you can save variables to save in a checkpoint. Contents need to be pickleable. None","title":"on_hpc_save"},{"location":"reference/ophthalmology/modules/simclr/#on_load_checkpoint","text":"def on_load_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning to restore your model. If you saved something with :meth: on_save_checkpoint this is your chance to restore this. Args: checkpoint: Loaded checkpoint Example:: def on_load_checkpoint(self, checkpoint): # 99% of the time you don't need to implement this method self.something_cool_i_want_to_save = checkpoint['something_cool_i_want_to_save'] Note: Lightning auto-restores global step, epoch, and train state including amp scaling. There is no need for you to restore anything regarding training.","title":"on_load_checkpoint"},{"location":"reference/ophthalmology/modules/simclr/#on_post_move_to_device","text":"def on_post_move_to_device ( self ) -> None Called in the parameter_validation decorator after","title":"on_post_move_to_device"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_batch_end","text":"def on_predict_batch_end ( self , outputs : Optional [ Any ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop after the batch. Parameters: Name Type Description Default outputs None The outputs of predict_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_batch_start","text":"def on_predict_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the predict loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_predict_batch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_dataloader","text":"def on_predict_dataloader ( self ) -> None Called before requesting the predict dataloader.","title":"on_predict_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_end","text":"def on_predict_end ( self ) -> None Called at the end of predicting.","title":"on_predict_end"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_epoch_end","text":"def on_predict_epoch_end ( self , results : List [ Any ] ) -> None Called at the end of predicting.","title":"on_predict_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_epoch_start","text":"def on_predict_epoch_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_epoch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_model_eval","text":"def on_predict_model_eval ( self ) -> None Sets the model to eval during the predict loop.","title":"on_predict_model_eval"},{"location":"reference/ophthalmology/modules/simclr/#on_predict_start","text":"def on_predict_start ( self ) -> None Called at the beginning of predicting.","title":"on_predict_start"},{"location":"reference/ophthalmology/modules/simclr/#on_pretrain_routine_end","text":"def on_pretrain_routine_end ( self ) -> None Called at the end of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_end"},{"location":"reference/ophthalmology/modules/simclr/#on_pretrain_routine_start","text":"def on_pretrain_routine_start ( self ) -> None Called at the beginning of the pretrain routine (between fit and train start). fit pretrain_routine start pretrain_routine end training_start","title":"on_pretrain_routine_start"},{"location":"reference/ophthalmology/modules/simclr/#on_save_checkpoint","text":"def on_save_checkpoint ( self , checkpoint : Dict [ str , Any ] ) -> None Called by Lightning when saving a checkpoint to give you a chance to store anything else you might want to save. Args: checkpoint: The full checkpoint dictionary before it gets dumped to a file. Implementations of this hook can insert additional data into this dictionary. Example:: def on_save_checkpoint(self, checkpoint): # 99% of use cases you don't need to implement this method checkpoint['something_cool_i_want_to_save'] = my_cool_pickable_object Note: Lightning saves all aspects of training (epoch, global step, etc...) including amp scaling. There is no need for you to store anything about training.","title":"on_save_checkpoint"},{"location":"reference/ophthalmology/modules/simclr/#on_test_batch_end","text":"def on_test_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop after the batch. Parameters: Name Type Description Default outputs None The outputs of test_step_end(test_step(x)) None batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_test_batch_start","text":"def on_test_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the test loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the test DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_test_batch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_test_dataloader","text":"def on_test_dataloader ( self ) -> None Called before requesting the test dataloader.","title":"on_test_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#on_test_end","text":"def on_test_end ( self ) -> None Called at the end of testing.","title":"on_test_end"},{"location":"reference/ophthalmology/modules/simclr/#on_test_epoch_end","text":"def on_test_epoch_end ( self ) -> None Called in the test loop at the very end of the epoch.","title":"on_test_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_test_epoch_start","text":"def on_test_epoch_start ( self ) -> None Called in the test loop at the very beginning of the epoch.","title":"on_test_epoch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_test_model_eval","text":"def on_test_model_eval ( self ) -> None Sets the model to eval during the test loop.","title":"on_test_model_eval"},{"location":"reference/ophthalmology/modules/simclr/#on_test_model_train","text":"def on_test_model_train ( self ) -> None Sets the model to train during the test loop.","title":"on_test_model_train"},{"location":"reference/ophthalmology/modules/simclr/#on_test_start","text":"def on_test_start ( self ) -> None Called at the beginning of testing.","title":"on_test_start"},{"location":"reference/ophthalmology/modules/simclr/#on_train_batch_end","text":"def on_train_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ]], batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop after the batch. Parameters: Name Type Description Default outputs None The outputs of training_step_end(training_step(x)) None batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_train_batch_start","text":"def on_train_batch_start ( self , batch : Any , batch_idx : int , unused : Optional [ int ] = 0 ) -> None Called in the training loop before anything happens for that batch. If you return -1 here, you will skip training for the rest of the current epoch. Parameters: Name Type Description Default batch None The batched data as it is returned by the training DataLoader. None batch_idx None the index of the batch None unused None Deprecated argument. Will be removed in v1.7. None","title":"on_train_batch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_train_dataloader","text":"def on_train_dataloader ( self ) -> None Called before requesting the train dataloader.","title":"on_train_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#on_train_end","text":"def on_train_end ( self ) -> None Called at the end of training before logger experiment is closed.","title":"on_train_end"},{"location":"reference/ophthalmology/modules/simclr/#on_train_epoch_end","text":"def on_train_epoch_end ( self ) -> None Called in the training loop at the very end of the epoch. To access all batch outputs at the end of the epoch, either: Implement training_epoch_end in the LightningModule OR Cache data across steps on the attribute(s) of the LightningModule and access them in this hook","title":"on_train_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_train_epoch_start","text":"def on_train_epoch_start ( self ) -> None Called in the training loop at the very beginning of the epoch.","title":"on_train_epoch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_train_start","text":"def on_train_start ( self ) -> None Called at the beginning of training after sanity check.","title":"on_train_start"},{"location":"reference/ophthalmology/modules/simclr/#on_val_dataloader","text":"def on_val_dataloader ( self ) -> None Called before requesting the val dataloader.","title":"on_val_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_batch_end","text":"def on_validation_batch_end ( self , outputs : Union [ torch . Tensor , Dict [ str , Any ], NoneType ], batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop after the batch. Parameters: Name Type Description Default outputs None The outputs of validation_step_end(validation_step(x)) None batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_batch_start","text":"def on_validation_batch_start ( self , batch : Any , batch_idx : int , dataloader_idx : int ) -> None Called in the validation loop before anything happens for that batch. Parameters: Name Type Description Default batch None The batched data as it is returned by the validation DataLoader. None batch_idx None the index of the batch None dataloader_idx None the index of the dataloader None","title":"on_validation_batch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_end","text":"def on_validation_end ( self ) -> None Called at the end of validation.","title":"on_validation_end"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_epoch_end","text":"def on_validation_epoch_end ( self ) -> None Called in the validation loop at the very end of the epoch.","title":"on_validation_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_epoch_start","text":"def on_validation_epoch_start ( self ) -> None Called in the validation loop at the very beginning of the epoch.","title":"on_validation_epoch_start"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_model_eval","text":"def on_validation_model_eval ( self ) -> None Sets the model to eval during the val loop.","title":"on_validation_model_eval"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_model_train","text":"def on_validation_model_train ( self ) -> None Sets the model to train during the val loop.","title":"on_validation_model_train"},{"location":"reference/ophthalmology/modules/simclr/#on_validation_start","text":"def on_validation_start ( self ) -> None Called at the beginning of validation.","title":"on_validation_start"},{"location":"reference/ophthalmology/modules/simclr/#optimizer_step","text":"def optimizer_step ( self , epoch : int , batch_idx : int , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int = 0 , optimizer_closure : Optional [ Callable [[], Any ]] = None , on_tpu : bool = False , using_native_amp : bool = False , using_lbfgs : bool = False ) -> None Override this method to adjust the default way the :class: ~pytorch_lightning.trainer.trainer.Trainer calls each optimizer. By default, Lightning calls step() and zero_grad() as shown in the example once per optimizer. This method (and zero_grad() ) won't be called during the accumulation phase when Trainer(accumulate_grad_batches != 1) . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers, this indexes into that list. optimizer_closure: Closure for all optimizers. This closure must be executed as it includes the calls to training_step() , optimizer.zero_grad() , and backward() . on_tpu: True if TPU backward is required using_native_amp: True if using native amp using_lbfgs: True if the matching optimizer is :class: torch.optim.LBFGS Examples:: # DEFAULT def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): optimizer.step(closure=optimizer_closure) # Alternating schedule for optimizer steps (i.e.: GANs) def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs): # update generator opt every step if optimizer_idx == 0: optimizer.step(closure=optimizer_closure) # update discriminator opt every 2 steps if optimizer_idx == 1: if (batch_idx + 1) % 2 == 0 : optimizer.step(closure=optimizer_closure) else: # call the closure by itself to run `training_step` + `backward` without an optimizer step optimizer_closure() # ... # add as many optimizers as you want Here's another example showing how to use this for more advanced things such as learning rate warm-up: .. code-block:: python # learning rate warm-up def optimizer_step( self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs, ): # warm up lr if self.trainer.global_step < 500: lr_scale = min(1.0, float(self.trainer.global_step + 1) / 500.0) for pg in optimizer.param_groups: pg[\"lr\"] = lr_scale * self.learning_rate # update params optimizer.step(closure=optimizer_closure)","title":"optimizer_step"},{"location":"reference/ophthalmology/modules/simclr/#optimizer_zero_grad","text":"def optimizer_zero_grad ( self , epoch : int , batch_idx : int , optimizer : torch . optim . optimizer . Optimizer , optimizer_idx : int ) Override this method to change the default behaviour of optimizer.zero_grad() . Args: epoch: Current epoch batch_idx: Index of current batch optimizer: A PyTorch optimizer optimizer_idx: If you used multiple optimizers this indexes into that list. Examples:: # DEFAULT def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad() # Set gradients to `None` instead of zero to improve performance. def optimizer_zero_grad(self, epoch, batch_idx, optimizer, optimizer_idx): optimizer.zero_grad(set_to_none=True) See :meth: torch.optim.Optimizer.zero_grad for the explanation of the above example.","title":"optimizer_zero_grad"},{"location":"reference/ophthalmology/modules/simclr/#optimizers","text":"def optimizers ( self , use_pl_optimizer : bool = True ) -> Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer , List [ torch . optim . optimizer . Optimizer ], List [ pytorch_lightning . core . optimizer . LightningOptimizer ]] Returns the optimizer(s) that are being used during training. Useful for manual optimization. Parameters: Name Type Description Default use_pl_optimizer None If True , will wrap the optimizer(s) in a :class: ~pytorch_lightning.core.optimizer.LightningOptimizer for automatic handling of precision and profiling. None Returns: Type Description None A single optimizer, or a list of optimizers in case multiple ones are present.","title":"optimizers"},{"location":"reference/ophthalmology/modules/simclr/#parameters_1","text":"def parameters ( self , recurse : bool = True ) -> Iterator [ torch . nn . parameter . Parameter ] Returns an iterator over module parameters. This is typically passed to an optimizer. Parameters: Name Type Description Default recurse bool if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. None Yields: Type Description Parameter module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) <class 'torch.Tensor'> (20L,) <class 'torch.Tensor'> (20L, 1L, 5L, 5L) |","title":"parameters"},{"location":"reference/ophthalmology/modules/simclr/#predict_dataloader","text":"def predict_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for prediction. It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note: Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Return: A :class: torch.utils.data.DataLoader or a sequence of them specifying prediction samples. Note: In the case where you return multiple prediction dataloaders, the :meth: predict will have an argument dataloader_idx which matches the order here.","title":"predict_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#predict_step","text":"def predict_step ( self , batch : Any , batch_idx : int , dataloader_idx : Optional [ int ] = None ) -> Any Step function called during :meth: ~pytorch_lightning.trainer.trainer.Trainer.predict . By default, it calls :meth: ~pytorch_lightning.core.lightning.LightningModule.forward . Override to add any processing logic. The :meth: ~pytorch_lightning.core.lightning.LightningModule.predict_step is used to scale inference on multi-devices. To prevent an OOM error, it is possible to use :class: ~pytorch_lightning.callbacks.BasePredictionWriter callback to write the predictions to disk or database after each batch or on epoch end. The :class: ~pytorch_lightning.callbacks.BasePredictionWriter should be used while using a spawn based accelerator. This happens for Trainer(strategy=\"ddp_spawn\") or training on 8 TPU cores with Trainer(tpu_cores=8) as predictions won't be returned. Example :: class MyModel(LightningModule): def predicts_step(self, batch, batch_idx, dataloader_idx): return self(batch) dm = ... model = MyModel() trainer = Trainer(gpus=2) predictions = trainer.predict(model, dm) Args: batch: Current batch batch_idx: Index of current batch dataloader_idx: Index of the current dataloader Return: Predicted output","title":"predict_step"},{"location":"reference/ophthalmology/modules/simclr/#prepare_data","text":"def prepare_data ( self ) -> None Use this to download and prepare data. .. warning:: DO NOT set state to the model (use setup instead) since this is NOT called on every GPU in DDP/TPU Example:: def prepare_data(self): # good download_data() tokenize() etc() # bad self.split = data_split self.some_state = some_other_state() In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)): Once per node. This is the default and is only called on LOCAL_RANK=0. Once in total. Only called on GLOBAL_RANK=0. Example:: # DEFAULT # called once per node on LOCAL_RANK=0 of that node Trainer(prepare_data_per_node=True) # call on GLOBAL_RANK=0 (great for shared file systems) Trainer(prepare_data_per_node=False) Note: Setting prepare_data_per_node with the trainer flag is deprecated and will be removed in v1.7.0. Please set prepare_data_per_node in LightningDataModule or LightningModule directly instead. This is called before requesting the dataloaders: .. code-block:: python model.prepare_data() initialize_distributed() model.setup(stage) model.train_dataloader() model.val_dataloader() model.test_dataloader()","title":"prepare_data"},{"location":"reference/ophthalmology/modules/simclr/#print","text":"def print ( self , * args , ** kwargs ) -> None Prints only from process 0. Use this in any distributed mode to log only once. Parameters: Name Type Description Default *args None The thing to print. The same as for Python's built-in print function. None **kwargs None The same as for Python's built-in print function. Example:: None def forward self, x self.print(x, 'in forward') None","title":"print"},{"location":"reference/ophthalmology/modules/simclr/#register_backward_hook_1","text":"def register_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. This function is deprecated in favor of :meth: ~torch.nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_backward_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_buffer_1","text":"def register_buffer ( self , name : str , tensor : Optional [ torch . Tensor ], persistent : bool = True ) -> None Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor or None): buffer to be registered. If None , then operations that run on buffers, such as :attr: cuda , are ignored. If None , the buffer is not included in the module's :attr: state_dict . persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features))","title":"register_buffer"},{"location":"reference/ophthalmology/modules/simclr/#register_forward_hook_1","text":"def register_forward_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after","title":"register_forward_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_forward_pre_hook_1","text":"def register_forward_pre_hook ( self , hook : Callable [ ... , NoneType ] ) -> torch . utils . hooks . RemovableHandle Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_forward_pre_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_full_backward_hook_1","text":"def register_full_backward_hook ( self , hook : Callable [[ ForwardRef ( 'Module' ), Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ], Union [ Tuple [ torch . Tensor , ... ], torch . Tensor ]], Optional [ torch . Tensor ]] ) -> torch . utils . hooks . RemovableHandle Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. For technical reasons, when this hook is applied to a Module, its forward function will receive a view of each Tensor passed to the Module. Similarly the caller will receive a view of each Tensor returned by the Module's forward function. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: Type Description None :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove()","title":"register_full_backward_hook"},{"location":"reference/ophthalmology/modules/simclr/#register_parameter_1","text":"def register_parameter ( self , name : str , param : Optional [ torch . nn . parameter . Parameter ] ) -> None Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Parameters: Name Type Description Default name string name of the parameter. The parameter can be accessed from this module using the given name None param Parameter or None parameter to be added to the module. If None , then operations that run on parameters, such as :attr: cuda , are ignored. If None , the parameter is not included in the module's :attr: state_dict . None","title":"register_parameter"},{"location":"reference/ophthalmology/modules/simclr/#requires_grad__1","text":"def requires_grad_ ( self : ~ T , requires_grad : bool = True ) -> ~ T Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). See :ref: locally-disable-grad-doc for a comparison between .requires_grad_() and several similar mechanisms that may be confused with it. Parameters: Name Type Description Default requires_grad bool whether autograd should record operations on parameters in this module. Default: True . None Returns: Type Description Module self","title":"requires_grad_"},{"location":"reference/ophthalmology/modules/simclr/#save_hyperparameters","text":"def save_hyperparameters ( self , * args , ignore : Union [ Sequence [ str ], str , NoneType ] = None , frame : Optional [ frame ] = None , logger : bool = True ) -> None Save arguments to hparams attribute. Args: args: single object of dict , NameSpace or OmegaConf or string names or arguments from class __init__ ignore: an argument name or a list of argument names from class __init__ to be ignored frame: a frame object. Default is None logger: Whether to send the hyperparameters to the logger. Default: True Example:: >>> class ManuallyArgsModel(HyperparametersMixin): ... def init (self, arg1, arg2, arg3): ... super(). init () ... # manually assign arguments ... self.save_hyperparameters('arg1', 'arg3') ... def forward(self, args, *kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14 >>> class AutomaticArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # equivalent automatic ... self.save_hyperparameters() ... def forward(self, *args, **kwargs): ... ... >>> model = AutomaticArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg2\": abc \"arg3\": 3.14 >>> class SingleArgModel(HyperparametersMixin): ... def __init__(self, params): ... super().__init__() ... # manually assign single argument ... self.save_hyperparameters(params) ... def forward(self, *args, **kwargs): ... ... >>> model = SingleArgModel(Namespace(p1=1, p2='abc', p3=3.14)) >>> model.hparams \"p1\": 1 \"p2\": abc \"p3\": 3.14 >>> class ManuallyArgsModel(HyperparametersMixin): ... def __init__(self, arg1, arg2, arg3): ... super().__init__() ... # pass argument(s) to ignore as a string or in a list ... self.save_hyperparameters(ignore='arg2') ... def forward(self, *args, **kwargs): ... ... >>> model = ManuallyArgsModel(1, 'abc', 3.14) >>> model.hparams \"arg1\": 1 \"arg3\": 3.14","title":"save_hyperparameters"},{"location":"reference/ophthalmology/modules/simclr/#set_extra_state_1","text":"def set_extra_state ( self , state : Any ) This function is called from :func: load_state_dict to handle any extra state found within the state_dict . Implement this function and a corresponding","title":"set_extra_state"},{"location":"reference/ophthalmology/modules/simclr/#setup","text":"def setup ( self , stage : Optional [ str ] = None ) -> None Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' Example:: None class LitModel ... def init (self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) | None |","title":"setup"},{"location":"reference/ophthalmology/modules/simclr/#share_memory_1","text":"def share_memory ( self : ~ T ) -> ~ T See :meth: torch.Tensor.share_memory_","title":"share_memory"},{"location":"reference/ophthalmology/modules/simclr/#state_dict_1","text":"def state_dict ( self , destination = None , prefix = '' , keep_vars = False ) Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included. Returns: Type Description dict a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] |","title":"state_dict"},{"location":"reference/ophthalmology/modules/simclr/#summarize","text":"def summarize ( self , mode : Optional [ str ] = 'top' , max_depth : Optional [ int ] = None ) -> Optional [ pytorch_lightning . utilities . model_summary . ModelSummary ] Summarize this LightningModule. .. deprecated:: v1.5 This method was deprecated in v1.5 in favor of pytorch_lightning.utilities.model_summary.summarize and will be removed in v1.7. Args: mode: Can be either 'top' (summarize only direct submodules) or 'full' (summarize all layers). .. deprecated:: v1.4 This parameter was deprecated in v1.4 in favor of `max_depth` and will be removed in v1.6. max_depth: The maximum depth of layer nesting that the summary will include. A value of 0 turns the layer summary off. Default: 1. Return: The model summary object","title":"summarize"},{"location":"reference/ophthalmology/modules/simclr/#tbptt_split_batch","text":"def tbptt_split_batch ( self , batch : Any , split_size : int ) -> List [ Any ] When using truncated backpropagation through time, each batch must be split along the time dimension. Lightning handles this by default, but for custom behavior override this function. Args: batch: Current batch split_size: The size of the split Return: List of batch splits. Each split will be passed to :meth: training_step to enable truncated back propagation through time. The default implementation splits root level Tensors and Sequences at dim=1 (i.e. time dim). It assumes that each time dim is the same length. Examples:: def tbptt_split_batch(self, batch, split_size): splits = [] for t in range(0, time_dims[0], split_size): batch_split = [] for i, x in enumerate(batch): if isinstance(x, torch.Tensor): split_x = x[:, t:t + split_size] elif isinstance(x, collections.Sequence): split_x = [None] * len(x) for batch_idx in range(len(x)): split_x[batch_idx] = x[batch_idx][t:t + split_size] batch_split.append(split_x) splits.append(batch_split) return splits Note: Called in the training loop after :meth: ~pytorch_lightning.callbacks.base.Callback.on_batch_start if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Each returned batch split is passed separately to :meth: training_step .","title":"tbptt_split_batch"},{"location":"reference/ophthalmology/modules/simclr/#teardown","text":"def teardown ( self , stage : Optional [ str ] = None ) -> None Called at the end of fit (train + validate), validate, test, predict, or tune. Parameters: Name Type Description Default stage None either 'fit' , 'validate' , 'test' , or 'predict' None","title":"teardown"},{"location":"reference/ophthalmology/modules/simclr/#test_dataloader","text":"def test_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be reloaded unless you set","title":"test_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#test_epoch_end","text":"def test_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of a test epoch with the output of all test steps. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: test_step_end , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: test_step , this won't be called. | None |","title":"test_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#test_step","text":"def test_step ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Operates on a single batch of data from the test set. In this step you'd normally generate examples or calculate anything of interest such as accuracy. .. code-block:: python # the pseudocode for these calls test_outs = [] for test_batch in test_data: out = test_step(test_batch) test_outs.append(out) test_epoch_end(test_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch. dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple test dataloaders used). Return: Any of. - Any object or value - ``None`` - Testing will skip to the next batch .. code-block:: python # if you have one test dataloader: def test_step(self, batch, batch_idx): ... # if you have multiple test dataloaders: def test_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single test dataset def test_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) test_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'test_loss': loss, 'test_acc': test_acc}) If you pass in multiple test dataloaders, :meth: test_step will have an additional argument. .. code-block:: python # CASE 2: multiple test dataloaders def test_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to test you don't need to implement this method. Note: When the :meth: test_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of the test epoch, the model goes back to training mode and gradients are enabled.","title":"test_step"},{"location":"reference/ophthalmology/modules/simclr/#test_step_end","text":"def test_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when testing with dp or ddp2 because :meth: test_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [test_step(sub_batch) for sub_batch in sub_batches] test_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: test_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT test_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) loss = self.softmax(out) self.log(\"test_loss\", loss) # -------------- # with test_step_end to do softmax over the full batch def test_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return out def test_step_end(self, output_results): # this out is now the full size of the batch all_test_step_outs = output_results.out loss = nce_loss(all_test_step_outs) self.log(\"test_loss\", loss) See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"test_step_end"},{"location":"reference/ophthalmology/modules/simclr/#to_1","text":"def to ( self , * args : Any , ** kwargs : Any ) -> 'DeviceDtypeModuleMixin' Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point desired :attr: dtype s. In addition, this method will only cast the floating point parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. Note: This method modifies the module in-place. Parameters: Name Type Description Default device None the desired device of the parameters and buffers in this module None dtype None the desired floating point type of the floating point parameters and buffers in this module None tensor None Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module None Returns: Type Description Module self Example:: >>> class ExampleModule(DeviceDtypeModuleMixin): ... def init (self, weight: torch.Tensor): ... super(). init () ... self.register_buffer('weight', weight) >>> _ = torch.manual_seed(0) >>> module = ExampleModule(torch.rand(3, 4)) >>> module.weight #doctest: +ELLIPSIS tensor([[...]]) >>> module.to(torch.double) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float64) >>> cpu = torch.device('cpu') >>> module.to(cpu, dtype=torch.half, non_blocking=True) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.to(cpu) ExampleModule() >>> module.weight #doctest: +ELLIPSIS tensor([[...]], dtype=torch.float16) >>> module.device device(type='cpu') >>> module.dtype torch.float16","title":"to"},{"location":"reference/ophthalmology/modules/simclr/#to_empty_1","text":"def to_empty ( self : ~ T , * , device : Union [ str , torch . device ] ) -> ~ T Moves the parameters and buffers to the specified device without copying storage. Parameters: Name Type Description Default device ( None class: torch.device ): The desired device of the parameters and buffers in this module. None Returns: Type Description Module self","title":"to_empty"},{"location":"reference/ophthalmology/modules/simclr/#to_onnx","text":"def to_onnx ( self , file_path : Union [ str , pathlib . Path ], input_sample : Optional [ Any ] = None , ** kwargs ) Saves the model in ONNX format. Parameters: Name Type Description Default file_path None The path of the file the onnx model should be saved to. None input_sample None An input for tracing. Default: None (Use self.example_input_array) None **kwargs None Will be passed to torch.onnx.export function. None","title":"to_onnx"},{"location":"reference/ophthalmology/modules/simclr/#to_torchscript","text":"def to_torchscript ( self , file_path : Union [ str , pathlib . Path , NoneType ] = None , method : Optional [ str ] = 'script' , example_inputs : Optional [ Any ] = None , ** kwargs ) -> Union [ torch . _C . ScriptModule , Dict [ str , torch . _C . ScriptModule ]] By default compiles the whole model to a :class: ~torch.jit.ScriptModule . If you want to use tracing, please provided the argument method='trace' and make sure that either the example_inputs argument is provided, or the model has :attr: example_input_array set. If you would like to customize the modules that are scripted you should override this method. In case you want to return multiple modules, we recommend using a dictionary. Args: file_path: Path where to save the torchscript. Default: None (no file saved). method: Whether to use TorchScript's script or trace method. Default: 'script' example_inputs: An input to be used to do tracing when method is set to 'trace'. Default: None (uses :attr: example_input_array ) **kwargs: Additional arguments that will be passed to the :func: torch.jit.script or :func: torch.jit.trace function. Note: - Requires the implementation of the :meth: ~pytorch_lightning.core.lightning.LightningModule.forward method. - The exported script will be set to evaluation mode. - It is recommended that you install the latest supported version of PyTorch to use this feature without limitations. See also the :mod: torch.jit documentation for supported features. Example: >>> class SimpleModel(LightningModule): ... def init (self): ... super(). init () ... self.l1 = torch.nn.Linear(in_features=64, out_features=4) ... ... def forward(self, x): ... return torch.relu(self.l1(x.view(x.size(0), -1))) ... >>> model = SimpleModel() >>> torch.jit.save(model.to_torchscript(), \"model.pt\") # doctest: +SKIP >>> os.path.isfile(\"model.pt\") # doctest: +SKIP >>> torch.jit.save(model.to_torchscript(file_path=\"model_trace.pt\", method='trace', # doctest: +SKIP ... example_inputs=torch.randn(1, 64))) # doctest: +SKIP >>> os.path.isfile(\"model_trace.pt\") # doctest: +SKIP True Return: This LightningModule as a torchscript, regardless of whether file_path is defined or not.","title":"to_torchscript"},{"location":"reference/ophthalmology/modules/simclr/#toggle_optimizer","text":"def toggle_optimizer ( self , optimizer : Union [ torch . optim . optimizer . Optimizer , pytorch_lightning . core . optimizer . LightningOptimizer ], optimizer_idx : int ) -> None Makes sure only the gradients of the current optimizer's parameters are calculated in the training step to prevent dangling gradients in multiple-optimizer setup. This is only called automatically when automatic optimization is enabled and multiple optimizers are used. It works with :meth: untoggle_optimizer to make sure param_requires_grad_state is properly reset. Parameters: Name Type Description Default optimizer None The optimizer to toggle. None optimizer_idx None The index of the optimizer to toggle. None","title":"toggle_optimizer"},{"location":"reference/ophthalmology/modules/simclr/#train_1","text":"def train ( self : ~ T , mode : bool = True ) -> ~ T Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Parameters: Name Type Description Default mode bool whether to set training mode ( True ) or evaluation mode ( False ). Default: True . None Returns: Type Description Module self","title":"train"},{"location":"reference/ophthalmology/modules/simclr/#train_dataloader","text":"def train_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ], Sequence [ Sequence [ torch . utils . data . dataloader . DataLoader ]], Sequence [ Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , torch . utils . data . dataloader . DataLoader ], Dict [ str , Dict [ str , torch . utils . data . dataloader . DataLoader ]], Dict [ str , Sequence [ torch . utils . data . dataloader . DataLoader ]]] Implement one or more PyTorch DataLoaders for training. Return: A collection of :class: torch.utils.data.DataLoader specifying training samples. In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> . The dataloader you return will not be reloaded unless you set","title":"train_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#training_epoch_end","text":"def training_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the training epoch with the outputs of all training steps. Use this in case you need to do something with all the outputs returned by :meth: training_step . .. code-block:: python # the pseudocode for these calls train_outs = [] for train_batch in train_data: out = training_step(train_batch) train_outs.append(out) training_epoch_end(train_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: training_step . If there are multiple optimizers, it is a list containing a list of outputs for each optimizer. If using truncated_bptt_steps > 1 , each element is a list of outputs corresponding to the outputs of each processed split batch. Return: | None | | None Note | None | | None | | If this method is not overridden, this won't be called. .. code-block | None | : python | None | | def training_epoch_end | self, training_step_outputs | # do something with all training_step outputs for out in training_step_outputs: ... | None |","title":"training_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#training_step","text":"def training_step ( self , batch , batch_idx_ ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx ( int ): Integer displaying index of this batch optimizer_idx ( int ): When using multiple optimizers, this argument will also be present. hiddens ( Any ): Passed in if :paramref: ~pytorch_lightning.core.lightning.LightningModule.truncated_bptt_steps > 0. Return: Any of. - :class:`~torch.Tensor` - The loss tensor - ``dict`` - A dictionary. Can include any keys, but must include the key ``'loss'`` - ``None`` - Training will skip to the next batch. This is only for automatic optimization. This is not supported for multi-GPU, TPU, IPU, or DeepSpeed. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder ... if optimizer_idx == 1: # do training_step with decoder ... If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step out, hiddens = self.lstm(data, hiddens) loss = ... return {\"loss\": loss, \"hiddens\": hiddens} Note: The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step.","title":"training_step"},{"location":"reference/ophthalmology/modules/simclr/#training_step_end","text":"def training_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ]] Use this when training with dp or ddp2 because :meth: training_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [training_step(sub_batch) for sub_batch in sub_batches] training_step_end(batch_parts_outputs) Parameters: Name Type Description Default batch_parts_outputs None What you return in training_step for each batch part. Return: None Anything When using dp/ddp2 distributed backends, only a portion of the batch is inside the training_step | None | .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self(x) # softmax uses only a portion of the batch in the denominator loss = self.softmax(out) loss = nce_loss(loss) return loss If you wish to do something with all the parts of the batch, then use this method to do it: .. code-block:: python | None | | def training_step | self, batch, batch_idx | # batch is 1/num_gpus big x, y = batch out = self.encoder(x) return {\"pred\": out} | None | | def training_step_end | self, training_step_outputs | gpu_0_pred = training_step_outputs[0][\"pred\"] gpu_1_pred = training_step_outputs[1][\"pred\"] gpu_n_pred = training_step_outputs[n][\"pred\"] # this softmax now uses the full batch loss = nce_loss([gpu_0_pred, gpu_1_pred, gpu_n_pred]) return loss See Also: | None | | See the | None | ref: advanced/multi_gpu:Multi-GPU training guide for more details. | None |","title":"training_step_end"},{"location":"reference/ophthalmology/modules/simclr/#transfer_batch_to_device","text":"def transfer_batch_to_device ( self , batch : Any , device : torch . device , dataloader_idx : int ) -> Any Override this hook if your :class: ~torch.utils.data.DataLoader returns tensors wrapped in a custom data structure. The data types listed below (and any arbitrary nesting of them) are supported out of the box: :class: torch.Tensor or anything that implements .to(...) :class: list :class: dict :class: tuple :class: torchtext.data.batch.Batch For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ...). Note: This hook should only transfer the data and not modify it, nor should it move the data to any other device than the one passed in as argument (unless you know what you are doing). To check the current state of execution of this hook you can use self.trainer.training/testing/validating/predicting so that you can add different logic as per your requirement. Note: This hook only runs on single GPU training and DDP (no data-parallel). Data-Parallel support will come in near future. Parameters: Name Type Description Default batch None A batch of data that needs to be transferred to a new device. None device None The target device as defined in PyTorch. None dataloader_idx None The index of the dataloader to which the batch belongs. None Returns: Type Description None A reference to the data on the new device. Example:: def transfer_batch_to_device(self, batch, device, dataloader_idx): if isinstance(batch, CustomBatch): # move all tensors in your custom data structure to the device batch.samples = batch.samples.to(device) batch.targets = batch.targets.to(device) elif dataloader_idx == 0: # skip device transfer for the first dataloader or anything you wish pass else: batch = super().transfer_batch_to_device(data, device) return batch | Raises: Type Description MisconfigurationException If using data-parallel, Trainer(strategy='dp') . See Also: | | - | meth: move_data_to_device | | - | meth: apply_to_collection |","title":"transfer_batch_to_device"},{"location":"reference/ophthalmology/modules/simclr/#type_1","text":"def type ( self , dst_type : Union [ str , torch . dtype ] ) -> 'DeviceDtypeModuleMixin' Casts all parameters and buffers to :attr: dst_type . Parameters: Name Type Description Default dst_type type or string the desired type None Returns: Type Description Module self","title":"type"},{"location":"reference/ophthalmology/modules/simclr/#unfreeze","text":"def unfreeze ( self ) -> None Unfreeze all parameters for training. .. code-block:: python model = MyLightningModule(...) model.unfreeze()","title":"unfreeze"},{"location":"reference/ophthalmology/modules/simclr/#untoggle_optimizer","text":"def untoggle_optimizer ( self , optimizer_idx : int ) -> None Resets the state of required gradients that were toggled with :meth: toggle_optimizer . This is only called automatically when automatic optimization is enabled and multiple optimizers are used. Parameters: Name Type Description Default optimizer_idx None The index of the optimizer to untoggle. None","title":"untoggle_optimizer"},{"location":"reference/ophthalmology/modules/simclr/#val_dataloader","text":"def val_dataloader ( self ) -> Union [ torch . utils . data . dataloader . DataLoader , Sequence [ torch . utils . data . dataloader . DataLoader ]] Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be reloaded unless you set","title":"val_dataloader"},{"location":"reference/ophthalmology/modules/simclr/#validation_epoch_end","text":"def validation_epoch_end ( self , outputs : List [ Union [ torch . Tensor , Dict [ str , Any ]]] ) -> None Called at the end of the validation epoch with the outputs of all validation steps. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default outputs None List of outputs you defined in :meth: validation_step , or if there are multiple dataloaders, a list containing a list of outputs for each dataloader. Return: | None | | None Note | None | | None | | If you didn't define a | None | meth: validation_step , this won't be called. | None |","title":"validation_epoch_end"},{"location":"reference/ophthalmology/modules/simclr/#validation_step","text":"def validation_step ( self , batch , batch_idx_ ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Args: batch (:class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. batch_idx (int): The index of this batch dataloader_idx (int): The index of the dataloader that produced this batch (only if multiple val dataloaders used) Return: - Any object or value - None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined(\"validation_step_end\"): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx): ... # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx): ... Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. ... Note: If you don't need to validate you don't need to implement this method. Note: When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled.","title":"validation_step"},{"location":"reference/ophthalmology/modules/simclr/#validation_step_end","text":"def validation_step_end ( self , * args , ** kwargs ) -> Union [ torch . Tensor , Dict [ str , Any ], NoneType ] Use this when validating with dp or ddp2 because :meth: validation_step will operate on only part of the batch. However, this is still optional and only needed for things like softmax or NCE loss. Note: If you later switch to ddp or some other mode, this will still be called so that you don't have to change your code. .. code-block:: python # pseudocode sub_batches = split_batches_for_dp(batch) batch_parts_outputs = [validation_step(sub_batch) for sub_batch in sub_batches] validation_step_end(batch_parts_outputs) Args: batch_parts_outputs: What you return in :meth: validation_step for each batch part. Return: None or anything .. code-block:: python # WITHOUT validation_step_end # if used in DP or DDP2, this batch is 1/num_gpus large def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self.encoder(x) loss = self.softmax(out) loss = nce_loss(loss) self.log(\"val_loss\", loss) # -------------- # with validation_step_end to do softmax over the full batch def validation_step(self, batch, batch_idx): # batch is 1/num_gpus big x, y = batch out = self(x) return out def validation_step_end(self, val_step_outputs): for out in val_step_outputs: ... See Also: See the :ref: advanced/multi_gpu:Multi-GPU training guide for more details.","title":"validation_step_end"},{"location":"reference/ophthalmology/modules/simclr/#xpu_1","text":"def xpu ( self : ~ T , device : Union [ int , torch . device , NoneType ] = None ) -> ~ T Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. .. note:: This method modifies the module in-place. Parameters: Name Type Description Default device int if specified, all parameters will be copied to that device None Returns: Type Description Module self","title":"xpu"},{"location":"reference/ophthalmology/modules/simclr/#zero_grad_1","text":"def zero_grad ( self , set_to_none : bool = False ) -> None Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Parameters: Name Type Description Default set_to_none bool instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. None","title":"zero_grad"}]}